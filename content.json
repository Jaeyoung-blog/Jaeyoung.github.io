{"pages":[{"title":"About","date":"2020-03-03T00:54:07.088Z","path":"about/index.html","text":""},{"title":"Categories","date":"2020-03-03T00:54:07.089Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2020-03-03T00:54:07.090Z","path":"tags/index.html","text":""}],"posts":[{"title":"02. PageRank Algorithm","date":"2020-09-20T12:39:05.000Z","path":"studynotes/recommender-systems/02-pagerank-algorithm/","text":"PageRank 알고리즘은 구글이 개발하고 사용중인 구글 검색엔진 알고리즘이다. 사용자가 검색어를 입력하면, 그 검색어에 해당하는 글들을 추천해서 띄워주는 추천 시스템이라고도 볼 수 있다. 본 문서에서는 PageRank 알고리즘에 대해서 정리보해보고자 한다. Introduction PageRank 알고리즘을 간단하게 요약하면, 각 웹페이지마다 랭크를 매긴 후, 검색어에 따라, 높은 랭킹의 웹 페이지를 상단에 위치시키기 위한 알고리즘이다. 즉, 페이지마다 랭크를 매기는 기술이다. 구글은 각 웹 페이지의 랭킹을 매기기 위해, 페이지간의 링크를 활용한다. 어떤 특정 웹 페이지로 가는 외부링크(유입경로)가 많다면, 그 웹 페이지의 랭크는 상승하도록 한다. 반면, 유입될 경로가 적은 웹 페이지의 경우, 그 랭크가 낮다. PageRank 알고리즘을 이해하기 위해서는 먼저 Markov chain을 이해해야 한다. 구글은 페이지 유입 경로인 링크를 통한 페이지 이동을 Markov chain으로 모델링하고, 해당 Markov chain으로 PageRank를 계산한다. Preliminaries 본 파트에서는 PageRank 알고리즘을 이해하기 위해 필요한 선행지식에 대해 최소한으로 논의해보고자 한다. PageRank를 이해하는데 다음의 개념을 아는 것이 필요하다. Markov chain Transition matrix Transition matrix smoothing Limiting distribution vs stationary distribution (Perron-Frobenius Theorem) Markov Chain 구글은 페이지간 링크 이동을 Markov chain으로 모델링해 놓았다. 페이지 $i$에서 $n$개의 페이지로 가는 링크가 있다고 하자. 그럼, 그 $n$개의 페이지로 갈 확률은 각각 $1/n$이다. 페이지 $i$에서 링크를 통해 페이지 $j$로 가는 확률 $p(i \\rightarrow j)$은 오직 페이지 $i$에 있을떄의 상태로부터만 영향을 받는다. 사용자가 페이지 $i$로 어떤 경로를 통해 들어왔건, $p(i \\rightarrow j)$에는 영향을 끼치지 않는다. 즉, 웹페이지 이동 모델은 Markov assumption을 따른다. 예를들어, $i$라는 페이지가 있다고 가정했을 때, 이 페이지에는 다른 외부페이지 $j, k, l$로 가는 링크가 존재한다고 하자. 그럼, 페이지 $i$에서 각 페이지로 이동할 확률은 각각 $1/3$이 된다. p(i \\rightarrow j) = p(i \\rightarrow k) = p(i \\rightarrow l) = \\frac{1}{3}Transition Matrix of Markov ChainMarkov chain에서는 상태를 이동하는 확률을 저장한 transition matrix를 정의할 수 있다. 웹 페이지가 $i, j, k$ 3개밖에 없다고 가정해보자. 그리고, $i$ 페이지에는 $j$로 가는 링크가 있고, $j$ 페이지에는 $i, k$로 가는 링크가 있고, $k$페이지에는 페이지 $i$로 가는 링크가 있다고 해 보자. 그럼, 세 개의 웹 페이지 $i, j, k$에 대해 transition matrix $A$는 다음처럼 정의할 수 있다. A = \\begin{pmatrix} 0 & 1 & 0 \\\\ \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ 1 & 0 & 0 \\end{pmatrix}첫 번째 행은 페이지 $i$에서 각각 $i, j, k$로 이동할 확률을 의미하며, 두 번째 행은 페이지 $j$에서 각각 $i, j, k$로 이동할 확률, 세 번째 행은 페이지 $k$에서 각각 $i, j, k$로 이동할 확률을 의마한다. 하지만, 알다시피, 구글을 통해 접근할 수 있는 웹 페이지는 한 두개가 아니다. 우리가 상상할 수 없을 정도의 개수만큼 웹 페이지가 존재하는데, 웹 페이지가 100만개 있다고 가정해보자. 그리고, 어느 한 페이지 $i$에 들어가본다고 가정해보자. 이때, 페이지 $i$에는 과연 100만개의 웹 페이지중에서 몇 개나 링크를 가지고 있을지 상상해 볼 수 있을 것이다. 당연히 대부분의 웹 페이지에 대한 링크는 가지고 있지 않고 매우 극소수의 페이지로 가는 링크만이 존재한다. 이것은 페이지 $i$ 뿐 아니라, 100만개의 모든 웹 페이지에게 성립한다. 즉, transition matrix의 관점으로 보면, 대부분의 원소가 0이 된다. Limiting Distribution (vs Stationary Distribution)그럼, 이 transition matrix로 페이지의 랭크를 어떻게 계산할까. 바로 transition matrix의 limiting distribution을 계산하는 것이 PageRank 알고리즘의 핵심이다. Transition matrix는 각 페이지에서 다른 페이지로 이동할 확률을 정의한 행렬이다. 그러나, 이 행렬은 각 페이지를 비교하는 어떤 수치값을 제공해주지는 못한다. 구글에서 취한 전략은 다음과 같다. Initial state distribution $\\pi_0$을 정의한다. Initial state distribution이란, 맨 처음 웹 페이지를 켰을 때, 각 페이지를 방문할 확률을 의미한다. 구글에서 접속할 수 있는 웹 페이지가 3개라면, 구글은 initial state distribution $\\pi_0$를 다음과 같이 정의한다. \\pi_0 = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix} 즉, 모든 웹 페이지로 시작할 확률이 같도록 정의한다. 이때, $\\pi_0$의 각 원소는 각각 $p(s_0 = 1)$, $p(s_0 = 2)$, $p(s_0 = 3)$을 나타낸다. $s_0$는 첫 번째 state(구글에서 처음 들어간 웹 페이지)를 의미하고, $p(s_0 = i)$는 그 첫 웹 페이지가 $i$일 확률을 의마한다. 사용자가 transition matrix $A$를 따라 랜덤으로 페이지의 링크를 따라간다고 가정한다. 이때, $t$번째 탐색에서 사용자가 각 페이지에 있을 확률분포 $\\pi_t$를 계산할 수 있다. \\pi_t = p(s_t) \\\\ = \\sum_{i} p(s_t | s_{t-1} = i) \\\\ = \\pi_{t-1} A 위 식이 이해가 잘 안된다면, $t=1$일때를 가정해보자. 첫 웹 페이지에서 첫 번째로 링크를 따라 들어가서 다음 페이지로 간다고 해 보자. 이때, 사용자가 랜덤으로 첫 웹 페이지를 선택하고 랜덤으로 첫 번째 링크를 따라간 뒤, 사용자가 각 웹 페이지에 있을 확률은 각각 다음과 같다. p(s_1=1) = \\sum_i p(s_1=1|s_0=i) \\\\ p(s_1 = 2) = \\sum_i p(s_1 = 2| s_0 = i) \\\\ p(s_1 = 3) = \\sum_i p(s_1 = 3 | s_0 = i) 얘네를 잘 관찰해보면, $\\pi_1 = p(s_1)$은 다음과 같음을 쉽게 알 수 있다. \\pi_1 = p(s_1) = \\pi_0 A 마찬가지로, 사용자가 첫 번째 링크를 따라온 상태라고 가정했을 때, 두 번째 링크를 따라가서 각 페이지에 있을 확률은 다음과 같다. \\pi_2 = \\pi_1 A = \\pi_0 A^2 이를 $t$번까지 확장해보면 다음과 같다. \\pi_t = \\pi_{t-1} A = \\pi_0 A^t 사용자가 랜덤으로 페이지 링크를 따라가는 과정을 무한번 반복한다고 가정해보자. 이때, 사용자가 각 웹 페이지에 있을 확률에 대한 분포 $\\pi_{\\infty}$는 다음과 같다. \\pi_{\\infty} = \\pi_{\\infty - 1} A $\\infty - 1$은 그냥 $\\infty$와 같다. 따라서, 다음이 성립한다. \\pi_{\\infty} = \\pi_{\\infty} A 즉, 무한번 링크를 따라가다보면, 사용자가 각 웹 페이지에 있을 확률분포는 더 이상 링크를 따라가봤자 변하지 않는다. 이때 $\\pi_{\\infty}$를 limiting distribution이라고 부른다. 구글이 각 페이지에 랭크를 매기는 방법은 바로 이 limiting distribution을 계산하는 것이다. 사용자가 임의로 웹 페이지를 링크를 따라 무한히 이동했을 때, 각 페이지에 있을 확률을 페이지의 랭크로 정의한다. 확률이 높다면, 랭크가 높은 것이고, 낮다면 랭크가 낮은 것이다. 이것은 어느정도 합리적인 구상이라고 볼 수 있는데, limiting distribution에서 높은 확률을 가진 페이지는 결국 방문 빈도가 많을 수 밖에 없고, 이 말은 그 페이지로 연결된 링크가 많다는 이야기이며, 인기가 많은 페이지라는 의미이기 때문이다. 선형대수학을 공부하신 분들이라면, limiting distribution의 식이 eigen decompostion과 모양이 같다는 것을 알 수 있을 것이다. 즉, limiting distribution을 계산하려면, eigen decomposition을 수행하면 되지 않을까 하는 생각을 가질 수 있다. 하지만, 불행히도, eigen decomposition을 통해 계산된 eigen vector는 limiting distribution이 맞을 수도 있고, 아닐 수도 있다. Eigen decomposition을 통해 계산된 eigen vector는 분명히 $A$를 더 곱해봤자 변하지 않는 벡터이다. 이 벡터가 확률분포를 만족한다면, 이 벡터를 stationary distribution이라고 부른다. Stationary distribution이 limiting distribution과 같다면, PageRank는 간단히 transition matrix를 eigen decomposition하는 것 만드로도 간단히(?) 계산할 수 있다. 물론, transition matrix의 크기가 장난 아니기 때문에 보통 문제가 아닐 수 있지만, Spark와 같은 분산처리 시스템이 있다면 해결될 문제이다. 하지만, 또 다른 문제가 있는데, eigen decomposition으로 얻을 수 있는 eigen value와 eigen vector 쌍은 최대 웹 페이지 개수만큼 존재할 것이라는 점이다($A$가 $n \\times n$ 차원이면, $n$개만큼의 eigen vector가 존재할 수도 있다). 하지만, 다음 섹션에서 논의할 Perron-Frobenius Theorem을 사용하면, eigen decomposition을 이용하여 계산된 stationary distribution이 limiting distribution과 같다는 것을 보장받을 수 있다. 또한, 이 이론은 모든 원소가 0보다 큰 eigen vector는 오직 1개밖에 존재하지 않음을 보장해준다. 이 말은, 확률분포가 될 자격이 있는 eigen vector가 eigen decomposition으로 얻을 수 있는 eigen vector들 중에 오직 1개밖에 없다는 것이다. 즉, eigen decomposition으로 계산된 그 유일한 eigen vector가 결국 limiting distribution과 같다는 것을 보장받을 수 있다는 의미이며, eigen decompositon 만으로 PageRank를 풀 수 있다는 이야기가 된다. Perron-Frobenius Theorem이 이론은 Markov chain의 transition matrix $A$가 다음 조건을 만족했을 때, $A$의 eigen decomposition 결과로 구한 stationary distribution이 limiting distribution과 같다는 것을 보장한다. $A$의 각 row는 적법한 probability distribution이어야 한다. 즉, $A$의 모든 row의 원소를 더했을 때, 모든 값은 1이 되어야 한다. $A$의 모든 원소는 반드시 0이 아닌 양의 값이어야 한다. 이 조건들은 위키백과에 나와있는 Perron-Frobenius Theorem과 조금 차이가 있는데, PageRank에 맞춰서 Perron-Frobenius Theorem을 해석한 위 조건을 사용하기로 한다. 즉, 위 조건들을 만족하면 $A$의 eigen decomposition만으로도 $A$의 limiting distribution을 계산할 수 있다는 의미가 된다. 앞에서 계속 논의하고 있던 Transition matrix $A$의 각 row는 합이 1이 나오도록 모델링을 한 상황이다. 하지만, 한 페이지가 엄청난 숫자의 페이지의 링크를 가지고 있는 경우는 없다고 봐야하므로, $A$의 원소는 0이 대부분이다. 따라서, Perron-Frobenius Theorem을 활용하려면 엄청난 숫자의 0을 다 양수로 만들어야 한다. 구글은 이를 위해서 transition matrix $A$에 smoothing이라는 작업을 하게 된다. 위키백과에 있는 원래의 Perron-Frobenius Theorem에서는 위 두가지 조건을 만족할 때, 모든 원소가 양의 실수인 eigen vector는 오직 1개만 존재한다는 것을 보장하고 있다. Transition Matrix SmoothingTransition matrix의 대부분이 0이 된다는 사실은 PageRank를 eigen decomposition으로 계산할 수 없게 만든다. 따라서, 구글은 transition matrix $A$를 그대로 사용하지 않고, $A$의 smoothing 버전을 대신 사용하게 된다. \\tilde{A} = 0.85 * A + 0.15 * U이때, $\\tilde{A}$는 smoothing이 적용된 transition matrix이고, $A$는 원래의 transition matrix이다. 그리고, $U$는 다음과 같이 정의한다. U = \\begin{pmatrix} 1/n & 1/n & \\cdots & 1/n \\\\ 1/n & 1/n & \\cdots & 1/n \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\end{pmatrix}$n$은 웹 페이지의 개수, 즉, $A$의 row 개수(또는 column 개수와도 같다)와 같다. 즉, 모든 페이지에 모든 페이지로 가는 링크 하나씩 박아놓은 transition matrix가 $U$이다. 이 $U$와 $A$를 weighted sum을 한 것이 $\\tilde{A}$가 되며, $\\tilde{A}$의 모든 원소는 0이 아닌 양수가 된다. 실제로 존재하지 않는 링크의 경우, 0이 아니라 매우 작은 양의 숫자로 만들어주는 것이 smoothing의 핵심이다. 이제, smoothing으로 Perron-Frobenius Theorem의 모든 조건을 만족시켰다. 즉, 구글의 검색 엔진 알고리즘인 PageRank 알고리즘을 transition matrix의 eigen decomposition만으로도 계산할 수 있다. PageRank Computation PageRank 알고리즘의 계산 과정을 정리하면 다음과 같다. Transition matrix $A$를 정의한다. Initial state distribution $\\pi_0$를 정의한다. Transition matrix $A$를 smoothing하여 $\\tilde{A}$를 계산한다. $\\tilde{A}$에 대해 eigen decomposition을 수행하고, 모든 원소가 양수인 eigen vector를 찾는다. 해당 eigen vector가 바로 각 웹 페이지의 랭크를 표현하는 PageRank이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"RecommenderSystems","slug":"RecommenderSystems","permalink":"https://wayexists02.github.io/tags/RecommenderSystems/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Recommender Systems","slug":"Study-Notes/Recommender-Systems","permalink":"https://wayexists02.github.io/categories/Study-Notes/Recommender-Systems/"}]},{"title":"Appendix 3. Variational Inference","date":"2020-09-14T12:08:16.000Z","path":"studynotes/bayesian-statistics/APPENDIX-3-Variational-Inference/","text":"Summary Latent variable 또는 hidden variable의 posterior approximation을 위한 optimization 기반의 variational inference 방법론 및 응용 분야 소개 Motivations of VI Hidden variable(latent variable, mixture variable) zzz의 posterior density p(z∣x)p(z|x)p(z∣x)를 계산하기 쉽지 않거나, 불가능에 가까울 경우에 사용하는 방법으로 MCMC(Markov chain Monte Carlo) 방법(approximation)이 존재했다. Stationary distribution이 posterior p(z∣x)p(z|x)p(z∣x)인 Markov chain을 모델링하고 그 체인으로부터 얻은 샘플들로 posterior p(z∣x)p(z|x)p(z∣x)에 대한 empirical(경험적) 추정을 하는 것이 기존의 방법이었다. 하지만, MCMC 방법은 매우 느려서 복잡한 모델이나 빨리 inference 를 수행해야 하는 작업에는 적합하지 못했다. VI(Variational inference)는 MCMC만큼 정확하지는 않지만, 나름 큰 스케일의 문제에 적용이 가능한 대안으로 제시되었다. VI는 MCMC와 같은 approximation 방법이지만, 샘플링 기반 방법이 아닌 optimization 기반의 방법이다. Variational Inference Variational inference는 다음과 같은 문제를 푸는 것이다. q∗(z)=argmin q(z)∈QKL(q(z)∣∣p(z∣x))q^*(z) = \\underset{q(z) \\in Q}{\\text{argmin }} \\text{KL}(q(z)||p(z|x)) q∗(z)=q(z)∈Qargmin ​KL(q(z)∣∣p(z∣x)) Goal: Posterior p(z∣x)p(z|x)p(z∣x)와 매우 근접한 approximation q(z)q(z)q(z)를 찾는 것 p(z∣x)p(z|x)p(z∣x)에 매우 근사하는 함수 q∗(z)q^*(z)q∗(z)를 찾기 위해 q(z)q(z)q(z)를 parameterize한 후, q(z)q(z)q(z)를 위 식에 맞추어 최적화하게 된다. 이에따라, q(z)q(z)q(z)는 적절히 flexible하게 parameterize되어야 하며, true posterior p(z∣x)p(z|x)p(z∣x)를 잘 추정하도록 충분히 간단한 함수여야 한다. Problem of Bayesian Inference Bayesian inference에서는 어떤 파라미터 θ\\thetaθ의 분포를 추정하고자 할 때, likelihood p(x∣θ)p(x|\\theta)p(x∣θ)와 parameter에 대한 prior p(θ)p(\\theta)p(θ)를 이용하여 posterior p(θ∣x)p(\\theta|x)p(θ∣x)를 계산하게 된다. 이 posterior가 파라미터의 density에 대한 추정이 된다. 그러나, p(θ∣x)p(\\theta|x)p(θ∣x)를 계산하기 위해서는 p(θ∣x)=p(x∣θ)p(θ)p(x)p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)} p(θ∣x)=p(x)p(x∣θ)p(θ)​ 를 계산해야 하는데, p(x)p(x)p(x)는 보통의 경우 계산이 불가능하다. 따라서, 일반적인 Bayesian inference(위 식을 푸는 것)로는 posterior를 추정할 수가 없는 경우가 많고, 이를 해결하기 위해 MCMC나 VI가 존재한다. VI에서는 p(θ∣x)p(\\theta|x)p(θ∣x)를 계산이 가능한 어떤 다른 함수 q(θ)q(\\theta)q(θ)로 근사시키고, q(θ)q(\\theta)q(θ)를 대신 사용하게 된다. VI vs MCMC MCMC와 VI는 같은 문제를 푸는 inference 알고리즘이다. 둘 다 true posterior를 계산하는게 힘들때 사용되며, approximation을 통해 true posterior를 추정한다. 다만, MCMC는 샘플링을 통해 추정하고 VI는 optimization을 통해 추정한다. MCMC VI에 비해 posterior 추정이 정확한 편 복잡한 모델일수록, 대규모 데이터셋을 사용할수록 적용하기가 어려움(느림) Target distribution(보통은 posterior를 가리킴)으로부터 상당히 정확한(진짜 그 distribution에서 샘플링한 듯한) 샘플을 얻을 수 있음 VI MCMC에 비해 posterior 추정이 부정확할 수 있음 Optimization 방법이라서 복잡하고 큰 문제(큰 데이터셋)에 잘 작동함(빠름) Target distribution의 density만 추정할 뿐, 샘플을 얻을 수는 없음 이러한 특성 때문에 MCMC와 VI의 사용 용도는 미세하게 다르다. MCMC는 density를 시뮬레이션하는 용도이고, VI는 density를 추정하는 용도이다. Evidence Lower Bound Variational inference에서는 다음과 같은 q∗(z)q^*(z)q∗(z)를 찾는 것이다. q∗(z)=argmin q(z)∈QKL(q(z)∣∣p(z∣x))q^*(z) = \\underset{q(z) \\in Q}{\\text{argmin }} \\text{KL}(q(z)||p(z|x)) q∗(z)=q(z)∈Qargmin ​KL(q(z)∣∣p(z∣x)) 하지만, 이 식 역시 true posterior p(z∣x)p(z|x)p(z∣x)가 있기 때문에 계산할 수가 없다. argmin 안의 KL divergence식을 조금 변형해볼 것이다. KL(q(z)∣∣p(z∣x))=Ez∼q(z)[log q(z)−log p(z∣x)]=Ez∼q(z)[log q(z)−log p(x,z)+log p(x)]\\text{KL}(q(z)||p(z|x)) = \\mathbb{E}_{z \\sim q(z)}[\\text{log } q(z) - \\text{log } p(z|x)] \\\\ = \\mathbb{E}_{z \\sim q(z)}[\\text{log } q(z) - \\text{log } p(x,z) + \\text{log }p(x)]KL(q(z)∣∣p(z∣x))=Ez∼q(z)​[log q(z)−log p(z∣x)]=Ez∼q(z)​[log q(z)−log p(x,z)+log p(x)] 이때, KL divergence는 항상 0보다 크거나 같으므로, 다음이 성립한다. Ez∼q(z)[log q(z)−log p(x,z)+log p(x)]≥0\\mathbb{E}_{z \\sim q(z)}[\\text{log } q(z) - \\text{log } p(x,z) + \\text{log }p(x)] \\geq 0 Ez∼q(z)​[log q(z)−log p(x,z)+log p(x)]≥0 그리고, evidence p(x)p(x)p(x)는 zzz와 무관하므로 밖으로 나올 수 있다. p(x)p(x)p(x)를 제외한 나머지 식을 우변으로 이항해보면 다음과 같은 식이 만들어진다. log p(x)≥Ez∼q(z)[log p(x,z)−log q(z)]=ELBO(q)\\text{log }p(x) \\geq \\mathbb{E}_{z \\sim q(z)}[\\text{log } p(x,z) - \\text{log } q(z)] = \\text{ELBO}(q) log p(x)≥Ez∼q(z)​[log p(x,z)−log q(z)]=ELBO(q) 이때, 우변의 식은 log evidence의 lower bound가 된다. 우변의 식을 evidence lower bound라고 부르며, ELBO라고도 부른다. 한편, 다시 최적화 식으로 돌아와서 보면, q∗(z)=argmin q(z)∈QKL(q(z)∣∣p(z∣x))q^*(z) = \\underset{q(z) \\in Q}{\\text{argmin }} \\text{KL}(q(z)||p(z|x)) q∗(z)=q(z)∈Qargmin ​KL(q(z)∣∣p(z∣x)) 이 식을 풀면 다음과 같다. q∗(z)=argmin q∗Ez∼q(z)[log q(z)−log p(x,z)+log p(x)]q^*(z) = \\text{arg}\\underset{q^*}{\\text{min }} \\mathbb{E}_{z \\sim q(z)}[\\text{log } q(z) - \\text{log } p(x,z) + \\text{log }p(x)] q∗(z)=argq∗min ​Ez∼q(z)​[log q(z)−log p(x,z)+log p(x)] 그리고, evidence p(x)p(x)p(x)는 zzz와 무관한 항이므로 최적화에서 제외할 수 있다. q∗(z)=argmin q∗Ez∼q(z)[log q(z)−log p(x,z)]=argmin q∗−ELBO(q)q^*(z) = \\text{arg}\\underset{q^*}{\\text{min }} \\mathbb{E}_{z \\sim q(z)}[\\text{log } q(z) - \\text{log } p(x,z)] \\\\ = \\text{arg}\\underset{q^*}{\\text{min }} -\\text{ELBO}(q)q∗(z)=argq∗min ​Ez∼q(z)​[log q(z)−log p(x,z)]=argq∗min ​−ELBO(q) 따라서, variational inference는 ELBO를 최대화하는 optimization을 푸는 문제로 approximation할 수 있다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"16. Lieklihood for Mixture Models","date":"2020-08-04T03:00:00.000Z","path":"studynotes/bayesian-statistics/16-Likelihood-for-Mixture-Models/","text":"16. Likelihood for Mixture Models Mixture model을 정의했으니, 이제 mixture model로 어떤 일을 할 수 있는가를 알아볼 차례이다. Mixture model은 데이터의 생성 프로세스를 모델링하는 데 사용된다. 즉, 데이터를 fitting하는 데 사용된다. 데이터를 생성하는 분포를 모델링한 것을 likelihood distribution이라고 부른다. Mixture model의 fitting은 이 likelihood distribution에서 학습 데이터를 넣었을 때 얻어진 likelihood를 최대화하는 방향으로 이루어진다. Hierarchical Representation Mixture model은 다음처럼 표현이 가능했다. f(x)=∑k=1Kωkgk(x)f(x) = \\sum_{k=1}^K \\omega_k g_k(x) f(x)=k=1∑K​ωk​gk​(x) KKK개의 mixture component가 있고, 데이터 xxx는 KKK개중 하나의 component gk(x)g_k(x)gk​(x)로부터 생성된다는 의미의 mixture model이다. 이렇게 바로 표현할 수도 있지만, mixture model이 복잡해지면 이렇게 바로 표현하는 것은 다른사람이 보기에 이해하기도 힘들고, 직관적이지 못할 수도 있다. Mixture model을 표현하는 또 다른 방법은 hierarchical하게 표현하는 것이다. xi∣ci∼gcici∼(ω1,ω2,...,ωK)x_i|c_i \\sim g_{c_i} \\\\ c_i \\sim (\\omega_1, \\omega_2, ..., \\omega_K)xi​∣ci​∼gci​​ci​∼(ω1​,ω2​,...,ωK​) 여기서, ccc는 KKK개중에서 어떤 mixture component를 선택할 것인지에 대한 random variable이고, gcg_cgc​는 ccc번째의 mixture component이다. 이때, ccc번째 mixure component를 선택할 확률은 ωc\\omega_cωc​이다. 이 표현법으로 f(x)f(x)f(x)를 계산해보면 다음과 같다. f(x)=∑k=1KP(x∣ck)P(ck)=∑k=1Kgk(x)ωkf(x) = \\sum_{k=1}^K P(x|c_k)P(c_k) \\\\ = \\sum_{k=1}^K g_k(x) \\omega_kf(x)=k=1∑K​P(x∣ck​)P(ck​)=k=1∑K​gk​(x)ωk​ 즉, 맨 처음 식과 일치한다. 이처럼, hierarchical representation은 mixture model의 또 다른 representation이며, 같은 mixture model에 대해 더 편리한 표현법을 제공해준다. Sampling from a Mixture Models Mixture model에서 데이터를 샘플링하기 위해서는(데이터를 생성하기 위해서는) 다음과 같은 과정을 거치면 된다. Mixture model은 hierarchical representation으로 표현되어야 한다. 다음과 같은 mixture model이 있다고 가정해보자. xi∣c∼gcc∼(ω1,ω2,...,ωK)x_i|c \\sim g_c \\\\ c \\sim (\\omega_1, \\omega_2, ..., \\omega_K)xi​∣c∼gc​c∼(ω1​,ω2​,...,ωK​) (ω1,ω2,...,ωK)(\\omega_1, \\omega_2, ..., \\omega_K)(ω1​,ω2​,...,ωK​)의 distribution으로부터 cic_ici​를 샘플링한다. cic_ici​에 해당하는 mixture component gci(x)g_{c_i}(x)gci​​(x)로부터 xix_ixi​를 샘플링한다. Likelihood Functions Mixture model은 데이터가 생성되었을 법한 likelihood 모델을 구성하고 그 모델의 파라미터를 학습하는 것을 목적으로 한다. 그러기 위해서는 먼저, 데이터를 수집하고 관찰한다. 그리고, 데이터가 샘플링되어 나왔을 법한 likelihood를 모델링해야 한다. Mixutre model을 사용하여 모델링할 수 있는 likelihood에는 두 가지가 존재한다. Observed data likelihood Complete data likelihood Observed Data Likelihood 데이터(observation)가 x1,x2,...,xnx_1, x_2, ..., x_nx1​,x2​,...,xn​와 같이 존재한다고 하고, 각 데이터 xix_ixi​는 같은 distribution에서 왔으며, 독립된 조건 속에서 샘플링되었다고 하자(independent identical distributed; iid). 그리고 데이터 샘플 하나는 다음과 같은 샘플링 과정을 거쳤다고 모델링했다고 해 보자. f(xi)=∑k=1Kωkgk(xi)f(x_i) = \\sum_{k=1}^K \\omega_k g_k(x_i) f(xi​)=k=1∑K​ωk​gk​(xi​) 이때, likelihood는 모든 data 확률의 joint distribution이므로, 다음과 같이 정의할 수 있다. L(ω)=∏i=1nf(xi)=∏i=1n∑k=1Kωkgk(xi)L(\\omega) = \\prod_{i=1}^n f(x_i) = \\prod_{i=1}^n \\sum_{k=1}^K \\omega_k g_k(x_i) L(ω)=i=1∏n​f(xi​)=i=1∏n​k=1∑K​ωk​gk​(xi​) 그러나, 이 형태는 multiplication과 summation이 공존하고, 계산이 다소 힘들다는 단점이 있다. 이 likelihood를 observed data likelihood라고 부르는데, 우리가 이상적으로 학습하기 원하는 likelihood는 observed data likelihood지만, 계산의 복잡성 때문에 latent variable을 추가하여 식을 단순화한 complete data likelihood를 보통 사용하게 된다. Complete Data Likelihood Observed data likelihood에 latent variable을 추가하여 식을 구성한 likelihood를 말한다. 다음과 같이 데이터가 생성된 과정이 hierarchical model로 모델링되었다고 가정해보자. xi∣ci∼gcici∼(ω1,ω2,...,ωK)x_i|c_i \\sim g_{c_i} \\\\ c_i \\sim (\\omega_1, \\omega_2, ..., \\omega_K)xi​∣ci​∼gci​​ci​∼(ω1​,ω2​,...,ωK​) 이때, complete data likelihood는 다음과 같다. L(ω,c)=∏i=1nf(xi,ci)=∏i=1n∏k=1K[gk(xi)ωk]I(ci=k)L(\\omega, c) = \\prod_{i=1}^n f(x_i, c_i) = \\prod_{i=1}^n \\prod_{k=1}^K [g_k(x_i) \\omega_k]^{\\mathbb{I}(c_i=k)} L(ω,c)=i=1∏n​f(xi​,ci​)=i=1∏n​k=1∏K​[gk​(xi​)ωk​]I(ci​=k) 즉, observed data likelihood에서는 한 개의 데이터 샘플에 대해 likelihood는 다음과 같았는데, P(xi)=f(xi)=∑k=1Kωkgk(xi)P(x_i) = f(x_i) = \\sum_{k=1}^K \\omega_k g_k(x_i) P(xi​)=f(xi​)=k=1∑K​ωk​gk​(xi​) Complete data likelihood에서는 한 개의 데이터 샘플에 대해 likelihood는 다음과 같다. P(xi,ci)=f(xi,ci)=ωcigci(xi)P(x_i, c_i) = f(x_i, c_i) = \\omega_{c_i} g_{c_i}(x_i) P(xi​,ci​)=f(xi​,ci​)=ωci​​gci​​(xi​) Parameter Identifiability 보통 distribution은 파라미터가 다르면 모양이 서로 다른 distribution이 된다. 예를 들어, 다음과 같은 두 개의 normal distribution은 서로 다른 모양의 distribution이 된다. f1(x)=12πexp{−x22}f2(x)=122πexp{−12(x−12)2}f_1(x) = \\frac{1}{\\sqrt{2\\pi}} \\text{exp}\\{ - \\frac{x^2}{2} \\} \\\\ f_2(x) = \\frac{1}{2\\sqrt{2\\pi}} \\text{exp}\\{ -\\frac{1}{2} (\\frac{x - 1}{2})^2 \\}f1​(x)=2π​1​exp{−2x2​}f2​(x)=22π​1​exp{−21​(2x−1​)2} 이렇게 파라미터로 두 개의 distribution을 구분할 수 있는 성질을 parameter identifiability라고 부른다. 하지만, 이 성질은 mixture model에는 적용되지 않는 경우가 있다. 즉, 파라미터가 서로 달라도 완전히 일치하는 모양이 나올 수도 있다. 이러한 경우는 세 가지로 나눌 수 있다. Label switching 현상 Component split 현상 0-weighted component 현상 Mixture model을 선택할 때는 위 세가지 현상을 주의해야 한다. 똑같은 데이터를 모델링했고, 서로 다른 파라미터를 얻었는데, 모양이 거의 같은 모델을 얻을 수도 있다. 이때는 모델의 mixture component를 줄일 수 없는지도 살펴보아야 하며, 순서만 바뀌지 않았는지 잘 분석해야 한다. Label Switching 다음의 두 개의 distribution은 파라미터가 서로 다르지만, 모양이 완전히 같은 분포이다. f1(x)=0.7⋅N(0,12)+0.3⋅N(1,22)f2(x)=0.3⋅N(1,22)+0.7⋅N(0,12)f_1(x) = 0.7 \\cdot \\mathbb{N}(0, 1^2) + 0.3 \\cdot \\mathbb{N}(1, 2^2) \\\\ f_2(x) = 0.3 \\cdot \\mathbb{N}(1, 2^2) + 0.7 \\cdot \\mathbb{N}(0, 1^2)f1​(x)=0.7⋅N(0,12)+0.3⋅N(1,22)f2​(x)=0.3⋅N(1,22)+0.7⋅N(0,12) 얼핏 보면 그냥 파라미터도 똑같은 것처럼 보이지만, 실제로는 다음과 같이 서로 다른 파라미터인 경우가 있다. {ω1=0.7,μ1=0,σ1=1,ω2=0.3,μ2=1,σ2=2ω1=0.3,μ1=1,σ1=2,ω2=0.7,μ2=0,σ2=1\\begin{cases} \\omega_1 = 0.7, \\mu_1 = 0, \\sigma _1= 1, \\omega_2 = 0.3, \\mu_2 = 1, \\sigma_2 = 2 \\\\ \\omega_1 = 0.3, \\mu_1 = 1, \\sigma _1= 2, \\omega_2 = 0.7, \\mu_2 = 0, \\sigma_2 = 1 \\end{cases}{ω1​=0.7,μ1​=0,σ1​=1,ω2​=0.3,μ2​=1,σ2​=2ω1​=0.3,μ1​=1,σ1​=2,ω2​=0.7,μ2​=0,σ2​=1​ 즉 파라미터가 완전히 스위칭되어 학습된 경우이다. 이럴때는 파라미터가 다르더라도 완전히 모양이 같은 분포가 된다. 이런 현상을 막기 위해 첫 번째 라벨에 대한 평균(μ1\\mu_1μ1​)은 두 번째 라벨에 대한 평균 μ2\\mu_2μ2​보다 작아야 한다거나 하는 constraint가 필요할 수도 있다. Component Split 다음 두 개의 distribution은 파라미터가 서로 다르지만, 모양이 완전히 같은 분포이다. f1(x)=0.7⋅N(0,12)+0.3⋅N(1,22)f1(x)=0.7⋅N(0,12)+0.1⋅N(1,22)+0.2⋅N(1,22)f_1(x) = 0.7 \\cdot \\mathbb{N}(0, 1^2) + 0.3 \\cdot \\mathbb{N}(1, 2^2) \\\\ f_1(x) = 0.7 \\cdot \\mathbb{N}(0, 1^2) + 0.1 \\cdot \\mathbb{N}(1, 2^2) + 0.2 \\cdot \\mathbb{N}(1, 2^2)f1​(x)=0.7⋅N(0,12)+0.3⋅N(1,22)f1​(x)=0.7⋅N(0,12)+0.1⋅N(1,22)+0.2⋅N(1,22) 이것은 흔히 over-estimate때문에 일어나는 현상인데, 실제로는 두 개의 component mixture만으로도 데이터의 distribution을 잡는 데 충분하지만, 세 개의 component로 over-estimate 하려고 하면 이런 현상이 발생할 수 있다. 0-weighted Component 다음 두 개의 distribution은 파라미터가 서로 다르지만, 모양이 완전히(거의) 같은 분포이다. f1(x)=0.7⋅N(0,12)+0.3⋅N(1,22)f1(x)=0.7⋅N(0,12)+0.2999⋅N(1,22)+0.0001⋅N(1,22)f_1(x) = 0.7 \\cdot \\mathbb{N}(0, 1^2) + 0.3 \\cdot \\mathbb{N}(1, 2^2) \\\\ f_1(x) = 0.7 \\cdot \\mathbb{N}(0, 1^2) + 0.2999 \\cdot \\mathbb{N}(1, 2^2) + 0.0001 \\cdot \\mathbb{N}(1, 2^2)f1​(x)=0.7⋅N(0,12)+0.3⋅N(1,22)f1​(x)=0.7⋅N(0,12)+0.2999⋅N(1,22)+0.0001⋅N(1,22) 이러한 현상 역시, mixture component를 많이 잡았을 때 발생할 수 있다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"15. Definition of Mixture Models","date":"2020-08-01T06:26:00.000Z","path":"studynotes/bayesian-statistics/15-definition-of-mixture-models/","text":"15. Definition of Mixture ModelsNormal distribution, poisson distribution, exponential distribution 등 여러가지 기본 distribution들로 모델링이 가능한 데이터가 있고, 그렇지 않은 데이터가 있다. 예를 들어, 남자와 여자의 키의 분포를 생각해보면, 여자의 키는 160근처에서 평균을 이루면서 normal distribution을 이루고, 남자의 분포는 175쯤에서 평균을 이루면서 normal distribution을 이룬다고 해 보자. 이때, 사람들의 키 분포는 두개의 봉우리가 있는 multi-modal distribution이 되고, 일반적인 distribution으로는 모델링이 불가능하다. 데이터에 사실상 두 개의 population이 존재하기 때문이다. 이번 section에서는 이러한 상황에서 데이터를 모델링하는 데 사용할 수 있는 도구인 mixture model에 대해서 다룬다. Definition of Mixture ModelsMixture model은 여러개의 population이 합친 데이터를 모델링할 수 있는 도구이다. 하나의 population에 하나의 distribution을 모델링한 후, 각 distribution을 weighted sum한 것이 mixture model이다. Mixutre model의 probability density(또는 probability mass) function $f(x)$는 다음과 같이 정의한다. f(x) = \\sum_{k=1}^K \\omega_k g_k(x)이 mixture model에는 $K$개의 분포 $g_k(x)$를 합친 형태이며, 이 분포들을 $\\omega_k$로 weighted sum한 형태이다. 이때, 다음과 같은 제약조건이 붙는다. \\sum_{k=1}^K \\omega_k = 1모든 $gk(x)$가 올바른 probability density라고 할때($\\int{-\\infty}^{\\infty} g_k(x) dx = 1$), 위 제약조건이 있으면 다음을 만족할 수 있게 된다. \\int_{-\\infty}^{\\infty} f(x) dx = 1가장 간단한 예제 중 하나인 사람들의 키 분포를 예로 들어보자. 사람들은 남자와 여자 두 가지 성별로 그룹핑할 수 있으며, 남자의 키 분포를 다음과 같다고 가정해보자. g_{ \\text{male} }(x) = \\frac{1}{ \\sigma \\sqrt{2 \\pi } } \\text{exp} \\{ -\\frac{(x - \\mu_{ \\text{male} })^2}{2 \\sigma^2 } \\}그리고, 여자의 키 분포를 다음과 같다고 가정해보자. g_{ \\text{female} }(x) = \\frac{1}{ \\sigma \\sqrt{ 2 \\pi} } \\text{exp}\\{ -\\frac{(x - \\mu_{ \\text{female} })^2}{2 \\sigma^2 } \\}그럼, 사람들의 키 분포는 다음과 같이 표현할 수 있다. f_{\\text{height}}(x) = \\omega_{\\text{male}}g_{\\text{male}}(x) + \\omega_{\\text{female}}g_{\\text{female}}(x)이 예제에서는 남, 여의 키 분포의 분산 $\\sigma^2$이 같다고 가정했다. 그러나, $\\sigma$도 다르게 둘 수도 있다. Expectation of Mixture Models분포의 기댓값은 분포의 특성을 파악하는 데 있어서 가장 중요한 statistics 중 하나이다. Mixture model의 기댓값은 다음과 같이 정의될 수 있다. \\mathbb{E} [X] = \\int_{ -{\\infty} }^{ \\infty } x f(x) dx이때, $f(x)$를 mixture density로 치환해보면 다음과 같다. \\mathbb{E} [X] = \\int_{ -\\infty }^{ \\infty }x \\sum_{k=1}^K \\omega_k g_k(x) dx \\\\ = \\sum_{k=1}^K \\omega_k \\int_{ -\\infty }^{ \\infty }x g_k(x) dx \\\\ = \\sum_{k=1}^K \\omega_k \\mathbb{E}_g [X]즉, mixture density의 기댓값은 각 구성원 분포 기댓값의 weighted sum과 같다. Variance of Mixture Models분포의 분산또한 평균과 마찬가지로 분포의 특성을 파악하는데 아주 중요하다. Mixture model의 분산은 다음과 같다. Var[X] = \\int_{-\\infty}^{\\infty}(x - \\mathbb{E}[X])^2 f(x) dx이때, 다시 $f(x)$를 mixture density 식으로 치환해보자. Var_f [X] = \\int_{-\\infty}^{\\infty}(x - \\mathbb{E}[X])^2 \\sum_{k=1}^k\\omega_k g_k(x) dx \\\\ = \\sum_{k=1}^k \\omega_k \\int_{-\\infty}^{\\infty} (x - \\mathbb{E}[X])^2 g_k(x) dx \\\\ = \\sum_{k=1}^k \\omega_k \\int_{-\\infty}^{\\infty} (x^2 g_k(x) - 2x\\mathbb{E}[X] g_k(x) + (\\mathbb{E}[X])^2 g_k(x)) dx \\\\ = \\sum_{k=1}^k \\omega_k \\mathbb{E}[X^2] - 2\\mathbb{E}[X]\\sum_{k=1}^k \\omega_k \\int_{-\\infty}^{\\infty} x g_k(x) + (\\mathbb{E}[X])^2 \\sum_{k=1}^k \\omega_k \\int_{-\\infty}^{\\infty} g_k(x) dx \\\\ = \\sum_{k=1}^k \\omega_k \\mathbb{E}[X^2] - 2 (\\mathbb{E}[X])^2 + (\\mathbb{E}[X])^2 \\\\ = \\sum_{k=1}^k \\omega_k \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\\\ = \\sum_{k=1}^k \\omega_k [Var_{g_k}[X] + (\\mathbb{E}_{g_k} [X])^2] - (\\mathbb{E}[X])^2Applications of Mixture Models가장 자주 사용되는 application으로는 다음과 같은 경우가 있다. 데이터가 multi-modal인 경우 Multi-modal인 경우에는 modal 수만큼 density를 mixture하는 경우가 많다. 0-inflated distribution(0이 크게 솟아있는 경우)인 경우도 0인 경우와 0이 아닌 경우로 distribution을 mixture하게 된다. 이때, 0인 경우는 point-mass function(dirac delta)을 사용한다. 데이터가 skewed 형태인 경우 데이터가 heavy tail을 가진 normal 형태인 경우","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"01. Introduction","date":"2020-06-15T01:00:00.000Z","path":"studynotes/reinforcement-learning/01_Introduction/","text":"Introduction 참고: Coursera Reinforcement Learning (Alberta Univ.) 강화학습(Reinforcement Learning) Reinforcement Learning이란, 환경(environment)가 있고, 그 environment 속에서 agent가 자신이 처한 상태(state)에서 가장 최적의 행동(action)을 취할 수 있도록 하는 알고리즘들을 말한다. Agent는 어떤 행위를 취하는 객체로, 자신에게 주어진 상태에서 액션을 취할 수 있고, 그 액션에 의해 environment가 영향을 받는다. 그에 따라 agent에게 주어지는 상태가 변화하고, environment로부터 reward 또는 피드백을 받게 된다. Reinforcement learning을 바탕으로 학습된 agent는 주어진 environment와 state를 바탕으로 최적의 액션을 취할 수 있다. Reinforcement learning은 현재 주어진 state에서 어떤 action을 취한다면, 어떤 reward를 얻을 것인지를 예측한다. 그리고 예측한 reward가 최대가 되는 action을 선택하게 된다. Elements of Reinforcement Learning Reinforcement learning에는 여러가지 중요한 요소들이 존재한다. 우선, reinforcement learning은 어떤 행위를 하는 agent를 학습하는 것이다. 즉, agent가 존재한다. 또한, agent는 action을 취할 수 있다. Agent는 아무 조건없이 action을 취할 수는 없다. Agent는 자신에게 주어진 상태(state or environmnet)가 있고, 그 environment를 바탕으로 action을 취하고 reward(또는 피드백)을 얻는다. Agent 어떤 주어진 environment 속에서 environment와 상호작용하며 action을 취하는 행위 당사자 Environment Agent가 직접 상호작용하는 주위 환경. 보통 agent는 environment의 일부분을 관찰하게 되며, 전체 environment는 모른다고 가정한다. State Agent가 environment와 상호작용하면서 자신이 처한 상태. Agent는 environment 전체를 볼 수는 없고, environment의 일부분인 state를 볼 수 있다. 그 state를 바탕으로 action을 취할 수 있다. Action Agent가 자신에게 주어진 state를 바탕으로 취할 수 있는 행동 Reward Agent가 주어진 state에서 action을 취했을 때 environment로부터 얻은 피드백. (+)일수도 있고 (-)일수도 있다. Agent가 action을 선택하는 방법은, 주어진 state에서 어떤 action을 취했을 때 얻을 수 있는 모든 미래의 reward의 기댓값을 계산하고 그 미래 reward 기댓값이 최대가 되는 action을 선택하는 것이다. 이때, agent가 미래에서 얻을 수 있는 모든 reward들의 기댓값을 value라고 정의한다. Agent는 계산한 value를 바탕으로 어떤 action을 취할지에 대한 policy를 가진다. Agent는 value와 policy를 바탕으로 action을 취할 수 있다. Policy 어떤 주어진 환경/상황에서 어떤 동작을 취해야 하는지에 대한 규칙 또는 정책, 또는 매핑 함수이다. 강화학습에서 핵심 역할을 하며, 단순한 매핑 테이블일수도, 아주 복잡한 함수나 확률적인 모델일 수도 있다. 현재의 시간을 ttt, 현재 시간에서 agent에게 주어진 상태를 sts_tst​라고 한다. 그리고, 취할 수 있는 액션을 ata_tat​라고 해 보자. 그럼, policy π\\piπ는 다음처럼 정의될 수 있다. Input: sts_tst​ (current state), ata_tat​ (possible action at current time ttt) Output: preference\\text{preference}preference (preference of action ata_tat​) preference=π(st,at)\\text{preference} = \\pi(s_t, a_t) preference=π(st​,at​) preference\\text{preference}preference는 현재 상태 sts_tst​에서 어떤 action ata_tat​를 취할지에 대한 선호도를 나타낸다. sts_tst​에서 가능한 action ata_tat​가 여러개가 존재할 수 있는데, π(st,at)\\pi(s_t, a_t)π(st​,at​)가 가장 높은 액션 at′a_t&#x27;at′​를 취할 수 있을 것이다. Value &amp; Value Function Reward가 매 action마다 얻을 수 있는 것이라면, value는 미래에 가능한 action들을 취해봤을 때 얻을 수 있는 reward들의 누적 합의 기댓값을 의미한다. 현재 시간을 ttt라고 해 보자, 현재 상태 이후부터 얻을 수 있는 reward의 합 QtQ_tQt​는 다음처럼 말할 수 있다. Qt=∑i=t+1∞ri=rt+1+rt+2+⋯Q_t = \\sum_{i=t+1}^{\\infty} r_i = r_{t+1} + r_{t+2} + \\cdots Qt​=i=t+1∑∞​ri​=rt+1​+rt+2​+⋯ Qt=rt+1+Qt+1Q_t = r_{t+1} + Q_{t+1} Qt​=rt+1​+Qt+1​ Value VtV_tVt​란, 현재 상태 sts_tst​가 주어지고, agent가 action을 취할 수 있는 policy π\\piπ가 주어졌을 때, 이 미래 reward의 누적값인 QtQ_tQt​의 기댓값이다. Input: sts_tst​ (current state), π\\piπ Output: VtV_tVt​ (value) Vt=v(st,π)=E[Qt]V_t = v(s_t, \\pi) = \\mathbb{E}[Q_t] Vt​=v(st​,π)=E[Qt​] 이때, value VtV_tVt​를 계산해 주는 함수 vvv를 value function이라고 부른다. Reinforcement learning은 당장의 reward rt+1r_{t+1}rt+1​보다는 value VtV_tVt​를 최대화하는 action을 선택하도록 해야 한다. Value function은 environment의 특성에 맞게 두 가지의 구현방법이 있다. Tabular solution method 가능한 state의 수와 가능한 action 수가 셀수 있는 경우에 사용한다. 이 경우, value function을 단순한 mapping table로도 표현이 가능하다(그래서 이름도 tabular method라고 부른다). Approximate solution method 가능한 state의 수가 무수히 많은 경우에 사용할 수 있다. 이때, value function의 입력 도메인(state)이 무한하므로 table로는 value function을 표현할 수 없고, function approximation을 사용한다. 보통 neural network를 approximator로 사용한다. Reinforcement learning에서 가장 중요한 개념은 value function과 policy라고 볼 수 있다. Reinforcement learning 알고리즘들은 value function을 계산하고 그로부터 최적의 policy를 이끌어내는 과정으로 이루어지기 때문이다. Difference with Supervised Learning Reinforcement learning은 결국, value를 예측하는 문제라고 볼 수도 있지 않을까? 주어진 상태에서 supervised learning 알고리즘을 사용하여 모든 액션에 대한 value를 예측하고 value값이 가장 큰 action을 취하면 되지 않을까? 하지만, reinforcement learning은 다음과 같은 차이점이 있다. 정답 라벨이 바로 주어지지 않는다. (delayed reward) Reinforcement learning에서 정답 라벨은 주어질 수 있다. 그러나, 바로 주어지지 않는다. 일단 action을 취해봐야 비로소 environment로부터 reward를 얻을 수 있다. 시간이라는 개념이 존재한다. Supervised learning의 경우, 시간이라는 개념이 보통 존재하지 않는다(항상 그렇지는 않지만). 존재한다고 하더라도, 일정 시간동안의 데이터는 미리 주어지며, 실시간으로 학습하지 않는다. 반면, reinforcement learning은 실시간으로 학습하면서 액션을 취한다. 데이터가 매우 비정형적이다. Reinforcement learning에서의 데이터는 dataframe이나 이미지로 끝나지 않고, 여러가지 센서(사람으로 치면 시각, 청각 등등)가 있을 수 있다. 데이터가 고정되지 않는다. Supervised learning은 미리 데이터가 주어지고, 정답 라벨이 주어진다. 그리고 그 데이터들로 학습을 하는데, reinforcement learning은 실시간으로 environment와 상호작용하면서 얻은 정보들로 학습해야 한다. Trials and errors Reinforcement learning에서는 시행착오를 통해 학습한다. 데이터가 주어지지 않은 채로 agent가 액션을 취하며, 그 액션을 통해 얻은 reward로 학습한다. 가능한 state의 수와 가능한 action 수가 너무 많다. 사실상 supervised learning으로는 계산이 불가능할 수도 있다. Types of Reinforcement Learning Reinforcement learning은 두 가지 타입으로 나눌 수 있다. Episodic task Continuous task Episodic task란, 흔히 게임 같은 환경을 말한다. 시작과 끝이 존재한다. 게임으로 치면 게임 시작이 있고, 게임의 끝이 있다. 이때, 한 판의 게임을 episode라고 부른다. Continuous task란, 시작과 끝이 없는 task를 의미한다. 현실 세계에서의 로봇이 그렇다. 한번 동작하면 끊임없이 environment와 통신한다. Unusual &amp; Unexpected Stretagy in RL Reinforcement learning은 최종 value를 최대화하면서 학습한다. 그리고, 최종 value를 가장 높게 하는 방법을 알아서 찾아나가는데, 이때, 그 방법이 소위 말해서 수단과 방법을 가리지 않는 방법일 수 있다. 또한, agent가 취하는 액션은 나중에 보면 최대 reward를 받는 방법이었다는 것이 드러나지만, 액션 하나하나를 보면 인간이 전혀 이해하지 못하는 방향의 액션일 수도 있다. 그러므로 매우 안전하지 않을 수 있고 agent가 비도덕적으로 행동할 수 있다. Exploitation-Exploration Dilema Reinforcement learning에서의 agent는 최대한 많은 reward를 얻는 action을 선택해가며 문제를 해결해야 한다. 이미 알고 있는 문제 해결 방법중에서 가장 큰 reward를 얻을 수 있는 action을 선택해서 문제를 해결하는 것이 합리적일 것이다(exploitation). 그러나, agent는 새로운 길을 탐색해 나가면서 더 나은 길을 찾을 필요가 있다(exploration). 하지만, exploration과정은 많은 비용이 들 수도 있고 탐험 결과가 좋은 reward를 주는 경로가 아닐 수도 있다. 그럼에도 exploration은 필요하다. Exploitation 이미 찾은 문제 해결 방법중에서 가장 나은 방법을 선택하는 것. 즉, 최대 reward를 찾아가는 것. Exploration 새로운 길을 탐색하는 것. 많은 비용이 들지만, 새로운 길이 지금까지 가지고 있었던 해결 방법들 보다 더 나은 reward를 줄 수도 있다. Exploration은 많은 비용이 들기 때문에, 당장은 reward를 얻지 못할 수도 있다. Exploitation을 하면 당장은 많은 reward를 얻을 수 있다. 이처럼 이 두가지는 서로 상반되고 동시에 좋아질 수는 없는데, 이를 exploitaiton-exploration dilema라고 부른다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"11. Feature Construction","date":"2020-06-14T23:09:08.000Z","path":"studynotes/reinforcement-learning/11-Feature-Construction/","text":"Feature Construction 참고: Coursera Reinforcement Learning (Alberta Univ.) Value function을 v^(s,w)\\hat{v}(s, w)v^(s,w)로 function approximation 한다고 하자. 이때, sss는 어떤 state, www는 이 value function의 weight parameter이며, v^(s,w)\\hat{v}(s, w)v^(s,w)는 value function을 parameterize한 함수이다. 만약, v^(s,w)\\hat{v}(s, w)v^(s,w)가 linear model이라면, v^(s,w)=x(s)⋅w\\hat{v}(s, w) = x(s) \\cdot wv^(s,w)=x(s)⋅w가 될 것이다. 여기서, tabular method는 x(s)x(s)x(s)가 state개수만큼의 길이를 가진 one-hot vector였다. 즉, x(si)x(s_i)x(si​)는 iii번째 원소가 1이고 나머지는 0인 벡터였다. 하지만, 이것은 feature의 매우 한정적인 예시일 뿐, feature를 꼭 one-hot vector로 할 필요는 없을 것이다. Feature를 제대로 선택하는 것은 매우 중요하다. Feature를 제대로 선택한다면, 간단한 linear model에서도 강력한 성능을 발휘할 가능성이 높다. 때로는 feature를 선택할 때, 어떤 분야의 domain 지식이 들어가기도 한다. 지금, feature를 어떻게 만들지에 대해서 정리하고자 한다. Coarse Coding 어떤 2차원 공간이 있다고 해 본다. 2차원 공간 속에서 어느 위치에 있는지를 하나의 state라고 해 본다. 그럼 state의 개수는 무한대이고, 이것을 tabular method로 나타내기는 불가능하다. 하지만, state aggregation을 한다면 이야기가 달라진다. 공간을 몇 등분으로 나누어서 각 공간에 들어가는 것을 하나의 상태로 표현하는 것이다. 그리고, state aggregation은 기본적으로, 각 state가 겹치지 않는다. 또한, feature vector도 one-hot vector의 형태를 띄게 된다. Coarse coding이란, aggregation을 겹치게 하도록 허용하는 방식으로, state aggregation를 조금 개선한 것이다. 이때, agent는 동시에 여러개의 aggregation에 들어갈 수 있고, feature vector는 하나만 1인 벡터 형태가 아닌, 여러 원소가 1을 가지는 벡터 형태가 된다. Coarse coding은 state aggregation의 generalization 형태라고 볼 수 있겠다. Generalization &amp; Discrimination of Coarse Coding Coarse coding에서, aggregation의 모양, 사이즈, 개수, 커버리지 범위 모양 등에 따라 generalization, discrimination이 크게 달라질 수 있다. aggregation이 겹친 상태에서 전체 state가 몇 개의 조각으로 나뉘는지 보면 discrimination이 얼마나 되는지 확인할 수 있다. 많은 개수로 나뉘면 discrimination이 많이 이뤄진다. 하나의 커버리지 범위가 넓고 개수가 많다면 discrimination이 또 증가하고, generalization또한 증가하는 경향이 있으나, 상황에 따라 다르기 때문에 적절히 결정할 필요가 있다. Tile Coding Coarse coding의 한 종류로써, aggregation 모양이 그리드이다. 이 그리드를 약간의 offset으로 여러번 움직인다. 만약, 타일 1개가 넓으면, generalization도 상당히 넓어지고, offset의 크기와 방향, 그리드가 몇개냐에 따라 discrimination 정도도 달라진다. tile coding은 그리드로 나누고, 각 타일이 동일한 크기의 정/직사각형이기 때문에 현재 state가 어느 타일에 들어가는지 매우 효율적으로 계산이 가능하다. 다만, dimension이 증가할수록 tile개수는 exponential하게 증가하게 된다. Neural Network for Feature Construction Neural network는 feature를 입력으로 받아서, hidden layer로 feature를 representation한 뒤, 최종 output을 낼 수 있는 복잡하고 유연한 parameterized function이다. Neural network를 function approximation에 사용할 때의 장점은 feature construction을 인간이 아닌, 컴퓨터에게 맞김과 동시에 매우 유연하게 feature를 뽑아낼 수 있다는 것이다. Neural network는 policy, value function, model 등, parameterized function이 필요한 모든 곳에 응용될 수 있다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"10. Prediction and Control with Function Approximation","date":"2020-06-14T22:20:08.000Z","path":"studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation/","text":"Prediction and Control with Function Approximation 참고: Coursera Reinforcement Learning (Alberta Univ.) 지금까지, state와 action이 discrete하며, (state, action) pair의 value가 deterministic한, tabular method를 보았다. 하지만, 이건 실생활에서 매우 한정적일 수 밖에 없다. Value function을 table로 표현하지 말고, function으로 추정하자는게 지금부터 다룰 내용이다. V(s)=fW(s)V(s) = f_W(s) V(s)=fW​(s) Function은 어떤 파라미터 WWW로 parameterized되어 있으며, function은 linear 형태의 function이나, 인공신경망과 같이 non-linear한 형태도 가능하다. Tabular Method is a Linear Function Tabular method는 linear function approximation의 한 방법이다. state의 개수만큼 feature가 있고, feature는 각 state를 나타내는 indicator가 된다. 그러면, 각 state의 value는 그 state의 weight가 된다. V(si)=(0⋯010⋯0)⋅(w1⋯wi−1wiwi+1⋯w16)=wiV(s_i) = \\begin{pmatrix} 0 \\\\ \\cdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\cdots \\\\ 0 \\end{pmatrix} \\cdot \\begin{pmatrix} w_1 \\\\ \\cdots \\\\ w_{i-1} \\\\ w_i \\\\ w_{i+1} \\\\ \\cdots \\\\ w_{16} \\end{pmatrix} = w_i V(si​)=⎝⎜⎜⎜⎜⎜⎜⎜⎜⎛​0⋯010⋯0​⎠⎟⎟⎟⎟⎟⎟⎟⎟⎞​⋅⎝⎜⎜⎜⎜⎜⎜⎜⎜⎛​w1​⋯wi−1​wi​wi+1​⋯w16​​⎠⎟⎟⎟⎟⎟⎟⎟⎟⎞​=wi​ 즉, tabular method는 위와 같이 indicator feature를 이용해서 linear function으로 표현이 가능하다. 따라서, tabular method는 linear function approximation의 한 instance이다. Generalization and Discrimination Generalization과 discrminiation은 reinforcement learning에서 상당히 중요하다. Generalization Generalizaiton은 어떤 state를 학습(value를 추정)했다면, 비슷한 다른 state까지 영향을 미쳐서 학습되는 것을 의미한다. Discrimination Discrimination은 서로 다른 state끼리는 학습 또는 추정시, 영향을 미치지 않아야 함을 의미한다. 즉, 어떤 state끼리는 독립적으로 value가 추정되어야 한다는 것이다. Tabular method는 generalization을 전혀 하지 못하고, discrimination을 100% 수행하는 방법이다. 반면, 모든 state를 똑같은 value를 두도록 설정하면, discrimination을 전혀 수행하지 못하고 generalization을 100% 수행하게 된다. RL에서는 generalization과 discrimination을 동시에 높여야 하며, trade-off관계라서, 적절히 조정하는게 필요하다. Value Estimation as Supervised Learning True reward 리턴값이 있다면, reward를 target label로 삼아서 fW(s)f_W(s)fW​(s)를 학습할 수 있지 않을까. 하지만, reinforcement learning에서는 각 데이터 샘플(하나의 state-action pair)이 서로 correlate되어 iid에 위반되므로, 모든 supervised learning technique이 다 잘 적용될 수 있는 것은 아니다. 또한, TD의 경우, target 자체가 estimation값(다음 state의 value는 또 다른 estimation 값임)이므로, target이 학습이 지속되면서 변한다. 이는, supervised learning이랑 상당히 다른 환경이다. The Objective for On-policy Prediction Function approximation을 하기 위해서는, target value와 얼만큼 가까운지 판단할 수 있는 objective function이 필요하다. The Value Error Objective 다음은 Mean squared error를 이용한 objective function이다. VE=∑sμ(s)[Vπ(s)−v^(s,W)]2\\text{VE} = \\sum_s \\mu(s)[V_{\\pi}(s) - \\hat{v}(s, W)]^2 VE=s∑​μ(s)[Vπ​(s)−v^(s,W)]2 이때, 각 state마다 서로 다른 가중치 μ(s)\\mu(s)μ(s)를 주어서, 상대적으로 중요한 state에게는 높은 가중치를, 덜 중요한 state에게는 낮은 가중치를 주도록 한다. Policy에 의해 자주 방문하는 state에 대해서는 높은 가중치를 줄 수도 있겠다. Gradient Monte Carlo for Policy Evaluation Gradient descent를 Monte Carlo RL에 맞게 수정한 것. Stochastic gradient descent. Value error식은 때때로 state개수가 너무 많아서 계산이 불가능하다. 대신, gradient를 approximation한다. 원래 Gradient descent식은 다음과 같다. 이때, x(s)x(s)x(s)는 state sss의 feature vector이다. W←W+α∑sμ(s)[Vπ(s)−v^(s,W)]∇v^(s,W)W \\leftarrow W + \\alpha \\sum_s \\mu(s)[V_{\\pi}(s) - \\hat{v}(s,W)] \\nabla \\hat{v}(s, W) W←W+αs∑​μ(s)[Vπ​(s)−v^(s,W)]∇v^(s,W) 그런데, 이 식을 쓰지 말고, 다음처럼 gradient를 approximate해서 쓰자는 것이 된다. W←W+α[Vπ(St)−v^(St,W)]∇v^(St,W)W \\leftarrow W + \\alpha [V_{\\pi}(S_t) - \\hat{v}(S_t, W)] \\nabla \\hat{v}(S_t, W) W←W+α[Vπ​(St​)−v^(St​,W)]∇v^(St​,W) 왜냐하면 다음이 성립하기 떄문. 즉, 위 gradient는 원래 gradient의 추정치라고 볼 수 있다. E[Vπ(St)−v^(St,W)]=∑sμ(s)[Vπ(s)−v^(s,W)]E[V_{\\pi}(S_t) - \\hat{v}(S_t,W)] = \\sum_s \\mu(s)[V_{\\pi}(s) - \\hat{v}(s,W)] E[Vπ​(St​)−v^(St​,W)]=s∑​μ(s)[Vπ​(s)−v^(s,W)] 이는, 샘플 하나 (state-action pair 1개)씩 보면서 한 번 업데이트하는 stochastic gradient descent 방식이다. 하지만, 이 경우는 target value function인 Vπ(s)V_{\\pi}(s)Vπ​(s)를 알아야 한다. 얘네도 GtG_tGt​를 이용해 approximation한다. W←W+α[Gt−v^(St,W)]∇v^(St,W)W \\leftarrow W + \\alpha [G_t - \\hat{v}(S_t,W)] \\nabla \\hat{v}(S_t,W) W←W+α[Gt​−v^(St​,W)]∇v^(St​,W) 역시 다음을 추정한 것이다. E[V(St)]=E[Gt∣St]E[V(S_t)] = E[G_t|S_t] E[V(St​)]=E[Gt​∣St​] 다음은 pseudo code. State Aggregation State개수가 너무 많아서 모든 state를 따로 업데이트하기 힘들 때, 비슷한 state끼리는 묶어서 하나의 state로 보는 것을 말한다. 따라서, 하나의 state가 업데이트되면 같은 그룹의 다른 state도 같은 값으로 업데이트된다. The Objective for TD On-policy learning인 Gradient Monte Carlo의 objective로 squared error를 사용했었다. 하지만, TD는 GtG_tGt​가 다음의 특성을 가진다. v(St,W)=Rt+1+γv(St+1,W)v(S_t, W) = R_{t+1} + \\gamma v(S_{t+1}, W) v(St​,W)=Rt+1​+γv(St+1​,W) (Gt=v^(St,At)G_t = \\hat{v}(S_t, A_t)Gt​=v^(St​,At​)이다. ) 즉, 다음의 gradient 수식에서, (Gt−v^(St,W))∇v^(St,W)(G_t - \\hat{v}(S_{t}, W)) \\nabla \\hat{v}(S_{t}, W) (Gt​−v^(St​,W))∇v^(St​,W) GtG_tGt​부분은 실제 value의 추정값이므로, TD learning에서는 v(St,W)v(S_t,W)v(St​,W)와 대체해야 한다. 그런데, 이놈은 v(St+1,W)v(S_{t+1}, W)v(St+1​,W)를 참조하고 있으며, 이 v(St+1,W)v(S_{t+1}, W)v(St+1​,W)는 true value의 estimation이기 보단 현재의 value estimation이다. 따라서, biased된 추정값이며, v(St,W)v(S_t, W)v(St​,W)를 미분해도 이 식이 WWW를 가지고 있으므로, Gradient Monte Carlo의 gradient 수식과 같게 나오지 않는다. 하지만, 그냥 v(St,W)v(S_t, W)v(St​,W)는 상수처럼 간주해버리고 쓰게 되는데(즉, WWW에 대한 함수가 아니라고 간주), 이를 semi-gradient 방법이라고 부른다. 최종적으로 WWW의 업데이트 식은 다음과 같이 쓴다. W←W+α(Rt+1+γv(St+1,W)−v^(St,W))∇v^(St,W)W \\leftarrow W + \\alpha(R_{t+1} + \\gamma v(S_{t+1}, W) - \\hat{v}(S_t,W)) \\nabla \\hat{v}(S_t, W) W←W+α(Rt+1​+γv(St+1​,W)−v^(St​,W))∇v^(St​,W) TD vs Monte Carlo Function approximation에서, TD와 Monte Carlo 방식의 차이점은 다음과 같다. TD 장점 에피소드가 끝나기 전에 바로바로 학습하므로 빠른 학습 속도(loss가 빠르게 줄어듬) 단점 Estimation이 최종 reward를 반영하지 않은, 현재의 value estimation을 true estimation으로 삼기 때문에 biased된 학습. 즉, 부정확할 수 있다. Local minimum의 근처까지밖에 못갈 수 있다. Monte Carlo 장점 에피소드의 최종 reward를 반영한 true value의 estimation을 사용하기에 TD보단 unbiased된 학습. 즉, local minimum을 다소 정확하게 찾는다. 단점 느리다. step size를 작게 줘야 한다. Linear TD Value function을 linear하게 모델링한 것을말하며, 간단하고 쉽지만, 잘 정제된 feature가 있다면, 매우 강력한 성능을 발휘한다. Tabular TD(0)는 linear TD의 한 종류인데, 다음처럼 feature가 생겼다고 가정한다면, 완벽히 linear TD이다. w=(w0w1w2w3⋯wd),x(si)=(0010⋯0),v^(s,w)=w⋅xw = \\begin{pmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ w_3 \\\\ \\cdots \\\\ w_d \\end{pmatrix}, x(s_i) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\cdots \\\\ 0 \\end{pmatrix}, \\hat{v}(s,w) = w \\cdot x w=⎝⎜⎜⎜⎜⎜⎜⎛​w0​w1​w2​w3​⋯wd​​⎠⎟⎟⎟⎟⎟⎟⎞​,x(si​)=⎝⎜⎜⎜⎜⎜⎜⎛​0010⋯0​⎠⎟⎟⎟⎟⎟⎟⎞​,v^(s,w)=w⋅x feature는 어떤 state인지 나타내는 indicator이고, weight가 각각 상응하는 state들의 value가 되는 셈. Feature xxx를 어떤 aggregation인지를 나타낸다고 하면, aggregation tabular TD(0)역시 linear TD의 모양이 되므로, aggregation tabular TD(0)역시, linear TD의 한 종류이다. 만약, squared error를 사용하는 linear TD라면, 다음 식으로 www가 업데이트된다. w←w+α(Rt+1+γv^(St+1,w)−v^(St,w))X(St)w \\leftarrow w + \\alpha (R_{t+1} + \\gamma \\hat{v}(S_{t+1}, w) - \\hat{v}(S_t, w)) X(S_t) w←w+α(Rt+1​+γv^(St+1​,w)−v^(St​,w))X(St​)","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"09. Models and Planning","date":"2020-06-14T22:00:08.000Z","path":"studynotes/reinforcement-learning/09-Models-and-Planning/","text":"Models and Planning 참고: Coursera Reinforcement Learning (Alberta Univ.) 지금까지, 두 가지 경우를 보았다. Agent가 environment를 알고 있어서 실제 action을 취하지 않고, value function을 계산하고 optimal policy를 찾을 수 있다. (Dynamic programming) Agent가 environment를 알 수 없어서, 오직 action을 취해서 얻은 sample들로만 value function을 추정했다. (Sample-based) 하지만, 이번에는 environment도 같이 추정해서 value function을 더 정확히 추정하고 더 나아가 더 나은 policy를 추정하는 방법을 찾고자 한다. (즉, 두 가지 방법을 합친 것) Models Environment를 모델링한 것이라고 보면 된다. Environment를 모델링함으로써, Experiment를 통한 sampling이 가능하다. 각 event(action, reward가 생기는)의 확률을 알고, likelihood를 계산할 수 있다. 또는 marginalization 등 확률적인 계산들이 가능하다. Model에는 sample model과 distribution model 두 가지가 있을 수 있다. 모델은 반드시 정확히 environment를 향해 가도록 모델링해야 한다. Bias되면 답이 없다. Sample Models 이 모델은 experiment를 통한 샘플링이 가능하도록 모델링한 것이다. 다음의 특징이 있다. 모델링하기 간단하다. 크기가 작다. Joint 확률까지는 필요없다. &quot;주사위 5개의 결과&quot;를 예로 들어보면, 주사위 1개의 확률분포만 모델링하면 된다. Distribution Models 이 모델은 각 state가 될 확률, reward의 확률분포를 알 수 있도록 모델링한 것이다. 모델링하기 복잡하고 크기가 크다. 가능한 경우의 수에 대해서 (joint)확률을 매겨야 한다. &quot;주사위 5개의 결과&quot;를 예로 들어보면, distribution model은 656^565개의 확률을 계산할 수 있어야 한다. Planning Model을 통해 experiment를 시행해서 episode를 만들고(샘플링), 그것을 이용해서 value function을 업데이트하는 과정을 의미한다. Model은 sample model에 해당한다. Random-sample One-step Tabular Q-Learning 이 방법은 planning중 하나의 방법으로, state transition dynamic을 sample model로 알고 있다고 가정한다. 또한, (시뮬레이션 중에)action을 어떻게 선택할지에 대한 전략도 있을 것이라고 가정한다. 다음과 같은 과정으로 이루어진다. 첫 state와 action을 선택 Model로부터 다음 state를 샘플링하고(given current state, action), action 선택 전략에 따라 action을 뽑음 Q-learning 알고리즘에 따라 value function을 업데이트 Q(St,At)=Q(St,At)+α⋅(Rt+1+γ⋅max Q(St+1,At+1)−Q(St,At))Q(S_t, A_t) = Q(S_t, A_t) + \\alpha \\cdot (R_{t+1} + \\gamma \\cdot \\text{max} ~ Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) Q(St​,At​)=Q(St​,At​)+α⋅(Rt+1​+γ⋅max Q(St+1​,At+1​)−Q(St​,At​)) 2,3번 반복 Value function이 수렴했으면, policy update π∗=maxa Q(s,a)\\pi_* = \\underset{a}{ \\text{max} } ~ Q(s,a) π∗​=amax​ Q(s,a) π←π∗\\pi \\leftarrow \\pi_* π←π∗​ Advantage of Planning Planning의 1-step 속도는 당연히 현실에서 1-step 가는것보다 빠르다. 따라서, 현실에서 action을 취하고 다음 action을 취하는 그 사이 간격에서 planning을 수행해줌으로써, value function의 효율적인 수렴이 가능할 수도 있다. Dyna Architecture Direct RL: Environment와 상호작용하면서 얻은 샘플로 value function을 업데이트하는 것. Planning: Environment를 모델링한 모델을 통해 얻은 샘플로 value function을 업데이트하는 것. 이라고 정의해보면(공식적인 정의는 아닌 듯 하다), 결국, 이 두 가지를 적절히 섞어서 value function을 업데이트하면, environment와 상호작용한 샘플이 적어도 효율적인 업데이트가 가능할 것이다. 이것을 구현한 것 중 하나로, Dyna architecture가 있다. Dyna architecture는 environment와 상호작용하면서 얻은 진짜 샘플로 value function을 업데이트함과 동시에 model도 학습해야 한다. 모델은 진짜 샘플로 학습된 이후, 시뮬레이션 sequence를 생성해서 value function 학습에 이용되게 한다. 결국 이 모든건, environment를 추정하는 과정이라고 볼 수 있다. 어찌됬든, environment를 경험해서 배우고 그 environment에서 각 state의 최적 액션을 배우고자 함이니까. 다만, 첫 번째 episode에서는 큰 효과를 발휘하지 못할 수도 있다. 모든 state가 처음이고, value function이 0으로 초기화되어있기 때문. 하지만, 첫 번째 episode를 마치고나면, reward로 인해 value function이 조금 업데이트되고, model이 학습되면서 planning에 의해 value function이 업데이트가 시작된다. 첫 번째 episode에서 방문했던 모든 state가 사실상 업데이트가 될 수 있으며, 이에 따라, 매우 소수의 episode만을 가지고도 상당히 정확한 policy를 얻을 수 있다. Tabular Dyna-Q Algorithm Environment가 deterministic하다고 가정한 Dyna architecture. Dyna-Q란, Dyna architecture에서 Q-learning을 채용한 알고리즘을 말한다. 다음은 Dyna 알고리즘의 일종인 tabular Dyna-Q 알고리즘의 pseudo code. 이때, Model(S,A)←R,S′Model(S,A) \\leftarrow R, S&#x27;Model(S,A)←R,S′의 의미는, model에다가 SSS상태일때, AAA를 취하면 무조건 RRR의 리워드를 얻고 S′S&#x27;S′상태로 간다고 매핑해 두라는 의미이다. (Tabular Dyna-Q 알고리즘은 deterministic environment라고 가정한다. 물론, 알고리즘을 수정하면 non-deterministic하게도 할 수 있겠지) 한번의 step을 할 때 마다 여러번(여기서는 nnn번)의 planning이 일어날 수 있다. Planning은 많이많이 해야 한다. Planning은 랜덤으로 start state를 고르고 1 step만 간다. Random Search in Dyna-Q 문제가 있다. 위에서 본 Dyna-Q에서는 planning을 랜덤으로 고른 state에서 진행하고 있다. 만약, 미로찾기에서, 모든 step의 reward는 0이고, final state에서만 +1일 경우, 1회의 episode를 마친 후면, final state의 바로 앞의 state의 value만 업데이트된다(TD종류의 알고리즘을 사용했다면). 그리고, planning은 반드시 아래 사진의 위치를 골라야 value가 업데이트될 것이다. 업데이트 가능 위치가 저기 하나뿐이다. 이것은 매우 많은 planning이 업데이트를 실패하게끔 만들며, environment 크기가 클수록 상황은 더 심각해진다. 다음의 개선책이 있겠다. Model이 value의 변화를 관찰하고 있다가, value가 변하는 state와 그 action을 기록한다. 그 state로 오게 만들었던 과거 state와 action을 업데이트하기 위해 backward로 간다. Update되고 있는 state, action pair가 있는 queue를 관리한다. (위 사진에선 state, action pair가 1개만 queue에 있겠지). 이 queue는 priority queue로, 어느 한쪽의 state만 업데이트되게 하는 것을 막아주는 목적이다. Priority가 높은 놈이 선택되어 그 놈부터 backward로 planning이 이루어지게 된다. 만약, 한 state가 일정 이상 업데이트가 이루어지면 priority를 낮춘다. Inaccurate Models 모델이 environment를 반영못하는 경우는 두 가지가 있을 수 있다. 알고리즘은 제대로 되있다고 가정한다. 학습 초기. Environment와의 interaction 횟수가 매우 적을 때 Environment가 변할때 이때는 문제가 될 수 있는게, planning이 잘못된 모델로부터 얻은 transition으로 value를 학습하게 된다. 첫번째 문제는 어쩔 수 없다. 그러나, 두번째 문제는 대처해야할 필요가 있다. Exploration-Exploitation Trade-off in Chaning Environment Environment가 변하는 것에 대처하기 위해서는 모델도 exploration을 해야 한다. 그저 옛 모델에 머물 수는 없다. 따라서, 모델이 모델링하고 있는 가상의 environment또한 exploration-exploitation dilema가 생기게 된다. Environment가 언제 변할지 알수 없고, 변하지 않는다면, exploration은 손해를 낳는다. Exploration을 모델에서 직접 할 수는 없고, policy 학습에 영향을 주어 environment에서 exploration을 하도록 유도하는 방식으로 이루어진다. 모델을 수정하지 않는 이상 모델의 가상 environment가 고정되어 있기 때문에 거기서 exploration해봤자 아무 의미가 없다. 즉, Planning 또한 exploration을 유도해야 한다. 아예 environment에서 bahavior policy의 exploration에만 의존하면 모델로 인한 planning이 너무 잘못된 방향으로 policy를 유도한다. Dyna-Q+ Algorithm Environment가 변할때를 반영하기 위해 Dyna-Q를 변형한 알고리즘이다. Dyna architecture에서 planning은 과거에 방문한 state만 업데이트하게 되는데, 문제는 environment가 변함으로써, 방문한 놈들의 reward, next state 분포가 변할 수 있다는 것. 따라서, 방문한 지 오래된 state에 대해서는 보너스 reward를 모델에서 할당한다. 그리고 그 보너스 reward를 value를 계산할 때 반영하게 된다. R←R+κτR \\leftarrow R + \\kappa \\sqrt{\\tau} R←R+κτ​ 이때, κ\\kappaκ는 작은 상수이며, τ\\tauτ는 방문한지 얼마나 됬는지에 대한 time step이다. 즉, 방문한 지 오래된 놈이면 reward를 증가시킨다. (Value에다가 더하지 말고, action에 따른 결과 reward에다가 더하자.) 그럼 취한 지 오래된 action으로 인한 transition이 장려될 것이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"Home","date":"2020-05-29T07:32:00.000Z","path":"index/","text":"Jaeyoung’s BlogThis is blog for my study notes. (Thanks to @Taeuk for helping creation of my blog) 모든 노트 재작성중… Contents Machine Learning Bayesian Statistics Reinforcement Learning","tags":[{"name":"Home","slug":"Home","permalink":"https://wayexists02.github.io/tags/Home/"}],"categories":[{"name":"Home","slug":"Home","permalink":"https://wayexists02.github.io/categories/Home/"}]},{"title":"Appendix 01. Which Algorithm Should be selected","date":"2020-03-24T00:12:08.000Z","path":"studynotes/reinforcement-learning/Appendix-1-RL-Cheatsheet/","text":"Which Algorithm Should be Selected 많은 RL알고리즘 중에서 어떤 알고리즘을 선택해야 하는지 난감하다. 이럴때는 다음과 같은 과정을 거쳐서 적절한 알고리즘을 선택해야 한다. 풀고자 하는 문제를 정의한다. 문제에서, environment의 구성, agent의 행동 범위 등을 정의한다. Environment를 기반으로 continuous task인지, episodic task인지 구분한다. Continuous task라면, $G_t$를 discounting, average reward중 하나를 사용해야만 한다. Episodic task라면, $G_t$에서 discounting을 사용하거나 사용하지 않아도 된다. Function approximation을 사용할 것인지 결정한다. Function approximation을 사용할 것이면, 적절한 모델링이 필요하다. Value function만 function approximation할 수도 있다. (Continuous action이면 좀 힘들 수 있다.) Actor-Critic처럼 value function, policy모두 function approximation할 수도 있다. Action이 discrete인지 확인해야 한다. Discrete하다면, softmax policy 또는 greedy policy, $\\epsilon$-greedy policy로 갈 수도 있다. Contiunous하다면, policy function approximation을 통한 gaussian policy같은 것을 선택할 수도 있다. Planning 여부를 결정한다. 방문하는 state의 개수가 한정적이라면 Dyna-Q 또는 Dyna-Q+를 사용할 수 있다. 방문하는 state가 많고, 각 state가 중복되기 힘든 상황이라면, planning은 매우 주의해서 사용해야 한다. 아직, open research area이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"13. Policy Gradient","date":"2020-03-23T00:12:08.000Z","path":"studynotes/reinforcement-learning/13-Policy-Gradient/","text":"Policy Gradient Policy gradient 방법은, policy를 parameterized function으로 모델링해서, state, action feature vector로부터 바로 policy를 추론하는 방법이다. 즉, state, action feature로부터 action value function을 추론하고 그로부터 policy를 계산하는 이전 방법에서, action value function을 거치지 않고 바로 policy를 추론하는 것이다. 차이점은, 이전 방법에서는 action value function을 parameterize했지만, policy gradient에서는 policy를 parameterize한다. 이때, policy funciton의 parameter는 $\\theta$로 표기한다. Softmax Policies Policy는 반드시 확률분포여야 한다. 즉, 다음이 성립해야 한다. $$ \\sum_a \\pi(a|s, \\theta)=1 ~ \\text{ for all } s $$ 즉, 각 state에서, action들의 확률들의 합은 1이어야 한다. 또한, 각 확률은 [0, 1] 범위에 있어야 한다. Action value function에서는 linear function을 이용했지만, policy function은 위와 같은 이유로 linear function의 결과값을 그대로 이용할 수 없다. 그 대신, linear function의 결과값에 softmax 함수를 적용해 준다. $$ \\pi(a|s, \\theta) = \\text{softmax}(h(s, a, \\theta)) = \\frac{e^{h(s, a, \\theta)}}{\\sum_{a’} e^{h(s, a’, \\theta)}} $$ 이때, $h(s, a, \\theta)$는 state $s$와 action $a$를 입력으로 받고 action preference를 출력하는 linear 함수(마지막 레이어가 linear이면 linear하다고 하자)이다. 이제, action preference는 linear해도 된다. action preference가 심지어 음수가 나와도 exponential에 의해 양수임이 보장되며, 분모의 normalization으로 인해 [0, 1]사이 값으로 유지됨이 보장되며, 합이 1임이 보장된다. Action Preference vs Action Value Action preference는 action value와는 다르다. Action value는 미래 expected return의 합으로 이루어져 있으나, action preference는 그러한 것을 고려하지 않는다. 또한, action value는 가장 높은 값을 가지는 action만이 높은 확률을 갖고($1 - \\epsilon$), 나머지 action은 $\\epsilon/N_a$값을 가진다($N_a$는 액션 개수). 즉, 높은 action value 값을 가지는 action이외의 action은 모두 작은 확률로 같은 확률을 가진다. 하지만, action preference는 그 값이 크고 작음의 순서가 유지가 된다. Softmax 함수는 preference가 클수록 큰 확률을 가지게 하며, 작을수록 작은 확률을 가지게 만들어준다. Stochastic Policies 때로는 deterministic policy를 가지면 풀기가 불가능한 문제도 있다고 한다. Epsilon greedy방법(action value approximation에서의)은 deterministic policy를 결과로 내놓기에, 이때 매우 성능이 좋지 않다. Policy gradient는 반면, softmax함수로 인해 stochastic policy를 출력한다. (Action value에 따라서 policy를 정할때도 epsilon greedy가 아닌 softmax를 씌우면 어떻게 될까) Selecting Function Approximation 때로는 action value를 parameterized function으로 모델링하는 것 보다 policy를 parameterized function으로 모델링하는게, 훨씬 간단할 때도 있고 반대의 경우도 존재한다. 따라서, 적절히 덜 복잡한 방향을 선택하면 될 듯 하다. Policy Gradient for Continuous Tasks Episodic task에서는 다음을 최대화하는 액션을 선택하게 하는 policy를 찾는 것이다. $$ G_t = \\sum_{t=0}^T R_t $$ Continuous task에서는 다음을 최대화하는 액션을 선택하게 하는 policy를 찾는 것이다. $$ G_t = \\sum_{t=0}^T \\gamma^t R_t ~ (0 \\leq \\gamma &lt; 1) $$ (Continuous이기 때문에 discounting이 필요. 안그러면 $\\infty$로 간다.) Continuous task에서는 다음을 최대화하는 액션을 선택하게 할 수도 있다. $$ G_t = \\sum_{t=1}^T (R_t - r(\\pi)) $$ 이때, $R_t - r(\\pi)$는 발산하지 않고 수렴하기에 discounting이 필요하지 않다. 어떤 policy가 최대 reward를 획득하게 한다는 것은 모든 state의 평균 reward를 의미하는 $r(\\pi)$를 최대화하는 것과 같다. 그 policy가 궁극적으로 총 reward가 최대가 되도록 학습한다면, $r(\\pi)$도 최대이기 때문. $r(\\pi)$는 다음과 같았다. $$ r(\\pi) = \\sum_s \\mu_{\\pi}(s) \\sum_a \\pi(a|s, \\theta) \\sum_{s’,r} p(s’, r|s, a)r $$ 이 $r(\\pi)$를 최대화하기 위해서, policy $\\pi$에 대해 미분해보면, $$ \\nabla r(\\pi) = \\nabla \\sum_s \\mu_{\\pi}(s) \\sum_a \\pi(a|s, \\theta) \\sum_{s’, r} p(s’, r|s, a)r $$ 인데, $\\mu_{\\pi}(s)$는 각 state의 방문 횟수로, policy에 의해 영향을 받는다. 하지만, $\\mu_{\\pi}(s)$는 stationary distribution으로, 추정하기 쉽지 않다. The Policy Gradient Theorem The Policy Gradient Theorem이라는 이름으로, $\\nabla r(\\pi)$는 다음으로 추정할 수 있다고 증명되어 있다. $$ \\nabla r(\\pi) = \\sum_s \\mu_{\\pi}(s) \\sum_a \\nabla \\pi(a|s,\\theta) \\cdot q_{\\pi}(s,a) $$ $\\nabla \\pi(a|s,\\theta)$는 액션 $a$의 확률값이 높아지는 방향의 gradient이다. 이것을 상응하는 action value와 weighted sum하게 된다. 그러면 전체 average reward가 상승하는 방향일 것이라는 것이고, 이 과정을 모든 state에서 계산하고 모두 더해준다. 예를들어, 어떤 state에서 액션이 상, 하, 좌, 우 네가지가 있고, 상, 좌 방향으로는 action value가 낮다고 하자. 하지만, 하, 우 방향으로는 높다고 할 때, 상대적으로, 하, 우 방향의 action value가 높으므로, 하, 우 방향이 gradient와 action value의 곱이 상대적으로 비중을 차지하는 비율이 증가할 것이다. 그리고, policy는 이 gradient 방향으로 업데이트하게 되면, policy는 하, 우 의 확률 비중을 약간 늘릴 수 있을 것이다. 이런 과정을 전체 state에 반복하면서 전체 average gradient를 높일 수 있다는 것이다. Estimation of Policy Gradient $\\nabla r(\\pi)$를 계산하는 대신 추정하고자 하는데, $\\nabla r(\\pi)$는 다음과 같았다. $$ \\nabla r(\\pi) = \\sum_s \\mu_{\\pi}(s) \\sum_a \\nabla \\pi(a|s,\\theta) q(s,a) $$ 그런데, 이것은 다음처럼 $\\mu$에 대한 기댓값으로 표현이 가능하다. $$ \\nabla r(\\pi) = E_{\\mu}[\\sum_a \\nabla \\pi(a|s,\\theta) q(s, a)] $$ 참고로, $E[X]$는 stochastic sample인 $x_i \\sim X$ 여러개로 추정이 가능하다. 따라서, $\\nabla r(\\pi)$는 다음처럼 추정할 수 있다. $$ \\nabla r(\\pi) = \\sum_a \\nabla \\pi(a|S_t,\\theta) q(S_t, a) $$ 이때, $S_t$는 agent가 environment와 상호작용하면서 얻은 샘플 또는 경험이다. 하지만, action에 대한 summation이 남아 있다. 이것은 계산이 가능하지만, 이것 또한 stochastic sample을 이용해서 추정이 가능하다. 다음처럼 $\\pi$에 대한 기댓값이 되도록 식을 수정한다. $$ \\nabla r(\\pi) = \\sum_a \\pi(a|S_t, \\theta) \\frac{1}{\\pi(a|S_t,\\theta)} \\nabla \\pi(a|S_t, \\theta) q(S_t, a) $$ $$ \\nabla r(\\pi) = E_{\\pi} [\\frac{\\nabla \\pi(a|S_t, \\theta)}{\\pi(a|S_t,\\theta)}q(S_t, a)] $$ 이것은 다음으로 추정이 가능할 것이다(stochastic sample). $$ \\nabla r(\\pi) = \\frac{\\nabla \\pi(A_t|S_t,\\theta)}{\\pi(A_t|S_t,\\theta)} q(S_t, A_t) $$ 최종적으로, policy gradient ascent는 다음과 같다. $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha \\cdot \\nabla r(\\pi) $$ $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha \\cdot \\frac{\\nabla \\pi(A_t|S_t,\\theta)}{\\pi(A_t|S_t,\\theta)} q(S_t, A_t) $$ ($\\nabla r(\\pi)$는 $r(\\pi)$가 증가하는 방향이므로 +를 한다.) 이것은 또 다음처럼 간단하게 쓸 수 있다. $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha \\cdot \\nabla ~ \\text{ln}[\\pi(A_t|S_t, \\theta) ]q(S_t, A_t) $$ $\\pi$는 parameterized function이라 계산이 가능하고, $q(S_t,A_t)$는 TD같은 것으로 추정할 수 있다(Value function도 여전히 추정해야 한다!). $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha \\cdot \\nabla \\text{ln}[\\pi(A_t|S_t,\\theta)](R_{t+1} - r(\\pi) + \\hat{v}(S_{t+1})) $$ Action value는 액션 $A_t$을 취했을 때 얻은 reward $R_{t+1}$와 그 후의 value, 즉 state value $v(S_{t+1})$을 합한 것과 같기 때문에 위 처럼 된다. 이 경우, action value function은 단순히 reward의 합이 아닌, differential reward의 합을 추정한 놈이므로 $q(S_t, A_t) = R_{t+1} - r(\\pi) + v(S_{t+1},W)$이 된다. Actor-Critic Algorithm Actor가 critic의 피드백을 받고 본인의 policy를 수정하면서 발전해가는 형식이라고 한다. Actor Parameterized policy function을 말하며, 어떤 행동을 하는 객체라고 해서 이렇게 이름을 붙인 듯. Critic Parameterized state value function을 말하며, actor가 다음에 action을 취할지, 즉, policy를 어떻게 업데이트할지에 대해, action value $q(S_t, A_t)$값으로 피드백을 주고 actor의 다음 행동에 영향을 미치게 한다. Agent가 environment와 상호작용하면서 얻은 샘플(또는 경험)들을 이용해서 actor와 critic을 동시에 업데이트시키게 된다. 그 전에, 업데이트의 편의를 위해 policy의 파라미터인 $\\theta$의 업데이트식에 action value baseline을 추가한다. $$ \\theta_{t+1} \\leftarrow \\theta_t + \\alpha \\cdot \\nabla \\text{ln}[\\pi(A_t|S_t,\\theta)](R_{t+1} - r(\\pi) + \\hat{v}(S_{t+1},W)- \\hat{v}(S_t,W)) $$ 이때, $\\hat{v}(S_t,W)$가 action value의 baseline이며, 이것을 추가하는 것은 $\\theta$의 업데이트 방향에 영향을 전혀 미치지 않는다. 왜냐하면, 위 식을 다시 기댓값 식으로 바꿔보면, $$ \\nabla r(\\pi) = \\nabla \\text{ln}[\\pi(A_t|S_t, \\theta)](R_{t+1} - r(\\pi) + \\hat{v}(S_{t+1},W)) $$ $$ \\nabla r(\\pi) = E_{\\pi} [\\nabla \\text{ln}[\\pi(a|S_t, \\theta)](R_{t+1} - r(\\pi) + \\hat{v}(S_{t+1},W))] $$ 그런데 여기서, 다음을 더해주는데, 아래 기댓값은 0이기 때문에 $\\nabla r(\\pi)$에 영향을 미치지 않는다. $$ E_{\\pi}[-\\nabla \\text{ln} [\\pi(a|S_t,\\theta)]\\hat{v}(S_t,W)] $$ 따라서, 다음과 같이 된다. $$ \\nabla r(\\pi) = \\nabla \\text{ln}[\\pi(A_t|S_t, \\theta)](R_{t+1} - r(\\pi) + \\hat{v}(S_{t+1},W) - \\hat{v}(S_t,W)) $$ 이 식이 Actor-Critic 알고리즘에서 사용될 policy gradient 식인데, baseline의 기댓값이 0인 이유는 다음과 같다. $$ E_{\\pi}[-\\nabla \\text{ln} [\\pi(a|S_t,\\theta)] \\hat{v}(S_t, W)] $$ $$ = -\\sum_a \\pi(a|S_t,\\theta)\\nabla \\text{ln} [\\pi(a|S_t,\\theta)] \\hat{v}(S_t, W) $$ $$ = -\\sum_a \\nabla \\pi(a|S_t,\\theta)\\nabla \\hat{v}(S_t, W) $$ 이때, $S_t$는 이미 주어져 있다($S_t$가 주어진 후, action/policy에 대한 기댓값이니까). 따라서, $\\hat{v}$은 action에 의해 영향을 받지 않는 값이며, 밖으로 나올 수 있다. $$ = -\\hat{v}(S_t, W) \\sum_a \\nabla \\pi(a|S_t,\\theta) $$ 그리고, gradient의 합은 합의 gradient이므로 다음과 같다. $$ = -\\hat{v}(S_t, W) \\nabla \\sum_a \\pi(a|S_t,\\theta) $$ 근데, 이때, $\\pi$는 확률분포이고, 그들의 합은 1이다. 따라서, $$ = -\\hat{v}(S_t,W) \\nabla 1 = 0 $$ 어쨌든 최종적으로, policy gradient식은 다음과 같다. $$ \\nabla r(\\pi) = \\nabla \\text{ln}[\\pi(A_t|S_t, \\theta)](R_{t+1} - r(\\pi) + \\hat{v}(S_{t+1},W) - \\hat{v}(S_t,W)) $$ $$ \\nabla r(\\pi) = \\nabla \\text{ln}[\\pi(A_t|S_t, \\theta)] \\delta_t $$ 이때, $\\delta$는 TD error이다(TD error로 만들어주기 위해 baseline을 넣은 것이다). 최종적으로, Actor-Critic 알고리즘의 pseudo code는 다음과 같다. (Learning rate는 actor, critic, 그리고 average reward 평균비율 전부 따로 줄 수 있다) State value function의 feature는 state로만 구성되지만, policy의 feature는 state와 action 모두로 구성된다. Policy의 feature는 state feature를 action 개수만큼 stack한 feature라고 가정해보자. 다음 그림에서 state feature는 $x_0(s), x_1(s), x_2(s), x_3(s)$ 4개로 구성되어 있고, state-action pair feature 개수는 state feature 4개를 3번 복사한, 총 12개로 구성되는 것이다. 만약, state value function의 구현으로 linear parameterized function을 사용한다면, 그의 파라미터 $w$의 업데이트 규칙은 다음처럼 될 것이다. $$ w \\leftarrow w + \\alpha \\cdot \\delta \\cdot x(s) $$ 또한, policy function을 softmax로 구현했다면, 그의 파라미터 $\\theta$의 업데이트는 다음처럼 될 것이다. $$ \\theta \\leftarrow \\theta + \\alpha \\cdot \\delta \\cdot (x_h(s,a) - \\sum_b \\pi(b|s,\\theta) x_h(s,b)) $$ 이때, $x(s)$는 4개로 구성된 state feature이고, $x_h(s,a)$는 12개로 구성된 state-action pair feature인데, action $a$에 해당하는 4개의 feature만 가저온 것이다. ($\\nabla \\text{ln} \\pi(a|s,\\theta) = x_h(s,a) - \\sum_b \\pi(b|s,\\theta) x_h(s,b)$이기 때문; $\\nabla h(s,a,\\theta) = x_h(s,a)$) Actor-Critic for Continuous Actions Action 개수만큼 state feature를 stacking하는 것도 가능한 action 집합이 discrete할때만 유효한 것이다. 만약, action 집합이 continuous하다면, critic을 softmax로 모델링 할 수 없다. 이런 경우에는, critic을 각 state에 따른 action을 distribution으로 모델링하는 것이다. 예를 들어, state에 따른 action 분포를 gaussian distribution으로 모델링한다고 가정한다. 이때, critic은 이 gaussian distribution의 파라미터 $\\mu, \\sigma$를 모델링하게 된다. 즉, $\\theta = { \\theta_{\\mu}, \\theta_{\\sigma} }$가 된다. 때로는 discrete한 액션 집합이더라도, 그 수가 많고 촘촘하다면, continuous하게 취급하는 것도 도움이 된다. Distribution으로 모델링할 때의 장점은 distribution이기 때문에, 하나의 action에 대한 업데이트도 범위의 액션에 영향을 미치기 때문에 action generalization이 실현된다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"12. Controls with Approximation","date":"2020-03-22T00:22:08.000Z","path":"studynotes/reinforcement-learning/12-Controls-with-Approximation/","text":"Controls with Approximation Value function을 function approximation을 통해 모델링했다면, 이제, 주어진 value function으로 policy를 계산해 낼 차례이다. Representation of Actions Sarsa, expected Sarsa, Q-learning등을 하려면, state value function보단, action value function이 더 유용한데, 앞에서 봤던 function approximation방법은 state value function을 추정하는 것이었다. Action을 추가한 function approximation은 두 가지 방법이 있을 수 있다. Action별로 따로 function을 모델링한다. 즉, 어떤 action 1개당 1개의 function이 있는 셈이며, 위 그림처럼 stacking해서 하나의 linear 형태로 표현이 가능하다. 하지만, 이 경우, action끼리의 generalization이 일어나지 않는다. 즉, $a_0$의 function과 $a_1$의 function은 서로 다른 weight를 사용하기 때문에 서로 영향을 미치지 못한다. 신경망으로 치자면 다음처럼 될 것이다. 마지막 hidden layer의 output이 state의 representation이며, 그것과 최종 output layer사이의 weight는 action끼리 서로 공유되지 않는다. Action도 function의 입력으로 넣는다. 신경망의 output은 하나로 통일해버리고, action은 input으로 넣는 것이다. 이렇게되면, state뿐 아니라 action 사이에서도 weight가 공유되므로, action generalization도 수행될 것이다. Controls with Function Approximation Sarsa with Function Approximation 방법은 tabular Sarsa와 semi-gradient TD와 상당히 유사하다. Value function을 weight로 모델링한 후, weight와 policy를 initialization한다. 그리고, 다음 코드를 구현한다. Semi-gradient 방법을 사용한다. Expected Sarsa with Function Approximation Sarsa와 Expected Sarsa는 Bellman equation 형태만 조금 다르므로, 그것만 수정해주면 된다. 다음은 Sarsa의 업데이트 방식이다. $$ w \\leftarrow w + \\alpha(R_{t+1} + \\gamma \\cdot \\hat{q}(s_{t+1}, a_{t+1}, w) - \\hat{q}(s_t, a_t, w)) \\nabla \\hat{q}(s_t, a_t, w) $$ 다음은 expected Sarsa의 업데이트 방식이다. $$ w \\leftarrow w + \\alpha(R_{t+1} + \\gamma \\cdot \\sum_{a_{t+1}} \\pi(a_{t+1}|s_{t+1}) \\hat{q}(s_{t+1}, a_{t+1}, w) - \\hat{q}(s_t, a_t, w)) \\nabla \\hat{q}(s_t, a_t, w) $$ 다만, action방향으로 expectation을 계산할 수 있어야 한다. (여기서는 action set이 finite하다고 가정) Q-learning with Function Approximation Q-learning은 expected Sarsa의 특수한 형태이다. 즉, expectation값을 구하는 대신 maximum action value를 취한다. $$ w \\leftarrow w + \\alpha (R_{t+1} + \\gamma \\cdot \\underset{a_{t+1}}{ \\text{argmax} } ~ \\hat{q}(s_{t+1}, a_{t+1}, w) - \\hat{q}(s_t, a_t, w)) \\nabla \\hat{q}(s_t, a_t, w) $$ Exploration with Function Approximation Function approximation에서도 exploration-exploitation dilema가 발생한다. 따라서, 이를 완화시켜야 하는데, function approxmiation은 각 state 사이에 value generalization이 이뤄지기 때문에 tabular settings보다 exploration에서 제한적이다. 예를들어, optimistic initial value를 들어본다. Optimistic initial value setting에서는 처음에 value function을 매우 높은 값으로 초기화하고, 어떤 state를 방문할수록 방문한 state의 value function이 낮아지며, 자연스럽게 아직 방문하지 않은 state에 방문하게 된다. 학습이 진행될수록, 낮은 value를 가진 state의 value funciton은 낮은 값이 되어 더 이상 방문하지 않게 될 것이다. 하지만, function approximation setting에서는 이것이 유효하지 않은데, generalization이 이뤄지기 때문에 방문하지 않은 state의 value function도 같이 낮아진다는 것이다. 따라서, optimistic initial value의 의도와는 다르게 흘러가며, exploration이 제대로 이뤄지지 않는다. $\\epsilon$-greedy with Function Approximation 하지만, $\\epsilon$-greedy 방식은 어떤 function approximation 방법과도 융합될 수 있다. $1 - \\epsilon$확률에 따라 exploitation $$ a \\leftarrow \\underset{a}{ \\text{argmax} } ~ \\hat{q}(s, a, w) $$ $\\epsilon$확률에 따라 exploration $$ a \\leftarrow \\text{random}(a) $$ 이외에 function approximation setting에서의 exploration-exploitation 조화는 아직 연구중인 분야라고 한다. Average Rewards 지금까지, 어떤 state에서 어떤 action을 취했을 때의 value는 discounting을 이용한 future reward의 합으로 정의했다. 하지만, 이것은 discount라는 hyperparameter가 존재하며, 이것을 정하는 것은 어떤 문제를 푸느냐에 따라 크게 달라질 수 있다. 때로는 discount rate가 알고리즘을 잘못된 방향으로 학습시킬 수 있다(큰 reward를 받는게 너무 먼 미래인 경우 discount가 너무 많이 된다). 이것은 continuous task일때도, 마찬가지로, 당장은 작은 reward, 먼 미래에 다소 큰 reward를 받는 액션 중 택하는 문제에서, discount는 큰 영향을 준다. Continuous task를 위한 RL알고리즘들은 보통 discounting대신 average reward방식을 사용한다고 한다. Average reward는 이것을 해결하기 위해서 나왔으며, 어떤 policy를 따를 때, 앞으로 받을 reward의 평균을 말한다. $$ r(\\pi) = \\lim_{h \\rightarrow \\infty} \\frac{1}{h} \\sum_{t=1}^h E[R_t|S_t,A_{0:t-1} \\sim \\pi] $$ Value의 평균이 아니라, reward의 평균이다. 이는 다음처럼 generalize할 수 있다. $$ r(\\pi) = \\sum_s \\mu_{\\pi}(s) \\sum_a \\pi(a|s) \\sum_{s’,r} p(s’,r|s,a)r $$ $\\mu_{\\pi}(s)$는 $s$가 해당 policy $\\pi$에 따른 방문 횟수 비율 분포이다. 예를들어, 다음 environment가 있다고 했을 때, Left로만 가라고 하는 deterministic policy $\\pi_L$일때, state $s$에서의 average reward $r(\\pi_L)$은 0.2이다. 왼쪽으로만 가라고 하고, 왼쪽으로 갔을 때, 총 5개의 state를 지나며, 총 +1의 reward를 얻을 수 있으니, 왼쪽으로 가는 action을 선택했을 때 평균 reward는 0.2이다. 오른쪽으로는 갈 수가 없으므로, 고려하지 않는다. 반면, 오른쪽으로만 가라고 하는 deterministic poliyc $\\pi_R$의 경우, average reward $r(\\pi_R)$은 0.4이다. 따라서, 두 개의 policy를 average reward로 비교했을 때, $\\pi_R$이 더 좋다고 할 수 있다. 즉, average reward를 통해 어떤 policy가 더 좋은지 판단할 수 있다. Policy 하나당 하나의 average reward를 계산할 수 있다. 하나의 policy에서 average reward를 계산했다면, 그 policy내에서 어느 action이 좋은지에 대해서도 판단할 수 있다. 즉, value function을 새롭게 계산할 수 있다는 것이다. 이때, value는 다음처럼 정의한다. $$ G_t = (R_{t+1} - r(\\pi)) + (R_{t+2} - r(\\pi)) + \\cdots $$ 이때, $R - r(\\pi)$를 differential return이라고 부른다. 즉, value를 미래 reward의 총합이 아닌, differential reward의 총합으로 differential return을 정의한다. 따라서, action value Bellman equation은 다음처럼 변형된다. $$ q(s, a) = \\sum_{s’, r} p(s’, r|s, a) \\sum_{a’} [r - r(\\pi) + \\sum_{a’}\\pi(a’|s’) q(s’,a’)] $$ Differential Sarsa Differential return을 이용해서 Sarsa를 변형한 것을 말한다. 그런데, average reward를 계산할 때, TD error로 계산하는게 더 효과적이라고 한다. (이렇게 되면 average reward가 아니라 average value로 differential return을 계산?)","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"KL Divergence","date":"2020-03-03T13:28:59.000Z","path":"studynotes/machine-learning/KL-Divergence/","text":"KL-Divergence KL-Divergence에는 두 가지가 있다. Forward KL-Divergence Reverse KL-Divergence 기본적으로 KL-divergence라고 하면 forward 방식을 가리키며, variational autoencoder에는 reverse방식을 사용한다. Forward KL-Divergence 다음이 forward KL-divergence의 식이다. KLD(P∣∣Q)=∑xP(x)⋅log(P(x)Q(x))KLD(P||Q) = \\sum_x P(x) \\cdot log(\\frac{P(x)}{Q(x)}) KLD(P∣∣Q)=x∑​P(x)⋅log(Q(x)P(x)​) KL-divergence는 두 확률분포 P,QP,QP,Q의 유사도를 나타낼 수 있다. 즉, P,QP,QP,Q가 서로 비슷한 모양으로 분포된 확률분포라면, KLD값은 낮다. 이 KL-divergence는 entropy와 관련이 있는데, entropy는 정보량의 기댓값으로, 정보량은 두 확률 사이의 차이가 크면 큰 값을 가진다. 즉, 확률값이 많이 다르면 entropy가 높다. 두 확률분포간 거리를 최소화하는게 목적이 아니라면, P,QP,QP,Q에 두 확률분포를 넣고 거리를 구하면 된다. 보통 PPP는 target, true 확률분포가 들어가고 QQQ에는 측정하고자 하는 대상이 들어간다. 두 확률분포간 거리를 최소화시키고자 할때는, P(x)P(x)P(x)는 target 확률 분포, 즉, 목표로 하는 확률분포이며, Q(x)Q(x)Q(x)는 최적화시키고자 하는 확률분포, 즉, 파라미터가 있는 확률분포이다. 그리고 KLD 식을 최소화하는 Q(x)Q(x)Q(x)를 수정한다. 즉, P(x)P(x)P(x)에 가깝게 Q(x)Q(x)Q(x)를 수정하게 된다. Forward KLD의 특징 Forward KLD는 P(x)&gt;0P(x)&gt;0P(x)&gt;0인 모든 지점에 대해서 확률 분포간의 차이를 줄이려고 노력한다. 최적화된 결과, *P(x)&gt;P(x)&gt;P(x)&gt;를 만족하는 모든 xxx의 범위를 Q(x)Q(x)Q(x)가 커버하게 된다. 다만, 다음처럼, 최소화된 이후의 KLD 값은 상당히 클 수가 있다. (그림 출처: https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/) 위의 경우, KLD를 최소화한 결과지만, 결과값도 상당히 큰 KLD값을 가진다. 왜냐하면, P(x)&gt;0P(x)&gt;0P(x)&gt;0인 전체 범위를 커버하려고 하기 때문에, Q(x)Q(x)Q(x)를 정확하게 모델링하지 않으면(위의 경우, 두 가우시안의 mixture model로 해야 할 것이다) 위와 같은 문제가 생긴다. Reverse KL-Divergence 다음이 Reverse KL-divergence의 식이다. RKLD(Q∣∣P)=∑xQ(x)⋅log(Q(x)P(x))RKLD(Q||P) = \\sum_x Q(x) \\cdot log(\\frac{Q(x)}{P(x)}) RKLD(Q∣∣P)=x∑​Q(x)⋅log(P(x)Q(x)​) Reverse KLD의 특징 만약, 두 분포간의 거리를 측정하고자 하면, forward 방식과 별 다를게 없다. 다만, 값의 차이는 있다. KLD는 대칭함수가 아니기 때문이다. 하지만, 최소화하려고 할 경우, 이번엔 파라미터가 있는 Q(x)Q(x)Q(x)분포와 target 분포 P(x)P(x)P(x)의 자리가 바뀌었다. 이때는, Q(x)Q(x)Q(x)가 굳이 P(x)&gt;0P(x)&gt;0P(x)&gt;0를 만족하는 모든 xxx범위를 커버하려고 하지 않는다. 식에서 보면, Q(x)≈0Q(x) \\approx 0Q(x)≈0으로 맞춰버리면 그 xxx범위는 최소화가 된다. 즉, 필요한 곳만 볼록 솟게 해서 그 범위에서 최소화를 시키고 나머지 봉우리는 Q(x)≈0Q(x) \\approx 0Q(x)≈0으로 해버리므로, 특정 부분만 캡쳐해서 분포간 거리를 최소화한다. 따라서 다음 그림처럼 된다. (그림 출처: https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/) 어떤 KLD를 사용해야 할까 만약, 모델링한 Q(x)Q(x)Q(x)가 target 분포 P(x)P(x)P(x)와 매우 가깝다고 자신이 있을 경우, 또는 P(x)&gt;0P(x)&gt;0P(x)&gt;0인 모든 xxx를 커버해야 할 경우, forward KLD를 사용하자. 하지만, 모델링한 Q(x)Q(x)Q(x)가 target 분포 P(x)P(x)P(x)와 가깝다는 자신이 없고, 분포의 major한 부분만 캡쳐해도 좋은 경우, Reverse KLD를 사용하자. Reference https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"Hidden Markov Models 2","date":"2020-03-03T13:28:57.000Z","path":"studynotes/machine-learning/Hidden-Markov-Models-2/","text":"Hidden Markov Models Udemy 강좌: https://www.udemy.com/course/unsupervised-machine-learning-hidden-markov-models-in-python Hidden Markov model(HMM)은 다음과 같이 maximum likelihood estimation을 이용해서 파라미터를 추정하게 된다. θ∗=argmaxθ p(x∣θ)\\theta^* = \\underset{\\theta}{\\text{argmax}} ~ p(x|\\theta) θ∗=θargmax​ p(x∣θ) 다음으로, HMM의 파라미터가 무엇인지 적어본다. θ=?\\theta = ?θ=? Parameters of HMM Markov model에서의 parameter는 initial distribution vector π\\piπ와 state transition matrix AAA였다. HMM에서는 state-to-observation matrix BBB가 추가된다. 즉, π,A,B\\pi, A, Bπ,A,B가 학습 parameter가 된다. π\\piπ Initial distribution. row vector이며, hidden state 개수가 MMM개일때, π\\piπ는 (1,M)(1, M)(1,M) 모양이다. π(i)\\pi(i)π(i)하면, iii번재 state의 initial 확률이다. AAA Hidden state transition matrix. 간단하게 state transition matrix이라고도 하며, ttt에서의 hidden state가 주어졌을 때, t+1t+1t+1에서의 hidden state의 확률분포이다. 즉, p(st+1∣st)p(s_{t+1}|s_t)p(st+1​∣st​)을 표현한다. 따라서, observation의 종류가 DDD개일때, M→DM \\rightarrow DM→D이므로, (M,D)(M, D)(M,D) 모양이다. A(i,j)A(i, j)A(i,j)의 원소는 p(st+1=j∣st=i)p(s_{t+1} = j | s_t = i)p(st+1​=j∣st​=i)를 의미한다. BBB Observation transition matrix이며, ttt에서의 hidden state가 주어졌을 때, ttt에서의 observation의 확률분포이다. p(xt∣st)p(x_t|s_t)p(xt​∣st​)를 표현한다. B(j,k)B(j, k)B(j,k)의 원소는 p(xt=k∣st=j)p(x_t = k|s_t = j)p(xt​=k∣st​=j)를 의미한다. 이들을 이용한 연산의 예를 잠깐 몇개 들어보면, (Sequence의 시작 index는 1이다.) πB=∑iπ(i)B(i,:)=∑z1p(z1)p(x1∣z1)=p(x1)\\pi B = \\sum_i \\pi(i) B(i,:) = \\sum_{z_1} p(z_1)p(x_1|z_1) = p(x_1)πB=∑i​π(i)B(i,:)=∑z1​​p(z1​)p(x1​∣z1​)=p(x1​)이다. πAB=∑i,jπ(i)B(i,j)A(j,:)=∑z1,z2p(z1)p(z2∣z1)p(x2∣z2)=p(x2)\\pi A B = \\sum_{i,j} \\pi(i) B(i,j) A(j,:) = \\sum_{z_1, z_2} p(z_1)p(z_2|z_1)p(x_2|z_2) = p(x_2)πAB=∑i,j​π(i)B(i,j)A(j,:)=∑z1​,z2​​p(z1​)p(z2​∣z1​)p(x2​∣z2​)=p(x2​)이다. Algorithms of HMM HMM에서도 다른 확률 모델과 마찬가지로 forward propagation, backward propagation과정이 존재한다. Forward Algorithms HMM의 forward 알고리즘은 데이터셋의 확률, 즉, likelihood를 계산하는 알고리즘으로 대표된다. Markov model과는 달리, HMM의 likelihood는 곧바로 파라미터로 나타낼 수가 없어서 likelihood를 적절히 변형해야 한다. 그리고 단순히 변형해도, 그 계산의 time complexity가 매우 커서 계산 최적화를 위한 작업을 해 줘야 한다. Problem 1: Find Likelihood Distribution 파라미터 π,A,B\\pi, A, Bπ,A,B를 바탕으로 likelihood를 계산할 수 있어야 한다. Likelihood가 있어야 ML 추정법을 적용할 수 있기 때문. Likelihood는 p(x∣π,A,B)p(x|\\pi, A,B)p(x∣π,A,B)와 같으며, observation xxx의 joint distribution에 해당한다. 파라미터는 수식의 모든 term에 존재하므로, 생략한다. 먼저, likelihood는 다음과 같다. TTT는 전체 sequence 길이이다. 이 likelihood를 파라미터에 대한 식으로 바꿔주어야 한다. p(x)=p(x1,x2,...,xT)p(x) = p(x_1, x_2, ..., x_T) p(x)=p(x1​,x2​,...,xT​) 이것을 hidden Markov model 구조(확률 그래프 모델이니까)에 따라 factorize하기 위해, hidden variable zzz를 삽입한다 (Marginalize). p(x)=∑z1∑z2⋯∑zTp(x1,x2,...,xT,z1,z2,...,zT)p(x) = \\sum_{z_1} \\sum_{z_2} \\cdots \\sum_{z_T}p(x_1, x_2, ..., x_T, z_1, z_2, ..., z_T) p(x)=z1​∑​z2​∑​⋯zT​∑​p(x1​,x2​,...,xT​,z1​,z2​,...,zT​) 이제 factorize한다. p(x)=∑z1∑z2⋯∑zTp(z1)p(x1∣z1)∏t=2Tp(zt∣zt−1)p(xt∣zt)p(x) = \\sum_{z_1} \\sum_{z_2} \\cdots \\sum_{z_T} p(z_1) p(x_1|z_1) \\prod_{t=2}^{T} p(z_{t}|z_{t-1})p(x_t|z_t) p(x)=z1​∑​z2​∑​⋯zT​∑​p(z1​)p(x1​∣z1​)t=2∏T​p(zt​∣zt−1​)p(xt​∣zt​) 이제 parameter에 대한 식으로 바꿀 수 있다. p(x)=∑z1∑z2⋯∑zTπ(z1)B(z1,x1)∏t=2TA(zt−1,zt)B(zt,xt)p(x) = \\sum_{z_1} \\sum_{z_2} \\cdots \\sum_{z_T} \\pi(z_1) B(z_1, x_1) \\prod_{t=2}^T A(z_{t-1}, z_t) B(z_t, x_t) p(x)=z1​∑​z2​∑​⋯zT​∑​π(z1​)B(z1​,x1​)t=2∏T​A(zt−1​,zt​)B(zt​,xt​) 그런데, 이 식의 time complexity를 봐야 한다. 위 식은 결국, 모든 hidden state 조합을 더하는 것이다. Hidden state의 개수는 MMM개이고, 이게 TTT-time 만큼 있으므로, MTM^TMT개의 hidden state조합이 존재한다. 또한, 하나의 hidden state 조합을 구하기 위해서는 O(T)O(T)O(T)시간이 걸리며, 총 O(TMT)O(TM^T)O(TMT) 시간이 걸리게 된다. 이것은 exponential한 time으로, 매우 비효율적이다. Answer to Problem 1: Forward/Backward Algorithm 그런데, 위 p(x)p(x)p(x)에는 겹치는 연산이 상당히 많다. 이것을 factorize해서(인수분해) 좀 더 효율적으로 p(x)p(x)p(x)를 계산할 수 있을 것 같다. 우선, T=2,M=2T=2, M=2T=2,M=2인 경우를 생각해본다. p(x)=∑z1∑z2π(z1)B(z1,x1)∏t=2TA(zt−1,zt)B(zt,xt)p(x) = \\sum_{z_1} \\sum_{z_2} \\pi(z_1)B(z_1, x_1)\\prod_{t=2}^T A(z_{t-1}, z_t)B(z_t, x_t) p(x)=z1​∑​z2​∑​π(z1​)B(z1​,x1​)t=2∏T​A(zt−1​,zt​)B(zt​,xt​) =π(1)B(1,x1)A(1,1)B(1,x2)+= \\pi(1)B(1, x_1)A(1, 1)B(1, x_2) + =π(1)B(1,x1​)A(1,1)B(1,x2​)+ π(1)B(1,x1)A(1,2)B(2,x2)+\\pi(1)B(1, x_1)A(1, 2)B(2, x_2) + π(1)B(1,x1​)A(1,2)B(2,x2​)+ π(2)B(2,x1)A(2,1)B(1,x2)+\\pi(2)B(2, x_1)A(2, 1)B(1, x_2) + π(2)B(2,x1​)A(2,1)B(1,x2​)+ π(2)B(2,x1)A(2,2)B(2,x2)+\\pi(2)B(2, x_1)A(2, 2)B(2, x_2) + π(2)B(2,x1​)A(2,2)B(2,x2​)+ 그런데, 중복된 연산이 너무 많다. 따라서, factorize를 해 주자. p(x)=p(x) = p(x)= π(1)B(1,x1)[A(1,1)B(1,x2)+A(1,2)B(2,x2)]+\\pi(1)B(1, x_1)[A(1, 1)B(1, x_2) + A(1, 2)B(2, x_2)] + π(1)B(1,x1​)[A(1,1)B(1,x2​)+A(1,2)B(2,x2​)]+ π(2)B(2,x1)[A(2,1)B(1,x2)+A(2,2)B(2,x2)]\\pi(2)B(2, x_1)[A(2, 1)B(1, x_2) + A(2, 2)B(2, x_2)] π(2)B(2,x1​)[A(2,1)B(1,x2​)+A(2,2)B(2,x2​)] T=3,M=2T=3, M=2T=3,M=2인 경우도 마찬가지로 할 수 있다. 수식으로 보면 다음처럼 표현할 수 있다. p(x)=∑z1∑z2∑z3p(z1)p(x1∣z1)∏t=23p(zt∣zt−1)p(xt∣zt)p(x) = \\sum_{z_1} \\sum_{z_2} \\sum_{z_3} p(z_1) p(x_1|z_1) \\prod_{t=2}^3 p(z_t|z_{t-1})p(x_t|z_t) p(x)=z1​∑​z2​∑​z3​∑​p(z1​)p(x1​∣z1​)t=2∏3​p(zt​∣zt−1​)p(xt​∣zt​) =∑z1∑z2∑z3p(z1)p(x1∣z1)p(z2∣z1)p(x2∣z2)p(z3∣z2)p(x3∣z3)= \\sum_{z_1} \\sum_{z_2} \\sum_{z_3} p(z_1)p(x_1|z_1)p(z_2|z_1)p(x_2|z_2)p(z_3|z_2)p(x_3|z_3) =z1​∑​z2​∑​z3​∑​p(z1​)p(x1​∣z1​)p(z2​∣z1​)p(x2​∣z2​)p(z3​∣z2​)p(x3​∣z3​) =∑z3p(x3∣z3)∑z2p(x2∣z2)p(z3∣z2)∑z1p(z1)p(x1∣z1)p(z2∣z1)= \\sum_{z_3} p(x_3|z_3) \\sum_{z_2} p(x_2|z_2)p(z_3|z_2) \\sum_{z_1} p(z_1)p(x_1|z_1)p(z_2|z_1) =z3​∑​p(x3​∣z3​)z2​∑​p(x2​∣z2​)p(z3​∣z2​)z1​∑​p(z1​)p(x1​∣z1​)p(z2​∣z1​) 위 식을 다음처럼 변형해본다. ∑z3p(x3∣z3)∑z2p(z3∣z2)[p(x2∣z2)∑z1p(z2∣z1)[p(x1∣z1)p(z1)]]\\sum_{z_3} p(x_3|z_3) \\sum_{z_2} p(z_3|z_2) [p(x_2|z_2) \\sum_{z_1} p(z_2|z_1)[p(x_1|z_1) p(z_1)]] z3​∑​p(x3​∣z3​)z2​∑​p(z3​∣z2​)[p(x2​∣z2​)z1​∑​p(z2​∣z1​)[p(x1​∣z1​)p(z1​)]] 여기서, α\\alphaα라고 하는 놈을 정의한다. α(3,z3)=p(x3∣z3)∑z2p(z3∣z2)α(2,z2)\\alpha(3, z_3) = p(x_3|z_3) \\sum_{z_2} p(z_3|z_2) \\alpha(2, z_2) α(3,z3​)=p(x3​∣z3​)z2​∑​p(z3​∣z2​)α(2,z2​) α(2,z2)=p(x2∣z2)∑z1p(z2∣z1)α(1,z1)\\alpha(2, z_2) = p(x_2|z_2) \\sum_{z_1} p(z_2|z_1) \\alpha(1, z_1) α(2,z2​)=p(x2​∣z2​)z1​∑​p(z2​∣z1​)α(1,z1​) α(1,z1)=p(x1∣z1)p(z1)\\alpha(1, z_1) = p(x_1|z_1)p(z_1) α(1,z1​)=p(x1​∣z1​)p(z1​) 이때, p(x)p(x)p(x)는 다음처럼 된다. p(x)=∑z3α(3,z3)p(x) = \\sum_{z_3}\\alpha(3, z_3) p(x)=z3​∑​α(3,z3​) 즉, 다음처럼 일반화가 가능하다. p(x)=∑zTα(T,zT)p(x) = \\sum_{z_T} \\alpha(T, z_T) p(x)=zT​∑​α(T,zT​) α(t,zt)=p(xt∣zt)∑zt−1p(zt∣zt−1)α(t−1,zt−1)\\alpha(t, z_t) = p(x_t|z_t) \\sum_{z_{t-1}} p(z_t|z_{t-1}) \\alpha(t-1, z_{t-1}) α(t,zt​)=p(xt​∣zt​)zt−1​∑​p(zt​∣zt−1​)α(t−1,zt−1​) α(1,z1)=p(x1∣z1)p(z1)\\alpha(1, z_1) = p(x_1|z_1)p(z_1) α(1,z1​)=p(x1​∣z1​)p(z1​) 이렇게 되면, likelihood p(x)p(x)p(x)를 계산하는데, O(MT)O(MT)O(MT)면 끝이 난다. Problem 2: Find the Most Likely Sequence of Hidden States Likelihood를 구했다면, 이번엔 가장 probable한 hidden states의 sequence를 찾을 수 있어야 한다. 즉, z∗=argmaxz p(z∣x)z^* = \\underset{z}{ \\text{argmax} } ~ p(z|x) z∗=zargmax​ p(z∣x) 를 만족하는 hidden states zzz의 joint distribution을 계산할 수 있어야 한다. 그런데, 이때, 위 식은 다음처럼 정리가 가능하다. z∗=argmaxz p(z∣x)=argmaxz p(x,z)p(x)=argmaxz p(x,z)z^* = \\underset{z}{ \\text{argmax} } ~ p(z|x) = \\underset{z}{ \\text{argmax} } ~ \\frac{p(x,z)}{p(x)} = \\underset{z}{ \\text{argmax} } ~ p(x, z) z∗=zargmax​ p(z∣x)=zargmax​ p(x)p(x,z)​=zargmax​ p(x,z) 그런데, 여기서, p(x,z)p(x, z)p(x,z)는 p(x)p(x)p(x)를 구하는 식에서 marginalization만 빼면 된다. 즉, p(x,z)=p(z1)p(x1∣z1)∏i=2Tp(zt∣zt−1)p(xt∣zt)p(x, z) = p(z_1)p(x_1|z_1) \\prod_{i=2}^T p(z_{t}|z_{t-1})p(x_t|z_t) p(x,z)=p(z1​)p(x1​∣z1​)i=2∏T​p(zt​∣zt−1​)p(xt​∣zt​) 이다. 하나의 joint probability를 계산하려면 O(T)O(T)O(T)시간이 걸리는 셈. 그러면, observations들에 맞게 가장 그럴듯한 hidden state들을 찾으려면, hidden state의 모든 조합을 저 식에 넣어보고 가장 큰 확률값을 주는 조합을 고르면 될 것이다. 그러나, 이 방법은 O(TMT)O(TM^T)O(TMT)가 걸린다. Answer to Problem 2: Viterbi Algorithm 지금, p(x,z)p(x, z)p(x,z)가 가장 큰 zzz조합을 구해야 한다. HMM은 Markov model이기 때문에 t−1t-1t−1까지 최적의 zzz sequence를 구해놨다면, ttt에서의 ztz_tzt​는 greedy하게 선택하면 ttt까지의 zzz sequence는 optimal이다. 즉, t=1t=1t=1에서, p(z1)p(x1∣z1)p(z_1)p(x_1|z_1)p(z1​)p(x1​∣z1​)이 최대가 되는 z1z_1z1​를 구하고, t=2t=2t=2에서, p(z2∣z1)p(x2∣z2)p(z_2|z_1)p(x_2|z_2)p(z2​∣z1​)p(x2​∣z2​)이 최대가 되는 z2z_2z2​를 구하고 이런식으로 앞에서부터 greedy하게 선택해도 된다는 것이다. Problem 3: Training HMM 다음을 만족하는 parameter π,A,B\\pi, A, Bπ,A,B를 계산한다. A∗,B∗,π∗=argmaxA,B,π p(x∣A,B,π)A^* , B^* , \\pi^* = \\underset{A,B,\\pi}{ \\text{argmax} } ~ p(x|A,B,\\pi) A∗,B∗,π∗=A,B,πargmax​ p(x∣A,B,π)","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"Hidden Markov Models 1","date":"2020-03-03T13:28:55.000Z","path":"studynotes/machine-learning/Hidden-Markov-Models-1/","text":"Hidden Markov Models Udemy 강좌: https://www.udemy.com/course/unsupervised-machine-learning-hidden-markov-models-in-python Markov Assumption Markov property라고도 부르며, time-series 데이터나, 상태 기반 데이터에서, 현재의 상태는 오로지 바로 이전 상태만으로부터 영향을 받는다는 가정이다. 즉 다음과 같다. P(st∣st−1st−2⋯s1)=P(st∣st−1)P(s_t|s_{t-1}s_{t-2}\\cdots s_1) = P(s_t|s_{t-1}) P(st​∣st−1​st−2​⋯s1​)=P(st​∣st−1​) 이전 상태들이 주어졌을 때, 현재 상태의 확률 분포는 오로지 바로 앞전 상태만으로부터 영향을 받는다. 즉, st−1s_{t-1}st−1​이 주어진다면, sts_tst​는 st−2,...,s1s_{t-2},...,s_1st−2​,...,s1​와 독립이다(Conditional independence). Markov assumption은 상당히 강력한 가정으로, 많은 분야에 응용되지만(자연어와 같은 time-series, state machine 기반 모델 등), 바로 이전 상태를 제외한 그 이전 상태들을 모두 무시하므로, 성능에 한계가 있다. 보통 markov assumption하면 first-order markov assumption을 의미하며, 이전 몇 개의 데이터로부터 영향을 받게 할 것인가에 따라 second-order, third-order 등이 있다. Second-order markov assumption은 다음과 같다. P(st∣st−1,⋯ ,s1)=P(st∣st−1,st−2)P(s_t|s_{t-1}, \\cdots, s_1) = P(s_t|s_{t-1}, s_{t-2}) P(st​∣st−1​,⋯,s1​)=P(st​∣st−1​,st−2​) Third-order markov assumption은 다음과 같다. P(st∣st−1,⋯ ,s1)=P(st∣st−1,st−2,st−3)P(s_t|s_{t-1},\\cdots,s_{1}) = P(s_t|s_{t-1},s_{t-2},s_{t-3}) P(st​∣st−1​,⋯,s1​)=P(st​∣st−1​,st−2​,st−3​) 그런데, 예상하다시피, 마르코프 가정으로 구현한 모델은 이전 모든 상태에 영향을 받게 모델링한 모델보다 성능이 떨어질 가능성이 높다. 그럼에도 불구하고 사용하는 이유는, 우리가 관심있는것은 지금까지 지나온 상태들의 joint distribution인데, 마르코프 가정이 없다면, joint distribution계산 과정이 매우 복잡해진다. 그래서, 쉽게 모델링하기 위해 마르코프 가정을 사용하며, 성능도 쓸만한 편이다. Markov Models 마르코프 가정(Markov assumption)을 바탕으로 모델링한 모델을 말한다. 다음과 같이 state machine도 마르코프 모델 중 하나이다. State machine은 일반적으로 다음 상태의 확률은 오직 현재 상태에 의해 영향을 받고 결정된다. 위와 같은 state machine에서의 transition probabilities는 행렬로 표현이 가능하며 이러한 행렬을 state transition probability matrix라고 부른다. 마르코프 모델에서는 현재에 기반한, 다음 상태 또는 다음 무언가의 확률 분포를 matrix로 표현이 가능하며, MMM개의 노드가 있을 때, transition probability matrix는 MMMxMMM행렬로 표현한다. State transition probability matrix의 한 행 원소의 합은 1이어야 한다. iii번째 행이 의미하는 것은 sis_isi​를 기반으로 다음 state의 확률분포이기 때문이다. Starting Position 지금까지 지나온 상태들의 joint distribution을 계산해 보면 다음과 같다. P(st,st−1,⋯ ,s1)=P(s1)∏i=2tP(si∣si−1)P(s_t,s_{t-1},\\cdots,s_1) = P(s_1) \\prod_{i=2}^{t} P(s_i|s_{i-1}) P(st​,st−1​,⋯,s1​)=P(s1​)i=2∏t​P(si​∣si−1​) State transition probability matrix를 정의했다면, P(si∣si−1)P(s_i|s_{i-1})P(si​∣si−1​)는 알 수 있다. 그런데, 초기 상태인 P(s1)P(s_1)P(s1​)는 행렬에 없다. 따라서, initial distribution을 정의해 주어야 하며, 111xMMM 벡터로 구성된다. Training of Markov Models 마르코프 모델의 학습은 MLE(Maximum Likelihood Estimation)으로 이루어진다. 즉, sj→sis_j \\rightarrow s_isj​→si​로의 확률 분포는 데이터셋에서 sjs_jsj​ 다음으로 sis_isi​가 얼만큼의 비율로 등장하느냐에 따라 결정된다. 예를들어, 다음의 문장이 있다. “I like cats” “I like dogs” “I love kangaroos” 그럼 상태의 집합은 {I,like,cats,dogs,love,kangaroos}\\{\\text{I}, \\text{like}, \\text{cats}, \\text{dogs}, \\text{love}, \\text{kangaroos}\\}{I,like,cats,dogs,love,kangaroos}이렇게 6개의 원소로 구성되어 있으며, initial distribution은 P(I)=1P(\\text{I})=1P(I)=1이고, 나머지 단어의 경우, 0이다. 또한, P(like∣I)=0.66,P(love∣I)=0.33,P(else∣I)=0P(\\text{like}|\\text{I})=0.66, P(\\text{love}|\\text{I})=0.33, P(else|\\text{I})=0P(like∣I)=0.66,P(love∣I)=0.33,P(else∣I)=0이다. Smoothing 그런데, 확률이 0이라는 것은 매우 위험하다. 반대로, 어떤 것의 확률이 1이라는 것 또한 매우 위험하다. MLE에 의해 트레이닝 데이터에 나오지 않은 것들은 모두 0이 되버리는데, 이는 오버피팅을 야기한다. 따라서 학습 데이터에 모든 경우의 수가 다 들어있기를 바래야 하는데, 이는 비현실적이다. 따라서 어떤 것에 1 또는 0의 확률을 할당하는 것을 피해야 하는데, 방법으로는 smoothing이라는 것이 있다. smoothing이란, 0확률을 막아주는 기법을 의미하는데, 다음의 경우가 있다. No smoothing 기본적인, smoothing을 적용하지 않은 경우. P(si∣sj)=count(sj→si)count(sj→∗)P(s_i|s_j) = \\frac{\\text{count}(s_j \\rightarrow s_i)}{\\text{count}(s_j \\rightarrow *)} P(si​∣sj​)=count(sj​→∗)count(sj​→si​)​ Add-one smoothing 분자에 +1, 분모에 +MMM을 해 준다. P(si∣sj)=count(sj→si)+1count(sj→∗)+MP(s_i|s_j) = \\frac{\\text{count}(s_j \\rightarrow s_i) + 1}{\\text{count}(s_j \\rightarrow *) + M} P(si​∣sj​)=count(sj​→∗)+Mcount(sj​→si​)+1​ 이때, MMM은 상태의 개수(자연어의 경우엔, 단어 개수)이다. 이러면, 모든 확률은 1 또는 0이 되지 않으며, ∑iP(si∣sj)=1\\sum_i P(s_i|s_j)=1∑i​P(si​∣sj​)=1이 유지된다. Add-epsilon smoothing 분자에 +1이 아니라, +ϵ\\epsilonϵ을 해 준다. 분모에는 +ϵM\\epsilon MϵM을 해 준다. P(si∣sj)=count(sj→si)+ϵcount(sj→∗)+ϵMP(s_i|s_j) = \\frac{\\text{count}(s_j \\rightarrow s_i) + \\epsilon}{\\text{count}(s_j \\rightarrow *) + \\epsilon M} P(si​∣sj​)=count(sj​→∗)+ϵMcount(sj​→si​)+ϵ​ 이때, ϵ\\epsilonϵ은 학습 파라미터로써, 추론해도 되고 hyper parameter로 해도 된다. Add-one 스무딩이 때로는 너무 강하거나 너무 약할때가 있다. 따라서, 스무딩의 강도를 조정하겠다는 이야기가 된다. Markov Chains 마르코프 모델이면서, 확률 과정(stochastic process)을 모델링한 것을 의미한다. 보통 통계에서샘플링이라 함은 샘플 하나를 얻는 과정을 말하지만, stochastic(random) process에서의 샘플링은 sequence of random variables을 얻는 과정이고, 하나의 샘플이 time series이다. 마르코프 체인 역시 stochastic process이며, 하나의 샘플은 time-series이다. State transition probability distribution matrix를 AAA라고 하고, initial distribution을 π\\piπ라고 했을 때, ttt번째 상태에서의 marginal distribution은 다음과 같다. P(st)=πAtP(s_t) = \\pi A^{t} P(st​)=πAt 이때, AAA의 iii번째 row는 iii번 상태에서 다른 상태로 갈 확률분포이며, AAA는 MMMxMMM 행렬이고, π\\piπ는 1xMMM벡터이다. 따라서, 위 식은 1xMMM벡터가 나온다. Marginal distribution에 대해 잠깐 설명해보면, 예를들어, 첫번째 상태 s1s_1s1​의 확률분포는 다음과 같다. P(s1)=∑s0P(s1,s0)=∑jπjAj,i=πAP(s_1) = \\sum_{s_0} P(s_1,s_0) = \\sum_j \\pi_j A_{j,i} = \\pi A P(s1​)=s0​∑​P(s1​,s0​)=j∑​πj​Aj,i​=πA Stationary Distribution 그런데, AAA를 반복해서 곱하다 보면(확률 과정을 반복), 어느 순간 marginal distribution의 변화가 다음과 같은 상태가 된다. P(st)=πAt=P(st−1)=πAt−1P(s_t) = \\pi A^t = P(s_{t-1}) = \\pi A^{t-1} P(st​)=πAt=P(st−1​)=πAt−1 이때, p(s)=p(s)Ap(s)=p(s)Ap(s)=p(s)A를 만족한다. 이때, p(s)p(s)p(s)를 stationary distribution이라고 부른다. 이 stationary distribution p(s)p(s)p(s)을 보면, 행렬 AAA의 전치행렬인 ATA^TAT의 eigenvector(p(s)p(s)p(s)는 벡터이다)와 같은 성질이다는 것을 알 수 있다. 다만, 그에 상응하는 eigen value는 1이다. Limiting Distribution 그래서, 어떤 stochastic process의 최종 distribution은 무엇일까. 이 최종 distribution은 limiting distribution 또는 equilibrium distribution이라고 부른다. 즉, 다음과 같다. p(s∞)=πA∞p(s_\\infty) = \\pi A^\\infty p(s∞​)=πA∞ 그런데, 이건 stationary distribution과 같은가? 일단, limiting distribution은 stationary distribution이다. 하지만, 모든 stationary distribution이 다 limiting distribution이 되는 건 아니다. Eivenvector는 최대 AAA의 차원만큼 개수가 존재하며, 그중에서 eigen value가 1인 eigen vector는 여러개 일 수 있다. 이들 중 어느놈이 limiting distribution일까… 일단, limiting distribution이 구해지면, 그 stochastic process를 통해 앞으로 나올 time series를 샘플링할 수 있다(MCMC의 원리?). Perron-Frobenius Theorem 선형 대수학에서의 어떤 이론인데, stochastic process에 맞아떨어지는 이론이다. 어떤 행렬 A=(ai,j)A = (a_{i,j})A=(ai,j​)에 대해, AAA는 nnn-by-nnn matrix이고, 모든 원소가 양수이면, AAA의 가장 큰 양수 eigenvalue rrr이 존재하고 그와 상응하는 eigenvector의 모든 원소는 양수이다. 그리고, 모든 원소가 양수인 eigenvector는 이 eigenvector가 유일하며, 다른 eigenvector는 반드시 음수가 하나이상 포함되어 있다. Stochastic process에서 다음 두 가지 조건을 만족시킨다면, 그 Markov chain은 반드시 유일한 stationary distribution을 가지며, 따라서, 해당 stationary distribution은 limiting distribution이라고 확신할 수 있다. Transition matrix AAA에 대해, ∑jai,j=1\\sum_j a_{i,j} = 1∑j​ai,j​=1, 즉, 한 row의 모든 원소 합이 1이다. 하나의 row는 probability distribution이다. ai,j≠0a_{i,j} \\not = 0ai,j​​=0, 어떠한 원소도 0이 아니다. 여기서, transition matrix AAA의 eigenvector는 distribution으로써의 역할을 해야 하므로 모두 양수여야 하는데, 그런 조건을 만족하는 eigenvector는 오직 하나밖에 없으므로, 이놈이 limiting distribution이라고 확신할 수 있다. Application of Markov Models Language Models‌ Second-order language model을 예로 들자. 먼저, 문장의 첫 두 단어에 대한 initial distribution을 만들고 앞 두 단어가 주어졌을 때, 현재 단어에 대한 transition matrix를 만든다.‌ 학습은 실제 문장들로 학습하며, 문장에서 앞 두 단어가 주어졌을 때, 현재 자리에 오는 단어의 비율을 transition matrix로 한다. 만약, 현재 단어가 끝 단어라면, 이 단어가 끝 단어일 확률 계산에 추가해준다.‌ 앞 $$k$$개의 단어를 바탕으로 현재 단어를 추정하는 Markov model이다. Google’s PageRank Algorithms‌ Google의 페이지랭크 알고리즘은 각 페이지를 방문할 확률인 stationary distribution(정확히는 limiting distribution)이 높은 순서대로 랭크를 매기는 것을 말한다. 한 페이지에서 다른 페이지로 가는 링크가 있을 것이고, AAA페이지에서 MMM개의 링크가 있고, BBB페이지로 가는 링크가 존재한다면, A→BA→BA→B 로의 transition probability는 1M\\frac{1}{M}M1​이 된다. 이렇게 transition matrix를 정의하고, matrix에서 0인 원소들을 smoothing을 이용해서 없앤 후, stationary distribution을 계산한다. 현재 페이지에서 다음 페이지로 갈 확률이 존재하는 Markov model이다. Hidden Markov Models 마르코프 모델에서 hidden state상태를 추가한 형태. hidden state가 markov chain을 이루고 hidden unit에서 visible variable이 컨디셔닝 되어 나온다. 다음 그림은 markov chain과 hidden markov chain을 표현한 것인데, 노란색이 hidden unit들, 파란색이 visible unit을 표현한 것이다. Hidden markov model에서는 observable state oto_tot​가 이전 observable state ot−1o_{t-1}ot−1​에 영향을 받지 않는다. 대신 같은 시간의 hidden state인 hth_tht​에 의해서만 영향을 받는다는 가정을 한다. Markov model은 initial distributoin π\\piπ와 transition probability matrix AAA가 존재하지만, hidden markov model에서는 initial distribution π\\piπ와 hidden state transition matrix AAA, hidden state로부터 visible state로의 변환을 의미하는 transition matrix BBB가 존재한다. Application of HMM 다음과 같은 application이 존재할 수 있다. Parts of Speech (POS) Tagging Systems Stock Price Models Parts of Speech (POS) Tagging Systems 각 단어를 visible unit으로, 명사인지 동사인지, 형용사인지 등을 hidden state로 삼아서 HMM을 모델링하는 것을 말한다. 크게, 음성 시그널을 최외곽 visible variable, 단어를 hidden state로 삼아서 markov chain을 구성하는데, 이 애들이 다시 다른 HMM에 들어가는 방식이라고 생각하면 된다. Stock Price Models HMM이 hidden time series(zzz들)를 캐치할 수 있다는 것에 주목해서 stock price의 hidden factor를 HMM으로 캐치하게 한 모델을 말한다. 이때, visible variable은 deterministic한 것이 아니라 generative하게 distribution으로 모델링할 수도 있다(위 그림처럼).","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"Restrict Boltzmann Machines 2","date":"2020-03-03T13:28:53.000Z","path":"studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd/","text":"Restrict Boltzmann Machines 이번에는 RBM의 학습, 즉, weight를 수정하는 방법을 알아보고자 한다. RBM의 학습 역시 MLE 방식을 이용하게 된다. 즉, 데이터에 대한 likelihood를 최대화하게 된다. $$ \\hat{W}, \\hat{b}, \\hat{c} = \\underset{W,b,c}{ \\text{argmax} } ~ p(v) $$ 데이터는 visible unit에 들어가게 되므로, $$p(v)$$를 최대화하게 된다. 그런데 왜 $$p(v)$$를 최대화하는게 RBM의 학습일까. 그 이유는, 현재 데이터가 얻어진 이유는 상대적으로 높은 확률을 가지기 때문에 얻어진 것일 것이고, 왠만하면 불안정 상태보다 안정 상태의 데이터일 가능성이 높다. 그런데, $$p(v)$$는 intractable하다. $$ p(v) = \\sum_h p(v,h) = \\sum_h \\frac{1}{Z} \\text{exp} { \\sum_i \\sum_j W_{i,j} v_i h_j + \\sum_i b_i v_i + \\sum_j c_j h_j } $$ 여기서, normalizaiton constant는 모델 크기가 매우 작지 않는 이상 intractable하다. Free Energy 일단 미분을 위해 다음을 정의한다. $$ F(v) = -\\text{log} \\sum_h e^{-E(v,h)} $$ $F(v)$를 Free energy라고 정의한다. 특별한 의미가 있는것은 아니라고 한다. 편의상 정의하는 것 같다. Free energy식을 조금만 변형한다. $$ e^{-F(v)} = \\sum_h e^{-E(v, h)} $$ Free energy를 이용해서 $p(v)$를 $F(v)$에 대한 식으로 정리해보면, $$ p(v) = \\sum_h p(v,h) = \\sum_h \\frac{1}{Z} e^{-E(v,h)} \\ p(v) = \\frac{1}{Z} e^{-F(v)} $$ 따라서, normalization constant는 다음과 같다고도 할 수 있다. $$ Z = \\sum_{v’} e^{-F(v’)} $$ 이제, $\\text{log} ~ p(v)$를 미분해보면, $$ \\frac{d \\text{log} ~p(v)}{d\\theta} = \\frac{d}{d\\theta}[\\text{log} \\frac{1}{Z}e^{-F(v)}] $$ $$ = \\frac{d}{d\\theta} [\\text{log} ~e^{-F(v)} - \\text{log}~Z] $$ $$ = \\frac{d}{d\\theta}[-F(v)] - \\frac{d}{d\\theta}[\\text{log} Z] $$ $$ = -\\frac{dF(v)}{d\\theta} - \\frac{1}{Z} \\frac{dZ}{d\\theta} $$ $$ = -\\frac{dF(v)}{d\\theta} - \\frac{1}{Z} \\frac{d}{d\\theta}[\\sum_{v’} e^{-F(v’)}] $$ $$ = -\\frac{dF(v)}{d\\theta} + \\sum_{v’} \\frac{1}{Z} e^{-F(v’)} \\frac{dF(v’)}{d\\theta} $$ $$ = -\\frac{dF(v)}{d\\theta} + \\sum_{v’} p(v’) \\frac{dF(v’)}{d\\theta} $$ 이때, 양변에 - 부호를 곱해주면서, $$ -\\frac{d \\text{log} ~p(v)}{d\\theta} = \\frac{dF(v)}{d\\theta} - \\sum_{v’}p(v’)\\frac{dF(v’)}{d\\theta} $$ 그런데, 이때, 두번째 항은 $E[\\frac{dF(v’)}{d\\theta}]$와 같다. $$ -\\frac{d\\text{log} ~ p(v)}{d\\theta} = \\frac{dF(v)}{d\\theta} - E[\\frac{dF(v’)}{d\\theta}] $$ 이제 이것을 적분해보면, negative log likelihood를 얻을 수 있다. 이는 곧 cost와 같다. $$ \\mathbb{L} = F(v) - E[F(v’)] $$ 이 loss는 두 개의 term으로 나뉘는데, 첫번째는 positive term이고, 두번째는 negative term인데, 다음과 같은 역할을 한다. Positive term Visible unit에 입력으로 들어간 데이터에 대한 에너지는 낮게 유도하는 효과가 있다. Negative term Visible unit에 입력으로 들어간 데이터 이외에 모든 조합들에 대해 에너지를 높게 유도한다. 사실상 대부분 EBM은 위와 같은 cost를 가진다고 한다. (또는 가저야만 한다고 한다) Contrastive Divergence EBM에서, positive data와 negative data를 가지고, positive data의 에너지는 상대적으로 낮게, negative data의 에너지는 상대적으로 높게 학습시키는 방법을 말한다. 위 cost function을 보면, negative term은 intractable하다. 가능한 모든 visible unit 조합이 필요하기 때문. 따라서 기댓값을 추정해야 하는데, 기댓값은 Monte Carlo estimation처럼 샘플들의 평균으로 추정할 수 있다. 즉, negative term은 다음처럼 표현이 가능하다. $$ E[F(v’)] = \\frac{1}{n} \\sum_{i=1}^n F(v’_i) $$ Visible unit을 샘플링해야 위 수식처럼 negative term을 추정할 수 있는데, visible unit을 샘플링해야 한다는 이야기가 된다. 이것은 RBM으로 샘플링이 가능하다. Gibbs Sampling Gibbs sampling이라고 하면, Markov chain Monte Carlo의 일종이다. Markov chain Monte Carlo 방법은 어떤 random variable에 대해 모델링한 후, Markov chain을 통해 샘플을 생성하는데, 모델이 여러 random variable을 포함하는 경우, Gibbs sampling을 이용한다. Gibbs sampling은 하나의 random variable 이외에 다른 random variable이 모두 주어졌다(given)고 가정하고 샘플링하는 방식이다. RBM은 다음과 같은 과정으로 샘플을 생성한다. 주어진 데이터로 $p(h=1|v)$를 계산한다. $p(h=1|v)$를 이용해서 $h$를 샘플링한다. 샘플링한 $h$를 이용해서 $p(v’=1|h)$를 계산한다. $v’$을 샘플링한다. 이렇게 하면, visible unit하나를 새로 샘플링한 것이다. RBM의 구조는 Markov chain의 구조이며(visible unit은 바로 앞의 hidden unit들에게 의해서만 영향을 받음), 위 처럼 샘플링하는 것은 MCMC의 일종이다. 특히, Gibbs sampling의 일종이라고 하는데, 왜 Gibbs sampling인지는 잘 모르겠다. 여러 블로그들은 그저 “이것은 Gibbs sampling이다” 라고만 소개되어 있고, 이유를 소개한 블로그는 찾기 힘들다. CD-k (Contrastive Divergence with k Samples) 그럼 visible unit을 어떻게 샘플링해야 하는지는 결정되었고(RBM을 통한 MCMC 샘플링), 과연 몇 개의 샘플이 필요한가가 논의될 필요가 있다. MCMC를 통한 기댓값 추정은 MCMC의 이름에서 알수 있다시피, Monte Carlo 추정법을 이용하게 된다. 그리고, Monte Carlo 추정은 샘플이 많을수록 좋고 정확하다. 하지만, RBM에서는 단 하나의 샘플로만 해도 괜찮다고 한다. 즉, CD-1 방식을 이용한다. RBM의 학습은, 데이터셋에서 관찰되는 visible unit조합들에 대해서의 에너지는 높게, 그 이외의 조합들에 대해서는 에너지가 낮게 하는게 목적이다. CD-1 방식은 CD-k 중에서는 그 효과가 가장 떨어진다고 할지라도, 크게 문제가 되지 않는다고 한다. Fake Loss RBM의 loss는 직접 계산할 수 없다. Negative term이 intractable하기 때문이다. 따라서, 이를 Monte Carlo 추정법으로 추정했다(1개의 샘플로). 즉, 다음처럼 loss가 수정될 수 있다. $$ \\text{fake-loss} = F(v) - F(v’) $$ Fake loss는 공식적으로 붙은 이름은 아니다. 진짜 loss는 아니지만, loss를 추정한 것이라서 이렇게 부르기로 한다. 진짜 loss는 다음 식이다. $$ \\text{loss} = F(v) - \\mathbb{E}_{v’}[F(v’)] $$ Intractability of Free Energy 그런데, free energy의 식을 보면 가능한 모든 hidden unit 조합에 대한 summation이 있다. $$ F(v) = -\\text{log} \\sum_h e^{-E(v,h)} $$ Hidden unit 조합을 모두 구한다는 것도 사실상 intractable하다. Free energy를 tractable하게 변환할 수 있다면, loss 함수를 계산할 수 있을 것이다. Free energy 식에서, $E(v,h)$를 원래 에너지 식으로 대체한다. $$ F(v) = - \\text{log} \\sum_h \\text{exp}(\\sum_i \\sum_j W_{i,j}v_ih_j + \\sum_i b_iv_i + \\sum_j c_j h_j) $$ 그리고 다음처럼 $$h$$와 관계없는 항은 제일 밖으로 뺄 수 있다. $$ F(v) = -\\text{log} ~\\text{exp}(\\sum_i b_i v_i) \\sum_h \\text{exp}(\\sum_i \\sum_j W_{i,j}v_i h_j + \\sum_j c_jh_j) $$ $$ = -\\sum_i b_iv_i - \\text{log} \\sum_h \\text{exp} (\\sum_i \\sum_j W_{i,j} v_i h_j + \\sum_j c_j h_j) $$ $\\text{exp}$안에 $\\sum_j$가 공통으로 포함되어 있으므로 밖으로 뺀다. $$ F(v) = -\\sum_i b_i v_i - \\text{log} \\sum_h \\prod_j \\text{exp} (\\sum_i W_{i,j} v_i h_j + c_j h_j) $$ $h_j$도 빼보자. $$ F(v) = - \\sum_i b_i v_i - \\text{log} \\sum_h \\prod_j \\text{exp} {h_j(\\sum_iW_{i,j} v_i + c_j)} $$ 그리고, $u_{j} = \\sum_i W_{i,j} v_i + c_j$라고 해 보자(어차피 $i$방향으로는 summation이므로 변수가 아니다. 따라서 $j$로만 인덱싱한다). $$ F(v) = -\\sum_i b_i v_i - \\text{log}\\sum_h \\prod_j \\text{exp} {h_j u_j} $$ $h_j$는 베르누이 변수이므로, ${0, 1}$중 하나의 값을 가진다. $\\text{log}$안의 term은 가능한 모든 $h$의 조합을 더한 것을 의미하게 된다. 즉, hidden unit 수가 $M$개라면, $2^M$개의 경우를 모두 더하는 것이다. 근데, 만약, hidden unit 개수가 2개라고 해 보자(즉 $j=[0,1]$). 그럼 $\\text{log}$안은 다음처럼 된다. $$ e^{0u_0}e^{0u_1} + e^{0u_0}e^{1u_1} + e^{1u_0}e^{0u_1} + e^{1u_0}e^{1u_1} $$ $$ = (e^{0u_0} + e^{1u_0})(e^{0u_1} + e^{1u_1}) $$ $j$번째 hidden unit $h_j$는 0과 1밖에 가지지 못하기에 위 4가지 경우가 전부. 그렇다면, hidden unit 개수가 $M=3$이라고 해 보자. 정리해보면 다음처럼 나올 것이다. $$ (e^{0u_0} + e^{1u_0})(e^{0u_1} + e^{1u_1})(e^{0u_2} + e^{1u_2}) $$ 즉, $$ \\sum_h \\prod_j \\text{exp} { h_j u_j } = \\prod_{j} \\sum_{h_j={0,1}} \\text{exp} {h_j u_j} $$ 이며, 이것의 time complexity는 $M$이다. (인수분해를 활용한 계산 최적화!) 즉, free energy는 다음처럼 정리가 가능하다. $$ F(v) = -\\sum_i b_i v_i - \\text{log} \\prod_j \\sum_{h_j={0,1}} \\text{exp}{ h_j(\\sum_i W_{i,j} v_i + c_j) } $$ $$ = -\\sum_i b_i v_i - \\sum_j \\text{log} (1+ \\text{exp} {\\sum_i W_{i,j} v_i + c_j}) $$ 이는, tractable하다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"Restrict Boltzmann Machines 1","date":"2020-03-03T13:28:52.000Z","path":"studynotes/machine-learning/Restrict-Boltzmann-Machines-1st/","text":"Restrict Boltzmann Machines Restrict Boltzmann machines은 Boltzmann machine에다가 일종의 제약조건을 추가한 형태의 neural network를 말한다. 보통 Boltzmann machine이라고 하면 각 뉴런이 bernoulli random variable인 경우가 많다. From Boltzmann Machine to RBM Statistical Mechanics 입자가 매우 많은 환경에서 입자의 운동을 deterministic하게 정의할 수 없을떄, 통계적으로 물리적 현상을 해석하는 학문을 의미한다. Boltzmann machine은 statistical mechanics의 영향을 많이 받은 neural network이다. Statistical mechanics에서는 **평형 상태(Equilibrium)**를 중요하게 여기는데, 자연계의 모든 것은 평형 상태로 가기를 원한다. 평형 상태에서는 에너지가 한곳에 치우치지 않고 고루 분포되어 있고, 시스템 전체의 에너지를 낮출 수 있으면 낮은 에너지를 갖게 된다. Statistical mechanics에서는 어떤 상태 $s_i$가 될 확률은, 그 상태가 가지는 에너지 $\\epsilon_i$와 그 상태에서의 온도 $T_i$에 의해 결정된다고 한다. $k$는 Boltzmann constant이다. $$ p(s_i) = \\frac{exp{\\epsilon_i/(kT_i)}}{\\sum_j exp{\\epsilon_j/(kT_j)}} $$ 위 수식을 Boltzmann Distribution 또는 Maxwell-Boltzmann Distribution이라고 부른다. 통계 역학에서 물리적 특성에 따른 각 상태의 확률 분포를 정의한다. 분자에는 하나의 상태에 대한 식이 들어가며, 분모에는 모든 상태의 식을 합한 normalization constant가 있다. 즉, 상태 $s_i$의 에너지가 높으면 $p(s_i)$는 작아진다. 즉, 자연계가 에너지가 높은 상태로 전이될 확룰을 작다. 반대로, 에너지가 낮은 상태로 전이될 확률은 높다. 이것은 일상 생활에서도 찾아볼 수 있는데, 대기중의 공기들이 갑자기 좁은 공간으로 모이는 상태가 될 확률은 매우 작다. 인간이 강제적으로 박스 내에 많은 공기를 가두는 것이 아닌 이상 말이다. 그리고, 만약, 박스 내에 많은 공기를 가두었다고 가정하자. 이 박스 내의 에너지는 높으며 불안정하다. 그리고 박스를 없애버리면 공기는 자연스럽게 퍼져나가고, 공기 밀도는 작아진다. 왜냐하면, 펴저나간 상태가 안정적이고 확률이 높기 때문이다. 그리고 그 상태는 작은 에너지를 가지는 상태이다. Neural Networks and Neuroscience Neural nework는 뇌를 모방한 확률 모델이다. 우리 뇌는 외부 신호를 받는 뉴런과 뇌 내부 속의 뉴런까지 모두 연결되어 있다. 따라서, Neuroscientist들은 가장 완벽한 형태의 neural network로 Boltzmann machine을 정의했다. 위 그림은 Boltzmann machine을 보여주고 있으며, 모든 뉴런이 서로 연결되어 있는 형태를 가진다. 그리고, Boltzmann machine의 뉴련은 외부와 맞닿아 있는 visible unit들과 내부에 숨겨저 있는 hidden unit들로 나뉜다. Visible unit들이 외부로부터 신호를 받는 input layer의 역할을 하며, hidden unit이 latent variable을 발견하고 pattern recognition 역할을 하는 hidden layer로 작동한다. Boltzmann Machines Boltzmann machine의 특징은 모든 유닛이 모두와 연결된 네트워크이며, 각 edge는 양방향이다. 즉, visible unit이 hidden unit으로 정보를 propagation하기도 하지만, hidden unit이 visible unit에게 propagation하기도 한다. 이건 visible unit들 사이 관계, hidden unit들 사이 관계에서도 마찬가지이다. Systems in BM Boltzmann machine은 하나의 시스템이며, 한 순간의 상태를 저장한다. 각 뉴런이 bernoulli variable이라고 하면, 각 variable의 값에 따라 전체 시스템의 상태가 정의된다. 그리고 visible unit개수가 $D$개, hidden unit 개수가 $M$개라고 하면, 이 Boltzmann machine에 의해 표현 가능한 시스템의 상태 개수는 $2^{D+M}$이 될 것이다. Boltzmann machine의 propagation 목적은 가장 낮은 에너지 값을 가지는 상태를 찾는 것이며, 낮은 에너지 값으로 수렴할 때 까지 edge를 오가며 unit들이 신호를 주고받는다. 주의할 것은, Boltzmann machine에서, 낮은 에너지 상태를 찾는다는 것은 weight가 변한다는 것이 아니다. 뉴런의 값이 변하는 것이다. 현재 가지고 있는 weight를 가지고, 유닛들이 서로 신호를 보내면서 유닛의 값을 고치면서 찾을 수 있는 가장 낮은 에너지 상태로 수렴하게 된다. Propagation in BM BM에서의 propagation은 생략하고 뒤에서 RBM의 propagation을 말하고자 한다. Convergence of BM Boltzmann machine에서는 각 edge에 weight를 두고 각 뉴런은 bias를 가지고 있다. 그리고, Boltzmann machine이 현재 상태에서의 에너지는 다음과 같이 계산한다. $$ E(s_i) = -[\\sum_i \\sum_j W_{i,j} z_i z_j + \\sum_k b_k z_k] $$ 이때, $z_i$는 Boltzmann machine의 $i$번 뉴런이다. Edge는 양 끝에 연결된 뉴런이 모두 1일 때, 활성화 되는 형태이다. 위 energy function을 Boltzmann distribution에 넣으면, 다음과 같을 것이다. (BM에선 온도는 사용하지 않는다. 다르게 말하면, 온도는 고정된 상수로 간주한다.) $$ p(s_i) = \\frac{exp{-E(s_i)}}{\\sum_j exp{-E(s_j)}} $$ Intractability of BM 하지만, 여기서 가장 큰 단점이자, BM이 현재로서는 사용할 수 없는 인공신경망이라는 것을 드러내 주는 것이 있다. 바로, Intractability(계산불가능) 특성이다. Boltzmann distribution을 보면, 분모를 계산하기 위해서는 모든 상태의 exponential을 계산해야 한다. 하지만, visible unit개수와 hidden unit개수가 증가하면, 가능한 상태의 개수는 exponential하게 증거하게 된다. 이 증가량으로 인해 실생활에 응용할 수 있는 neuron 개수만큼 unit을 생성하게 되면, Boltzmann machine이 가지는 상태 수는 엄청나게 증가한다. 따라서, 현실적으로 계산 가능한 모델을 구현하기 위해 BM에 제약(restrict)을 걸어서 사용하게 되는데, 이를 **Restrict Boltzmann machine(RBM)**이라고 한다. Restrict Boltzmann Machine BM에 다음의 제약을 준 신경망이다. Visible unit끼리는 직접 연결되지 않는다. Hidden unit끼리는 직접 연결되지 않는다. 그 외에, 기본적인 이론은 BM과 모두 같다. 다만, visible unit과 hidden unit을 구분해서 쓴 energy function은 다음과 같다. $$ E(v, h) = -[\\sum_i \\sum_j w_{i,j} v_i h_j + \\sum_i b_i v_i + \\sum_j c_i h_i] $$ $c$는 hidden unit에 있는 bias이다. Propagation in RBM RBM에서 propagation은 다음과 같이 두 가지가 있을 수 있겠다. $p(h|v)$ $p(v|h)$ 즉, visible unit이 주어진 후, hidden unit으로의 propagation과 hidden unit이 주어진 후 visible unit으로의 propagation이다. 일단 결론부터 말하면, propagation은 다음의 식으로 이루어진다. $$ p(h=1|v) = \\sigma (W^T v + c) $$ $$ p(v=1|h) = \\sigma (W h + b) $$ 놀랍게도 하나의 unit이 1이 될 확률은 그냥 일반적인 neural network처럼 sigmoid 결과 형태이다. 그리고, 실제로 energy function을 바탕으로 유도한 결과 위 식처럼 나온다. 이제, 위 식을 유도해 보고자 한다. 먼저, 우리가 필요한 건 propagation을 위한, $p(h|v), p(v|h)$이다. 이 두 가지 식을 유도하는 과정은 정확히 일치하므로, $p(h|v)$만 유도하고자 한다. 먼저, $p(h|v)$는 Bayes rule을 통해 다음처럼 적을 수 있겠다. $$ p(h|v) = \\frac{p(v,h)}{p(v)} $$ Boltzmann distribution의 분모를 $Z$라고 하자. 분모는 normalization constant이다. 이때, Boltzmann machine의 상태는 visible unit과 hidden unit의 조합으로 표현이 가능하므로, $p(s) \\leftrightarrow p(v,h)$로 대체하기로 한다. $$ p(v,h) = \\frac{1}{Z} exp{\\sum_i \\sum_j W_{i,j} v_i h_j + \\sum_i b_i v_i + \\sum_j c_j h_j } $$ 그리고 $p(v) = \\sum_h p(v,h)$이므로, (by Marginalization) $$ p(v) = \\sum_j \\frac{1}{Z} exp { \\sum_i \\sum_j W_{i,j} v_i h_j + \\sum_i b_i v_i + \\sum_j c_j h_j } $$ 그리고, 위 두개의 식을 $p(h|v)$에 넣어보면 $Z$가 약분되어 사라진다. $$ p(h|v) = \\frac{exp{ \\sum_i \\sum_j W_{i,j} v_i, h_j + \\sum_i b_i v_i + \\sum_j c_j h_j }}{\\sum_j exp{ \\sum_i \\sum_j W_{i,j} v_i, h_j + \\sum_i b_i v_i + \\sum_j c_j h_j }} $$ 그리고, 이 식의 분모는 또 다른 normalization constant이다. 이를 $Z’$라고 하자. $$ p(h|v) = \\frac{1}{Z’} exp{ \\sum_i \\sum_j W_{i,j} v_i, h_j + \\sum_i b_i v_i + \\sum_j c_j h_j } $$ 그리고 지수 법칙(?)을 이용해서 다음처럼 변형한다. ($e^{a+b} = e^a e^b$) $$ p(h|v) = \\frac{1}{Z’} exp{ \\sum_i b_i v_i } exp{ \\sum_i \\sum_j W_{i,j} v_i, h_j + \\sum_j c_j h_j } $$ 여기서, $p(h|v)$는 $h$의 함수이며, $v$는 이미 주어져 있다($h \\text{ given } v$이니까). 따라서, $exp{\\sum_i b_i v_i}$또한 constant로써, $Z’$와 합체할 수 있다. 이것을 $Z’’$라고 하자. $$ p(h|v) = \\frac{1}{Z’’} exp{ \\sum_i \\sum_j W_{i,j} v_i, h_j + \\sum_j c_j h_j } $$ 그리고, 다시 지수법칙을 이용해서 $j$와 관련된 항을 밖으로 뺀다. (역시 $e^{a+b} = e^a e^b$) $$ p(h|v) = \\frac{1}{Z’’} \\prod_j exp { \\sum_i W_{i,j} v_i h_j + c_j h_j } $$ 그런데, RBM 아키텍처를 다시 한번 소환해보자. 보다시피, visible unit이 주어진 상태에서는 hidden unit끼리는 independent이다. 즉, conditional independent이다. 따라서, $p(h|v) = \\prod_j p(h_j|v)$가 성립한다. 위 식을 $h_j$에 대한 확률로 변경해보자. $$ p(h_j|v) = \\frac{1}{Z’’} exp { \\sum_i W_{i,j} v_i h_j + c_j h_j } $$ $h$끼리는 다 독립이고, 독립일때 joint distribution은 각 확률의 곱이므로 $\\prod_j$가 존재했으나, $h_j$하나만 가저올려면 $\\prod_j$를 없애면 된다. 그런데, 이때, Boltzmann machine의 각 뉴런은 bernoulli random variable이라고 했으므로, 다음과 같이 $p(h_j|v)$를 나눌 수 있다. $$ p(h_j|v) = \\begin{cases} p(h_j = 1|v) \\ p(h_j = 0|v) \\end{cases} $$ 그리고, 다음과 같을 것이다. $$ \\begin{cases} p(h_j=1|v) = \\frac{1}{Z’’} exp { \\sum_i W_{i,j} v_i + c_j } \\ p(h_j=0|v) = \\frac{1}{Z’’} exp { 0 } = \\frac{1}{Z’’} \\end{cases} $$ 그리고 확률의 합은 1이다. 즉, $$ p(h_j=1|v) + p(h_j=0|v) = 1 $$ 이 식에 대입해서 $Z’’$에 대해 정리해보면, $$ Z’’ = exp { \\sum_i W_{i,j} v_i + c_j } + 1 $$ 그렇다면, $p(h_j=1|v)$는 다음과 같이 정리할 수 있다. $$ p(h_j = 1|v) = \\frac{exp{ \\sum_i W_{i,j} v_i + c_j }}{exp{ \\sum_i W_{i,j} v_i + c_j } + 1} $$ 그리고 이것은 sigmoid 함수 형태이며, 최종적으로 다음처럼 정리가 가능하다. $$ p(h = 1|v) = \\sigma (W^T v + c) $$ 지금까지 propagation에 쓰이는 확률을 유도해봤는데, 아직, weight 업데이트에 해당하는 &quot;학습&quot;은 시작도 안 했다. 최적화를 위한 미분 과정도 intractable하기 때문에 좀 복잡하다. RBM에서 propagation의 문제는 normalization constant의 계산이 불가능하다는 것이었는데, 어떻게어떻게 normalization constant를 trick으로 계산한 후 최종적으로 유도한 것을 보았다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"Distillation Methods","date":"2020-03-03T13:07:01.000Z","path":"studynotes/machine-learning/Distillation-Methods/","text":"Distillation Methods 다음을 참고했다. Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks Distilling knowledge in a Neural Network Distilling Knowledge in a Neural Network Distillation이란, [Distilling knowledge in a Neural Network]라는 논문에서 등장한 것으로 보이며, 이 논문에서는 다음과 같은 과정을 통해 네트워크의 지식을 다른 네트워크에게 전달해 줄 수 있다고 한다. 먼저, 데이터 $(X, Y)$를 충분히 잘 학습할 수 있도록 큰 네트워크 $F$를 충분히 학습한다. 학습은 $F$의 정확도를 최대한 올리도록 진행한다. 작은 네트워크 $F_d$를 만들고, 같은 데이터셋 $(X, Y)$를 이용해서 그 네트워크를 학습하는데, 다음과 같은 과정을 거친다. 데이터 $X$를 큰 네트워크 $F$에 통과시켜서 softmax에 들어가기 바로 전 값, 즉, logit $F(X)$를 얻는다. 큰 네트워크의 파라미터는 모두 고정시킨다. 데이터 $X$를 작은 네트워크 $F_d$에 통과시켜서 logit 값 $F_d(X)$를 얻는다. $\\sigma(F_d(X))$를 ground truth인 $Y$와 가깝게 학습시키는 loss를 정의한다. $\\sigma$는 softmax이다. $$ L_{CE}(F_d(X), Y) $$ 또, 작은 네트워크가 예측한 결과는 큰 네트워크가 예측한 결과를 최대한 따라가도록 학습하도록 한다. 그에 맞는 loss를 정의한다. $$ L_{CE}(\\sigma(\\frac{F_d(X)}{T}), \\sigma(\\frac{F(X)}{T})) $$ 이때, logit을 하이퍼파라미터 $T$로 나눠줌으로써, 조금 약하게 한다. 이것은, 작은 네트워크가 큰 네트워크의 데이터셋 $(X, Y)$를 학습한 결과를 최대한 따라가도록 만드는 효과가 있으며, 큰 네트워크의 지식을 작은 네트워크에게 전수한다고 볼 수 있다. 이러한 방법으로, 매우 유사한 성능을 내는 compact한 네트워크를 만들 수 있으며, 큰 네트워크 대신 작은 네트워크를 이용하면 computation complexity를 크게 줄일 수 있을 것이다. Generalization using Distillation Distillation은 모델을 generalization하는 방법으로도 응용할 수 있다. 이 방법으로 상당한 adversarial attack 또한 방어가 가능하다(한때는 adversarial attack에 대한 state-of-the-art 기술이었다고 하는 듯 하다). 방법은 다음과 같다. 똑같은 구조를 가지지만 weight를 공유하지 않는 두 네트워크 $F, F_d$를 생성한다. 먼저, 데이터셋 $(X,Y)$를 이용해서 $F$를 충분히 학습한다. 이후, $F$의 파라미터는 고정시킨다. 같은 데이터셋 $(X, Y)$를 이용해서 $F_d$를 다음과 같이 학습한다. 데이터 $X$를 $F$에 통과시킨, softmax 결과 $F(X)$를 구한다. 데이터 $X$를 $F_d$에 통과시킨, softmax 결과 $F_d(X)$를 구한다. $F(X)$과 $F_d(X)$를 가깝게 학습한다. $$ \\text{argmin} ~ KLD(F(X)||F_d(X)) $$ (KL-divergense말고 다른걸 써도 됨) 이 방식은, 첫 번째 네트워크 $F$를 학습할 때, one-hot label $Y$를 이용하지만, 두 번째 네트워크 $F_d$를 학습할 때는, one-hot label이 아니라 $F$의 softmax값을 사용하게 된다. One-hot label $Y$를 이용하게 되면, 해당 정답 라벨에 모델이 over-confident하게 된다. Softmax값을 이용하게 되면, 정답 라벨이 될 확률이 크게 학습되는것은 같다. 그러나, 덜 confident하게 되어 overfitting확률이 줄어든다. 이 방법으로 학습된 네트워크는 adversarial attack을 매우 효과적으로 막아냈으며, generalization이 그 이유라고 분석되고 있는 듯 하다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"08. Temporal Difference Learning","date":"2020-03-03T01:00:07.000Z","path":"studynotes/reinforcement-learning/08-Temporal-Difference-Learning/","text":"Temporal Difference Learning 참고: Coursera Reinforcement Learning (Alberta Univ.) 지금까지 봐 왔던 RL 학습법은, 일단 한 episode를 다 플레이하고 value function을 업데이트했으나, TD(Temporal difference) learning 방식은, episode를 플레이함과 동시에 value function을 업데이트한다. t−1t-1t−1일때의 state st−1s_{t-1}st−1​에서, 액션 at−1a_{t-1}at−1​을 취해서 reward RtR_{t}Rt​를 얻었다면, v(st−1)v(s_{t-1})v(st−1​)은 즉시 업데이트 가능하다. 결국, value function은 Bellman equation에서 보다시피 next state에서의 reward와 value에 의해서 계산될 수 있기 때문. v(st)=E[Gt∣s=st]v(s_{t}) = E[G_t|s=s_t] v(st​)=E[Gt​∣s=st​] v(st)=E[Rt+γGt+1∣s=st]v(s_t) = E[R_t + \\gamma G_{t+1}|s=s_t] v(st​)=E[Rt​+γGt+1​∣s=st​] 또한, Monte Carlo estimation을 통해 기댓값을 추정한다면, v(st)≈1n∑i[Rt+γGt]v(s_t) \\approx \\frac{1}{n}\\sum_{i} [R_t + \\gamma G_t] v(st​)≈n1​i∑​[Rt​+γGt​] 일 것인데, 이러면, 모든 샘플을 저장하고 있어야 한다. 따라서, 샘플 하나 모으고 반영하고 하나 모으고 반영하기 위해, incremental 하게 구현하려면 다음과 같이 value function 식을 수정할 수 있다. v(st)=v(st)+α(Gt−v(st))v(s_t) = v(s_t) + \\alpha(G_t - v(s_t)) v(st​)=v(st​)+α(Gt​−v(st​)) v(st)=v(st)+α(Rt+1+γGt+1−v(st))v(s_t) = v(s_t) + \\alpha(R_{t+1} + \\gamma G_{t+1} - v(s_t)) v(st​)=v(st​)+α(Rt+1​+γGt+1​−v(st​)) 여기서 α\\alphaα는 step size이며, 1n\\frac{1}{n}n1​이 들어간다고 보면 된다. Temporal Difference Error Rt+1+γGt+1−v(st)R_{t+1} + \\gamma G_{t+1} - v(s_t)Rt+1​+γGt+1​−v(st​)부분을 말한다. 원래의 평균치인 v(st)v(s_t)v(st​)로부터 얼만큼 업데이트 될 것인지를 나타내기도 하며, 원래 기댓값와 새로운 기댓값의 오차 정도로 이해하면 될듯. TD(0) Algorithm Monte Carlo prediction을 위해서 한 episode의 history를 저장해놓고 있어야 했지만, TD(0)에서는 바로 앞 전 previous time에서의 정보만 저장해두고, time step마다 업데이트를 incremental하게 시행한다. (과거 1스텝만 본다고 해서 TD(0)이다) TD는 MC방법보다는 low variance라고 한다. 즉, 적어도 일관된 대답은 내놓을 수 있게 학습된다(low noise). TD vs MC Temporal difference방법은 episode가 끝나지 않아도 value function의 업데이트가 이루어지지만, 일반적인 Monte Carlo 방식은 episode가 완전히 끝나야 value function의 업데이트가 이루어진다. 수렴속도가 TD가 더 빠르다는 실험 결과가 있다. y축은 실제 value와 예측된 value의 RMS(Root mean squared) error이고, x축은 episode 횟수이다. Batch TD(0) Episode를 무한히 만들 수 없고, 한정된 episode만 이용할 수 있을 때, (즉, 데이터셋이 한정되어 있음) batch TD(0)를 사용하기도 한다. 먼저 episode가 100개가 있다고 가정하면, 각 episode를 돌면서 다음을 계산한다. increment=α(Rt+1+γV(St+1)−V(St))\\text{increment} = \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) increment=α(Rt+1​+γV(St+1​)−V(St​)) episode 100개를 모두 한번씩 보는 것을 1 batch라고 하면, 1 batch를 모두 볼때까지 value function V(S)V(S)V(S)를 업데이트 하지 않는다. 100개를 모두 보고 난 후, 각 episode마다 계산된 increment\\text{increment}increment를 state마다 모두 합해서 V(S)V(S)V(S)를 업데이트하게 된다. 그리고, 다시, 업데이트된 V(S)V(S)V(S)를 이용해서 episode 100개를 다시 반복해서 본다. 일반적인 TD(0)는 1 episode 를 돌때도 즉시 value function을 업데이트하지만, batch TD(0)는 모든 episode를 본 후, 각각 increment\\text{increment}increment를 계산하고 이들의 합으로 value function을 업데이트한다. 즉, 모든 episode를 본 후, 비로소 한 번의 업데이트가 이루어진다. Temporal Difference Learning for Control Control이라 함은, policy control을 의미한다. TD(0) 알고리즘은 policy evaluation 또는 prediction으로 이용해서 한 episode가 끝나지 않더라도, value function을 업데이트가 가능하게 해 주었다. 하지만, value function을 한번 업데이트했다면, policy또한 업데이트가 가능할 것이다. Policy control을 하려면, state value보단, action value가 편하다. State가 주어졌을 때, 큰 action value를 가지는 action을 선택하면 되기 때문. Action value function을 TD(0)알고리즘에 이용하기 위해, increment하게 바꾼 형태는 다음과 같다. Q(St,At)=Q(St,At)+α(Rt+1+γQ(St+1,At+1)−Q(St,At))Q(S_t, A_t) = Q(S_t, A_t) + \\alpha(R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) Q(St​,At​)=Q(St​,At​)+α(Rt+1​+γQ(St+1​,At+1​)−Q(St​,At​)) (State value의 incremental 식이랑 똑같다) Sarsa Algorithm 한 episode 내에서 한 time step마다 value function을 업데이트하고, policy를 업데이트하는, generalized policy iteration 과정을 거치게 할 수도 있다. TD(0)를 generalized policy control에도 응용한 것으로, 이것을 Sarsa algorithm이라고 부른다. 일반 Monte Carlo 방식에서는, 한 episode가 모두 마무리되어야 value function을 업데이트하고 policy를 학습한다. 그러나, 이 경우, 초기 policy를 어떻게 initialize했느냐에 따라 학습 초창기에 episode가 지나치게 길어질 수 있다. 물론, 몇 episode가 끝나면 policy가 업데이트되어 어느정도 빨라지겠지만 말이다. 이것은 한 episode가 모두 끝나고나서 업데이트하는 방식의 단점이라고 할 수 있으며, Sarsa algorithm은 이를 피하게 해 준다. 가는 도중에도 policy가 업데이트되어 수렴도 빠르다. Value function 업데이트 수식은 다음과 같다.(action value 이용) Q(St,At)=Q(St,At)+α(Rt+1+γQ(St+1,At+1)−Q(St,At))Q(S_t, A_t) = Q(S_t, A_t) + \\alpha(R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) Q(St​,At​)=Q(St​,At​)+α(Rt+1​+γQ(St+1​,At+1​)−Q(St​,At​)) Sarsa 알고리즘은 다음과 같은 특징이 있다. Action value Bellman equation와 Monte Carlo estimation을 이용해서 policy evaluation을 수행한다. Policy evaluation과 improvement를 번갈아 수행한다. 즉, policy iteration의 방식을 따른다. 기본적으로 behavior policy와 target policy가 같은 on-policy learning이다. 즉, At+1∼πA_{t+1} \\sim \\piAt+1​∼π이다. Q-Learning Sarsa 알고리즘은 기본적으로 on-policy를 기반으로 한다. (물론, off-policy로 구성할 수도 있을 것 같다.) Q-learning은 off policy TD control 방법으로, action value function은 다음처럼 업데이트한다. Q(St,At)=Q(St,At)+α(Rt+1+γ⋅maxa Q(St+1,a)−Q(St,At))Q(S_t, A_t) = Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\cdot \\underset{a}{ \\text{max} }~Q(S_{t+1}, a) - Q(S_t, A_t)) Q(St​,At​)=Q(St​,At​)+α(Rt+1​+γ⋅amax​ Q(St+1​,a)−Q(St​,At​)) Q-learning은 Sarsa와는 달리, 다음의 특징이 있다. Action value Bellman optimality equation과 Monte Carlo estimation을 통해 policy evaluation을 수행한다. Policy evaluation만 계속 하다가 마지막에 policy improvement를 한다. 즉, value iteration의 방식이다. Behavior policy대로 action을 취하고 episode가 만들어지지만, value function의 업데이트는 greedy policy, 즉, target policy를 따르도록 업데이트된다. 위 식에서 a∼π∗a \\sim \\pi^*a∼π∗가 되며, 이는 Q-learning이 off-policy learning이라는 것을 알려준다. Q-Learning with/without Importance Ratio Q-learning은 importance ratio를 곱할 필요가 없다. 왜냐하면, behavior policy에서 뽑은 액션대로 업데이트를 바로 하지 않기 때문이다. Sarsa 알고리즘에서 target policy를 따로 둔다면, importance ratio를 곱해주어야 겠지만, Q-learning은 value function 업데이트는 behavior policy의 action을 사용하지 않는다. 무조건, maximum value를 가지는 액션에 따라 업데이트하게 되며, 이는, target policy에 따라 업데이트하는 것이다. (어차피 target policy는 maximum action만 확률이 1.0이고 나머진 0이다) Sarsa vs Q-Learning Sarsa는 policy iteration 방법이고, Q-learning은 value iteration 방법이다. Sarsa는 아주 optimal인 path를 찾는게 느리며, 못찾을 수도 있다. 그러나, 매우 reliable한 path를 찾아서 간다. Q-learning은 value iteration을 통해 optimal value를 찾고 바로 optimal policy를 계산해낸다. 이것은 Q-learning이 reliablity와 상관없이 가장 빠른 길을 찾게 한다. Reliable path란, path 중간에 매우 부정적인 reward를 주는 위험요소가 없는 path이다. Sarsa와 Q-learning이 모두 epsilon-greedy behavior policy를 사용한다는 가정 하에, 다음 그림을 생각해보면, Cliff(절벽)에 빠지면 -100 reward, 그냥 1스탭은 -1 reward를 얻는다. SSS에서 GGG로 가야 한다. Sarsa는 파란색 길을 찾을 가능성이 더 높다. 반면, Q-learning은 빨간색 길을 찾을 확률이 더 높다. 그러나, 이때의 단점은, epsilon-greedy의 특성상, exploration이 발동될 수 있고, cliff에 빠질 확률이 있다. Sarsa는 episode의 모든 액션을 샘플로 포함시키면서 value function을 계산하고 policy를 업데이트하기에, epsilon의 확률로 인한 다음 액션도 고려하게 된다. 반면, Q-learning은 episode에서 취한 액션이 최적 액션이 아닐 경우, 그 액션 결과는 보지 않는다. 다음 액션이 cliff로 빠지는 액션이라면, 그 액션은 당연히 최적 액션이 아니고, cliff에 빠졌다는 결과는 보지 않게 된다. 그 결과, 아주 optimal value와 optimal policy는 빠르게 찾지만, 위험할 수 있다. Sarsa는 학습하는 policy와 액션을 취하는 policy가 같으니까 exploration을 고려하면서 업데이트하게 되고, Q-learning은 target policy를 업데이트하지만, exploration하는 behavior policy를 그다지 고려하지 않는다. 그저 최적 루트만 찾게 된다. Expected Sarsa Algorithm Sarsa의 value function 업데이트 식은 다음과 같다. Q(St,At)=Q(St,At)+α(Rt+1+γ⋅Q(St+1,At+1)−Q(St,At))Q(S_t, A_t) = Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\cdot Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) Q(St​,At​)=Q(St​,At​)+α(Rt+1​+γ⋅Q(St+1​,At+1​)−Q(St​,At​)) 즉, 샘플링한 action을 기반으로 value function을 업데이트하게 된다. Expected Sarsa는 다음과 같이 식을 수정한다. Q(St,At)=Q(St,At)+α(Rt+1+∑a′π(a′∣St+1)Q(St+1,At+1)−Q(St,At))Q(S_t, A_t) = Q(S_t, A_t) + \\alpha (R_{t+1} + \\sum_{a&#x27;} \\pi(a&#x27;|S_{t+1}) Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) Q(St​,At​)=Q(St​,At​)+α(Rt+1​+a′∑​π(a′∣St+1​)Q(St+1​,At+1​)−Q(St​,At​)) 즉, action 샘플 방향으로만 업데이트하지 말고, 가능한 모든 액션 방향을 고려하자는 것이다. Q(St+1,At+1)→∑a′π(a′∣St+1)Q(St+1,At+1)Q(S_{t+1}, A_{t+1}) \\rightarrow \\sum_{a&#x27;} \\pi(a&#x27;|S_{t+1})Q(S_{t+1}, A_{t+1}) Q(St+1​,At+1​)→a′∑​π(a′∣St+1​)Q(St+1​,At+1​) Action value를 업데이트할때, 다음 state의 action value를 고려해야 하는데, Sarsa에서는 다음 state에서 episode에서 취한 action만을 이용한다. 즉, 다른 액션을 통한 action value는 이용하지 않는다. Expected Sarsa에서는 episode에서 취한 action도 고려하면서 아예 policy에 따른 기댓값으로 action value를 계산한다. Expected Sarsa도 Monte Carlo 방식이다. 다만, action value update식만 조금 다를 뿐. 다만, 한 state에서의 액션 개수가 많으면 계산 속도가 급감한다. Sarsa 보다 low variance라고 한다. Expected Sarsa vs Sarsa Expected Sarsa는 value function을 좀 더 일반적으로 고려하므로(episode 방향만 고려하는게 아니니까) 좀 더 안정적인 업데이트가 가능하다고 한다. Sarsa는 어찌됬든 많은 episode를 수행하다보면 value function의 추정이 정확해진다. 하지만, 각 episode에서 잘못된 액션이 있다할지라도, 그 방향으로 업데이트를 수행한다. 하지만, expected Sarsa는 잘못된 액션이든, 올바른 액션이든, 무조건 기댓값을 취하므로, 항상 안정적인 업데이트가 가능하다. Expected Sarsa에서는 큰 step size α\\alphaα를 사용하기 쉽다. Sarsa에서는 α\\alphaα가 크면 잘못된 방향으로도 큰 업데이트를 수행하겠지만, expected Sarsa에서는 그 정도가 작다. 기댓값으로 업데이트하기 때문. 이것은 심지어 optimal value function으로의 수렴 속도까지 expected Sarsa가 뛰어나게 만들기도 한다. (운이 좋다면, Sarsa가 빠를수도 있다) 또한, value function이 거의 다 수렴한 상태에서도, 큰 α\\alphaα를 가진다면, Sarsa는 샘플링하는 대로 업데이트를 똑같이 큰 step으로 지속하게 된다. 어쩌다보면 발산 방향으로 갈 수도 있다. 반면, expected Sarsa에서는 샘플링이 계속되도, 기댓값은 크게 변하지 않으므로, value function또한 안정적으로 유지된다. Expected Sarsa vs Q-Learning Expected Sarsa는 Sarsa와 다르게 on-policy와 off-policy learning 둘 다에 해당한다. Expected Sarsa의 식은 다음과 같다. Q(St,At)=Q(St,At)+α(Rt+1+γ⋅∑a′π(a′∣St+1)Q(St+1,a′)−Q(St,At))Q(S_t, A_t) = Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma \\cdot \\sum_{a&#x27;} \\pi(a&#x27;|S_{t+1})Q(S_{t+1}, a&#x27;) - Q(S_t, A_t)) Q(St​,At​)=Q(St​,At​)+α(Rt+1​+γ⋅a′∑​π(a′∣St+1​)Q(St+1​,a′)−Q(St​,At​)) 이때, 미래의 action value를 계산할 때, target policy에 대한 기댓값을 계산하게 되는데, 이는, 액션 a′a&#x27;a′이 behavior policy에서 나온 것이라도 동일하다. 즉, 자연스럽게 위 value function은 target policy를 따르는 기댓값을 이용하게 되며, importance sampling 없이 off-policy learning을 달성한다. Expected Sarsa는 Q-learning의 일반화 버전이다. 만약, target policy가 greedy한 policy라면, 위 식은 Q-learning과 똑같아진다. 즉, expected Sarsa 방식은 greedy한 target policy이던, greedy하지 않은 target policy이던 적용할 수 있는 off-policy learning 알고리즘이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"07. Off-policy Learning","date":"2020-03-03T01:00:06.000Z","path":"studynotes/reinforcement-learning/07-Off-Policy-Learning/","text":"Off Policy Learning 참고: Coursera Reinforcement Learning (Alberta Univ.) 학습 데이터를 수집할때, agent가 action을 취할 policy와 학습과정에서 업데이트할 policy를 따로 구분해 놓은 경우를 off-policy learning이라고 부른다. 반대로, 그냥 하나의 policy를 두고, agent는 그 policy에 따른 action을 취하고 업데이트도 그 policy를 업데이트하는 방식을 on-policy learning이라고 부른다. On-policy Learning Policy 오직 하나의 policy만 존재하며, agent가 학습 데이터를 모으기 위한 action을 취할 때, 이 policy에 따라 액션을 선택한다. 또한, policy control에서도 이 policy를 직접 수정하게 되고, 다음 학습 데이터를 모을 때 영향을받게 된다. Off-policy Learning Target Policy 학습 데이터를 모을 때는 이 policy에 따라 action을 취하지 않는다. 다만, 업데이트 할때는 이 target policy를 업데이트한다. 테스트에서는 target policy에 따라 agent가 action을 취하게 된다. Behavior Policy Agent는 학습 데이터를 모을 때는 이 policy에 따라 action을 취한다. 업데이트되지 않아서, 학습이 진행되도 최적 action만 따라가는 현상을 없앨 수 있다. 왜 policy를 분리했냐면, 결국 exploration-exploitation dilema때문이다. Policy를 하나만 두면, 학습이 진행될수록 다소 안좋은(?) state는 방문횟수가 급격히 줄고, agent는 평소 하던 action만 취한다. 하지만, 학습 데이터를 수집할 때는, 적절히 exploration을 해야 할 필요가 있다. Behavior policy는 random uniform policy와 같은, 다소 불확실하지만, 모든 state가 고르게 방문될 수 있도록 설정하게 된다. Off-policy의 장점은, target policy를 stochastic하게 둘 필요가 없이 deterministic하게 둬도 된다는 것이다. Target policy가 ϵ\\epsilonϵ-soft일필요가 없다. Importance Sampling 그런데, Off-policy learning에 문제가 있다. 만약, Monte Carlo estimation을 시행한다고 하면, 많은 샘플을 모으고 그 샘플들의 단순 평균을 계산하여 기댓값을 계산한다. 샘플 (St,At,Rt)(S_t, A_t, R_t)(St​,At​,Rt​)을 모을 때, agent는 behavior policy에 따라 action을 취하고 샘플을 얻게 된다. 따라서, 그 샘플들의 평균은 behavior policy bbb에 대한 기댓값 Eb[G]E_b[G]Eb​[G]가 되지, Eπ[G]E_{\\pi}[G]Eπ​[G]가 되지 않는다. 따라서, policy evaluation을 할 때, bbb에서 샘플링한 샘플로 π\\piπ에 대한 기댓값을 추정하도록 해야 한다. Importance sampling이란, Monte Carlo estimation을 할 때, 다른 분포에서 샘플링한 샘플을 이용해서 분포의 기댓값을 추정하는 것을 말한다. 분포 bbb를 따르는 random variable XXX와 그 샘플 xxx에 대해, 분포 bbb에 대한 XXX의 기댓값은 다음처럼 계산할 수 있다. xi∼bx_i \\sim b xi​∼b Eb[X]≈∑i=1nxi⋅b(xi)E_b[X] \\approx \\sum_{i=1}^n x_i \\cdot b(x_i) Eb​[X]≈i=1∑n​xi​⋅b(xi​) 그리고, 여기서 약간의 수정을 가해서 다른 분포 π\\piπ에 대한 XXX의 기댓값을 계산하도록 할 수 있다. Eπ[X]=∑XX⋅π(X)E_{\\pi}[X] = \\sum_X X \\cdot \\pi(X) Eπ​[X]=X∑​X⋅π(X) =∑XX⋅π(X)⋅b(X)b(X)= \\sum_X X \\cdot \\pi(X) \\cdot \\frac{b(X)}{b(X)} =X∑​X⋅π(X)⋅b(X)b(X)​ =∑XX⋅b(X)⋅π(X)b(X)= \\sum_X X \\cdot b(X) \\cdot \\frac{\\pi(X)}{b(X)} =X∑​X⋅b(X)⋅b(X)π(X)​ =∑XX⋅ρ(X)⋅b(x), (ρ(X)=π(X)b(X))= \\sum_X X \\cdot \\rho(X) \\cdot b(x), ~ (\\rho(X) = \\frac{\\pi(X)}{b(X)}) =X∑​X⋅ρ(X)⋅b(x), (ρ(X)=b(X)π(X)​) =Eb[Xρ(X)]≈1n∑i=1nxi⋅ρ(xi)= E_b[X\\rho(X)] \\approx \\frac{1}{n}\\sum_{i=1}^n x_i \\cdot \\rho(x_i) =Eb​[Xρ(X)]≈n1​i=1∑n​xi​⋅ρ(xi​) 따라서, Eπ[X]≈1n∑i=1nxi⋅ρ(xi)E_{\\pi}[X] \\approx \\frac{1}{n} \\sum_{i=1}^n x_i \\cdot \\rho(x_i) Eπ​[X]≈n1​i=1∑n​xi​⋅ρ(xi​) 이때, ρ(x)\\rho(x)ρ(x)를 importance sampling ratio라고 하며, 한 샘플에 대해, 두 분포간의 확률 비율을 말한다. Policy iteration에서, 특정 시점 ttt에서의 policy πt\\pi_tπt​가 있고, 그 policy πt\\pi_tπt​를 이용해 value function을 계산해야 하는데, 이 과정에서 Monte Carlo estimation이 사용된다. 여기서, behavior policy를 통해 얻은 샘플들을 이용해서 πt\\pi_tπt​에 대한 기댓값을 계산하게 된다. Implementation of Off-Policy Learning Behavior policy로 episode를 만들고, 각 타임에서의 state와 action을 얻고, 다음 식을 통해 value-function을 Monte Carlo estimation하게 되면, policy bbb에 대한 기댓값 추정이 된다. Gt←γ⋅Gt+1+RtG_{t} \\leftarrow \\gamma \\cdot G_{t+1} + R_t Gt​←γ⋅Gt+1​+Rt​ Gt appends to Returns(St)G_t \\text{ appends to } Returns(S_t) Gt​ appends to Returns(St​) V(St)←mean(Returns(St))V(S_t) \\leftarrow \\text{mean}(Returns(S_t)) V(St​)←mean(Returns(St​)) 여기서, RtR_tRt​는 environment dynamic distribution으로부터 나왔으니, importance sampling에서 예외로 하고, Gt+1G_{t+1}Gt+1​은 behavior policy bbb에 대한 value 기댓값일 것이다. 따라서, Gt+1G_{t+1}Gt+1​에 importance ratio를 곱해주어야 한다. Gt←γ⋅W⋅Gt+1+RtG_t \\leftarrow \\gamma \\cdot W \\cdot G_{t+1} + R_{t} Gt​←γ⋅W⋅Gt+1​+Rt​ Gt appends to Returns(St)G_t \\text{ appends to } Returns(S_t) Gt​ appends to Returns(St​) V(St)←means(Returns(St))V(S_t) \\leftarrow \\text{means}(Returns(S_t)) V(St​)←means(Returns(St​)) 그럼 이제 importance ratio를 계산해야 하는데, Gt+1G_{t+1}Gt+1​은 바로 다음 시점 t+1t+1t+1로부터, policy bbb에 대한 기댓값이므로, importance ratio는 t+1t+1t+1시점부터 episode의 끝 TTT까지 분포 bbb로부터 π\\piπ로 바꿔주는 역할을 해 주어야 한다. 즉, ρt\\rho_tρt​ 대신, ρt+1:T\\rho_{t+1:T}ρt+1:T​를 계산해야 한다는 의미이다. ρt+1:T=∏i=t+1Tπ(Ai∣Si)p(Si+1,Ri+1∣Si,Ai)b(Ai∣Si)p(Si+1,Ri+1∣Si,Ai)\\rho_{t+1:T} = \\prod_{i=t+1}^T \\frac{\\pi(A_i|S_i)p(S_{i+1},R_{i+1}|S_i,A_i)}{b(A_i|S_i)p(S_{i+1}, R_{i+1}|S_i, A_i)} ρt+1:T​=i=t+1∏T​b(Ai​∣Si​)p(Si+1​,Ri+1​∣Si​,Ai​)π(Ai​∣Si​)p(Si+1​,Ri+1​∣Si​,Ai​)​ =∏i=t+1Tπ(Ai∣Si)b(Ai∣Si)= \\prod_{i=t+1}^T \\frac{\\pi(A_i|S_i)}{b(A_i|S_i)} =i=t+1∏T​b(Ai​∣Si​)π(Ai​∣Si​)​ =∏i=t+1Tρi= \\prod_{i=t+1}^T \\rho_i =i=t+1∏T​ρi​ 그리고 이것은 보다시피 incremental implementation이 가능하다. ρT:T=ρT\\rho_{T:T} = \\rho_T ρT:T​=ρT​ ρT−1:T=ρT−1ρT=ρT−1⋅ρT:T\\rho_{T-1:T} = \\rho_{T-1}\\rho_T = \\rho_{T-1} \\cdot \\rho_{T:T} ρT−1:T​=ρT−1​ρT​=ρT−1​⋅ρT:T​ ρT−2:T=ρT−2ρT−1ρT=ρT−2⋅ρT−1:T\\rho_{T-2:T} = \\rho_{T-2}\\rho_{T-1}\\rho_T = \\rho_{T-2} \\cdot \\rho_{T-1:T} ρT−2:T​=ρT−2​ρT−1​ρT​=ρT−2​⋅ρT−1:T​ Monte Carlo prediction은 TTT로부터 backward 방식으로 이루어지므로, 위와 같이 incremental하게 구현이 가능하다. 이는 다음 pseudo code에서 WWW를 업데이트 하는 방식에서 드러난다. 다음은 pseudo-code. On-policy Monte Carlo로부터, episode를 하나 플레이할때, policy π\\piπ를 따르지 않고, policy bbb를 따른다는게 특징. 또한, importance sampling을 위해 G←γG+Rt+1G \\leftarrow \\gamma G + R_{t+1}G←γG+Rt+1​에서 G←γWG+Rt+1G \\leftarrow \\gamma W G + R_{t+1}G←γWG+Rt+1​로 변경되었다. WWW는 importance ratio를 계산한 것이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"06. Sample-based Reinforcement Learning","date":"2020-03-03T01:00:05.000Z","path":"studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/","text":"Sample-based Reinforcement Learning 참고: Coursera Reinforcement Learning (Alberta Univ.) Motivations 지금까지 K-arm bandit problem으로 reinforcement learning의 기초를 보았고, 그를 이용해서 exploration-exploitation dilemma를 보았다. 또한, Exploration, exploitation이 충분히 이루어져서 environment dynamic에 해당하는, transition probability p(s′,r∣s,a)p(s&#x27;,r|s,a)p(s′,r∣s,a)가 이미 모델링되었다고 가정했을 때, MDP 환경에 한정해서 value function과 policy를 어떻게 정의하고 계산하는지 보았다. Bellman equation을 이용해서 dynamic programming 방식으로 policy iteration, value iteration 등, optimal value function과 optimal policy를 계산하는 방법을 보았다. 하지만, 여기서, 현실은 environment는 너무나도 복합적인 것이라, 모델링하기가 쉽지 않다. 그러나, 지금까지 해 왔던 방법들은 environment가 필수적으로 모델링되어 있어야 한다. 이렇게, environment를 모델링하는 데 있어서 어려움이 있다는 문제를 해결하는 방법 중 하나가 Sample-based Reinforcement Learning이다. Introduction 이름에서 알 수 있다시피, 이 방법의 주요 특징은, environment dynamic에 해당하는 transition probability function p(s′,r∣s,a)p(s&#x27;,r|s,a)p(s′,r∣s,a)를 모델링하는 대신, sample-based로 추정해보겠다는 의미이다. 특히 Monte Carlo Estimation이 이용된다. Monte Carlo Estimation Monte-Carlo Estimation이란, 파라미터의 기댓값을 계산하고자 할 때, 그 파라미터를 모델링하는 과정을 거치지 않고, 파라미터에 대한 샘플을 많이 얻어서 그 샘플들의 평균값을 기댓값으로 추정하는 방법을 의미한다. 샘플 개수가 많을수록, Central Limit Theorem에 의해, 샘플들의 평균은 실제 평균과 매우 가깝다고 확신할 수 있다. 여기서 직감할 수 있다시피, 정확한 기댓값을 추정하기 위해서는 상당히 많은 샘플이 필요하다. Implementation Overview Episodic task로 예를 들려고 한다. 일단 하나의 episode를 완주한다. 즉, 게임이 끝날 때 까지 일단 플레이를 한다. 지나온(또는 처해있었던) state, 취했던 action들, 받았던 reward들의 History는 기록해 둔다. 위 history를 S0,A0,S1,R1,A1,S2,...,ST−1,RT−1,At−1,ST,RTS_0, A_0, S_1, R_1, A_1, S_2, ..., S_{T-1}, R_{T-1}, A_{t-1}, S_T, R_TS0​,A0​,S1​,R1​,A1​,S2​,...,ST−1​,RT−1​,At−1​,ST​,RT​라는 sequence로 표현했을 떄, TTT에서 backward방향으로 reward 기댓값, 즉, value를 계산한다. (Final state는 정의에 따라 value가 0이다) GT=0G_T = 0 GT​=0 GT−1=RT+γGTG_{T-1} = R_{T} + \\gamma G_T GT−1​=RT​+γGT​ GT−2=RT−1+γGT−1G_{T-2} = R_{T-1} + \\gamma G_{T-1} GT−2​=RT−1​+γGT−1​ ⋯\\cdots ⋯ GtG_tGt​를 ttt일때의 state였던 놈, StS_tSt​에 대한 value의샘플이라고 간주한다. 하나의 episode에 그 state를 지난 횟수만큼 샘플이 생긴다 여러 episode를 플레이해본다. 모아진 value sample을 이용해서 state-value function을 추정한다(평균내기). 하지만, 하나의 episode가 모든 state를 골고루 방문하지는 않으므로, 여러 episode를 시행해도, state마다 샘플 수는 다르다. 따라서 어떤 state는 state-value의 정확한 추정이 어려울 수 있다. Exploring Starts Action value function도 state value function을 추정하는 방법과 똑같이 추정할 수 있다. Action value function을 굳이 쓰는 이유는 한 state에서 어떤 액션을 선택할지에 대한, 즉 policy를 찾는데 도움을 줄 수 있기 때문이다. 하지만, 만약, deterministic policy를 따르고 있으면, 각 state에서 특정 action만 수행한다. 따라서, 하나의 액션만 exploitation하게 되는데, 이러면, policy를 비교, 조정할 수 없다. 이에 대한 해결책 중 하나로, episode의 시작은 무조건 random state에서 random action을 취하도록 시작하는 것이다. 첫 번째 액션을 취한 이후로는 policy를 따르게 된다. 이 문제는 결국, exploration에 관한 문제이다. Monte Carlo Prediction Reinforcement learning의 context에서, Monte Carlo prediction이란, 주어진 episode를 플레이한 history S0,A0,R1,S1,A1,...,RT,STS_0, A_0, R_1, S_1, A_1, ..., R_T, S_TS0​,A0​,R1​,S1​,A1​,...,RT​,ST​를 이용해서 value function을 추정하는 것을 의미한다. Monte Carlo for Policy Control Monte Carlo를 통해 generalized policy iteration을 구현할 수 있다. Policy Evaluation: 1번의 Episode 수행 및 Monte Carlo Estimation Policy Improvement: 계산된 value function을 통한 policy의 greedify 이전과 같이 policy evaluation과 improvement를 반복하게 되며, evaluation 단계에서 Monte Carlo prediction을 적용하여 value function을 추정하게 된다. Improvement 단계에서는 추정된 value function을 바탕으로 greedy한 policy를 생성해 낸다. 한 번의 반복(evaluation-improvement) 동안, 단 1번의 episode를 플레이하기 때문에 evaluation 단계에서 value function을 완전히 추정하지 않는다. Evaluation을 완성하려면 수많은 episode를 플레이하고 value function을 제대로 추정하고 improvement로 넘어가야 할 것이다. 하지만, 엄청 오래 걸릴 것이다. Pseudo-code는 다음과 같다. 1234567891011121314151617181920def monte_carlo_gpi(states, actions, gamma): pi = initialize_policies(states) action_values = initialize_action_values(states, actions) returns = initialize_rewards(states, actions) while True: s0, a0 = exploring_starts(states, actions) estimated_action_values, history, T = play_one_episode(pi) G = 0 for t in range(T-1, 0, by=-1): G = history[t+1][\"reward\"] + gamma * G state = history[t+1][\"state\"] action = history[t+1][\"action\"] returns[state][action].append(G) action_values[state][action] = mean(returns[state][action]) pi[state] = argmax(action_values[state][action]) Epsilon-soft Policy Exploring starts 방식은 deterministic policy 환경에서 출발점에서나마 랜덤으로 state와 action을 선택하게 함으로써, 모든 state들이 그래도 한번씩은 다 방문되도록 하게끔 하는 것이다. 하지만, 다음 문제점이 있다. State 개수가 너무 많을 경우, 첫 시작을 임의로 시작한다고 한들, 모든 state를 다 방문하기엔 역부족이다. 계산 불가능할 정도로 많은 시도횟수를 요구할 것이다. 하지만, exploring 방법은 여전히 필요하며, exploitation만 할 수는 없다. exploring starts의 대안으로 나온 것이 바로 ϵ\\epsilonϵ-soft 방식이다. 이것은 ϵ\\epsilonϵ-greedy을 포함하는 상위 개념으로, optimal action에는 좀 높은 확률을 두고, 나머지 액션은 확률 0이 아니라 작은 확률을 설정해 두는 것이다. 구현 방법은 value evaluation에서 계산된 value function으로부터 가장 좋은 action을 뽑아내고, 그 액션에 좀 높은 확률을 주고, 나머지 액션은 작은 확률을 준다. ϵ\\epsilonϵ-soft는 stochastic policy이다. 즉, optimal policy보다는 value 기댓값이 적다. 하지만, 적절히 greedy한 액션도 취해가면서 확률적으로 많은 state를 방문할 수 있게 해 준다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"05. Policy Evaluation & Control","date":"2020-03-03T01:00:04.000Z","path":"studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/","text":"Policy Evaluation &amp; Control 참고: Coursera Reinforcement Learning (Alberta Univ.) 현재 가지고있는 policy가 좋은지 평가하고(evaluation) 더 좋은 policy로 향상시키는 작업(control)을 의미한다. 현재 가지고 있는 policy π\\piπ와 dynamic environment를 표현하는 p(s′,r∣s,a)p(s&#x27;,r|s,a)p(s′,r∣s,a)분포가 있으면, dynamic programming을 통해 value function을 계산해낼 수 있고, 그 value function을 이용해서 policy를 평가(evaluation)할 수 있다. 또한, dynamic programming을 통해 더 나은 policy를 찾을 수 있다(control). Policy Evaluation 어떤 policy가 좋은지 평가하는 방법은 각 policy에 대해 value function을 계산하고, value function의 값을 비교하는 것이다. 이때, 주어진 policy에 대해 value function을 계산하는 과정을 policy evaluation이라고 한다. Iterative Policy Evaluation 주어진 policy를 이용하여 value function을 계산하는 한 가지 방법으로, dynamic programming을 통한 iterative 방법이다. 처음에 모든 state의 value를 0으로(또는 임의의 아무 숫자) 초기화시킨 후, state Bellman equation을 통해 모든 state의 value가 수렴할때까지 반복적으로 value function을 업데이트하게 된다. 방법은 다음과 같다. Define threshold θ\\thetaθ initialize VVV, initialize uniform policy π\\piπ while True: V′←vπ(s∣V,π)V&#x27; \\leftarrow v_{\\pi}(s|V,\\pi)V′←vπ​(s∣V,π) compute ϵ=max∣V−V′∣2\\epsilon = \\text{max}|V - V&#x27;|^2ϵ=max∣V−V′∣2 if ϵ&lt;θ\\epsilon &lt; \\thetaϵ&lt;θ: break V←V′V \\leftarrow V&#x27;V←V′ 이 루프를 계산하고나면, 해당 policy에 대해 최적에 가까운 value function을 계산할 수 있다. 이때, policy는 바뀌지 않는다. Policy Control Policy control이란, 주어진 policy와 그것으로부터 계산해낸 value function을 가지고, 그 value function에서의 새로운 optimal policy를 찾는 과정을 말한다. Policy Improvement Theorem Action value function q(s,a)q(s, a)q(s,a)가 존재하고, 두 policy π1,π2\\pi_1, \\pi_2π1​,π2​가 있고 각 policy에서 상태 sss에서 취한 액션을 a1,a2a_1, a_2a1​,a2​라고 할 때, 다음을 만족하는 policy π′\\pi&#x27;π′는 항상 π1,π2\\pi_1, \\pi_2π1​,π2​보다 항상 같거나 좋은 policy이다. π′(s)←max a(q(s,a1),q(s,a2))\\pi&#x27;(s) \\leftarrow \\underset{a}{\\text{max }}(q(s, a_1), q(s, a_2)) π′(s)←amax ​(q(s,a1​),q(s,a2​)) 위 이론에 따라, 현재 policy보다 좀 더 좋은 policy를 찾는 방법은, 현재 policy를 바탕으로 계산한 value function에서 다시 greedy한 policy를 계산하는 것이다. π′=argmaxa∑s′,rp(s′,r∣s,a)[r+γ⋅vπ(s′)] (for all state s)\\pi&#x27; = \\underset{a}{\\text{argmax} } \\sum_{s&#x27;,r} p(s&#x27;,r|s,a)[r + \\gamma \\cdot v_{\\pi}(s&#x27;)] ~ \\text{(for all state } s\\text{)} π′=aargmax​s′,r∑​p(s′,r∣s,a)[r+γ⋅vπ​(s′)] (for all state s) 한 가지 짚고 넘어가야 할 점은, 어떤 value function을 바탕으로 greedy한 policy를 선택했다고 해서, 그 policy로 다시 value function을 계산해보면, 그 value function에 대해서는 현재 policy가 greedy하지 않을 수 있다. Policy를 찾은 후, 다시 value function을 계산하면 이전의 value function과 같지 않을 수 있다. 그리고, 새로 계산한 value function에 대해 greedy한 policy를 다시 찾으면 그 policy는 이전 policy와 다를 수 있다. 그래서, value function 계산과 policy 구하는 과정을 반복해서 수행하고 더 이상 달라지지 않으면, 수렴했다고 간주하고, 마지막 policy를 우리가 가지고 있는 environment에 대한 최종 optimal policy로 삼는다. 이렇게 value function과 policy 계산을 반복적으로 수행하는 것을 policy iteration이라고 부른다. Policy Iteration - Dynamic Programming Optimal policy를 찾는 알고리즘으로, 다음과 같은 과정으로 이루어진다. 일단 어떤 state에서 어떤 액션을 취하면 어떤 immediate reward를 받는지는 이미 알고 있다고 가정한다. Immediate reward의 분포 p(r∣s)p(r|s)p(r∣s)은 exploration &amp; exploitation 으로 추정해야 하거나 개발자가 이미 정해놓거나? Initialize π0\\pi_0π0​. 즉, 최초의 policy를 만들고 이를 현재의 policy π\\piπ로 삼는다. 최초의 policy는 아무거나로 한다. 모든 액션을 uniform distribution에 따라 선택하는 policy로 해도 된다. Evaluate π\\piπ. 즉, 주어진 policy에 대해 value function을 계산한다. Control π\\piπ. 즉, 계산된 value function을 바탕으로 모든 상태에서 greedy한 action을 선택하는 새로운 policy π′\\pi&#x27;π′를 만든다. π′=π\\pi&#x27; = \\piπ′=π라면, π\\piπ를 반환하고 끝낸다. 아니라면, π\\piπ에 π′\\pi&#x27;π′를 대입하고 2번으로 간다. 이 과정을 통틀어서 policy iteration 방법이라고 부른다. 다음은 전체 pseudo code. 일단, 가장 처음 policy를 제외하고 이후 control에 의해 만들어진 모든 policy는 greedy하고 deterministic한 policy이다. 하지만, 이것은 새로 evaluate로 생성된 value function에서 greedy하지 않게 된다. 즉, evaluate과정을 거처서 만들어낸, 현 policy를 따르는 value function에서 현재 policy가 greedy하지 않게 되고, control하는 과정을 거치면 greedy한 policy를 만들 수 있지만, 이건 또 다시 policy evalutation을 통해 더 좋은 policy가 있다는 것이 밝혀진다. 이 과정을 통해, 더 이상 좋은 policy가 없을 때 까지 수렴하게 된다. Generalized Policy Iteration Policy Iteration의 일반적인 형태. 앞서 나온 policy iteration은 policy evaluation과 control를 번갈아가면서 수행했다. 그리고, evaluation에서는 value function이 수렴할때까지 loop를 돌렸고, 수렴한 다음에야 policy control을 시행했다. Policy control 또한 완전한 greedy한 deterministic policy를 선택했다. 하지만, generalized policy iteration은 다음처럼 작동한다. Policy evaluation은 loop를 돌지 않고 한번만 회전하고 policy control또한 조금 완화된 greedy action을 선택하게끔 한다. Value Iteration Generalized policy iteration의 한 방법으로, policy evaluation과 control를 번갈아서 수행하지 않고 evaluation을 policy와 관계없이 value값에 대해서만 수행해서 수렴시키고 control를 최종적으로 수행한다. 알고리즘은 위와 같은데, value funciton을 계산할 때, &quot;어떤 state에서는 무조건 이 액션을 선택해&quot;라고 말하는 policy를 넣지 않고, 그냥 value를 최대로 하는 액션을 선택하도록 한다. 그리고 그 최대 value로 업데이트한다. 이 과정을 반복하면 value function 혼자서 optimal에 수렴하게 되고 optimal value function을 이용해서 policy를 뽑아낸다. Asynchronous Dynamic Programming 한번 value function을 업데이트할때, 모든 state를 순차적으로 다 돌지 말고, 필요한 state에 대한 value만, 순서관계없이 업데이트하자는 것이라고 한다. 또한, 모든 state를 다 업데이트하는것이 아니라 관계있는 state들만 업데이트한다. Monte Carlo Methods 지금까지 dynamic programming을 통해 value function과 policy를 계산 및 추정했는데, dynamic programming을 통한 방법 외에도 여러가지 방법이 존재한다. Monte carlo method는 하나의 state에 대해 각 액션을 많이 취해보고 Monte carlo estimation을 통해 value 추정값을 계산하자는 방법이다. 즉, 그 state에서 각각 액션을 많이 취해보고 얻은 reward들을 단순 평균내자는 이야기이다. 이 방법은 optimal policy를 매우 정확하게 찾을 것을 보장해준다.(단, action을 해서 reward를 한 trial이 많아야 한다.) Monte carlo estimation의 단점은 모든 state에서 모든 액션을 많이 취해봐야 정확한 value function을 추정할 수 있는데, 그게 현실적으로 불가능하다. Brute-Force Estimation Brute-force 방법은 간단하다. 가능한 모든 deterministic policy 조합을 나열하고 그중에서 optimal policy를 찾는 것을 말한다. 이 방법 역시 optimal policy를 반드시 찾을 것을 보장해준다. 하지만, action수에 따라 가능한 policy 조합이 exponential하게 증가한다. 그래서 사실상 적용이 불가능하다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"04. Policies and Value Functions","date":"2020-03-03T01:00:03.000Z","path":"studynotes/reinforcement-learning/04-Policies-and-Value-Functions/","text":"Policies and Value Functions 참고: Coursera Reinforcement Learning (Alberta Univ.) RL에서의 학습이란, action을 결정하는 policy와 value function을 추정하는 과정이라고 볼 수 있을 만큼, 두 과정의 RL에의 영향은 절대적이다. Policies 우리말로, 정책이라고 하며, 말 그대로, 주어진 현재 상황에서 agent가 어떤 action을 선택할지 결정해주는 정책을 의미한다. Policy는 각 state에서 가능한 action들의 probability distribution이다. Value function과 그 의미가 매우 유사해보인다. 하지만, Value function은 reward의 기댓값을 계산해주는 것일 뿐, action을 결정하는 것은 policy에게 달려 있다. Deterministic Policies π(s)=a\\pi(s)= a π(s)=a 위 식처럼, deterministic policy란 state를 입력으로 받아서 action 1개를 반환하는 함수이다. 즉, state를 action으로 매핑하는 함수라고 볼 수 있다. Stochastic Policies π(a∣s)= Probability distribution matrix \\pi(a|s) = \\text{ {Probability distribution matrix} } π(a∣s)= Probability distribution matrix Stochastic policy는 한 state를 주게 되면, 그 state에서의 action probability distribution을 반환하는 함수이다. 따라서, 한 state를 주면, 여러 action이 나오게 된다. 이때, action probability distribution matrix는 다음을 만족해야 한다. ∑aπ(a∣s)=1\\sum_a \\pi(a|s) = 1 a∑​π(a∣s)=1 π(a∣s)≥0\\pi(a|s) \\geq 0 π(a∣s)≥0 즉, 행렬의 각 row가 적법한 probability distribution이어야 한다. action probability distribution은 각 state마다 다르다. Valid &amp; Invalid Policies MDP에서 policies는 반드시 현재 타임에서의 상태에 존재하는 정보를 이용해서 action을 결정해야 한다. 즉, policies의 인자는 반드시 time ttt에서는 sts_tst​만이 되어야 하며, 따라서 다음과 같다. π(st)→at\\pi(s_t) \\rightarrow a_t π(st​)→at​ 만약, policy가 이전 time의 state나 action을 이용해서 action을 결정한다면, 적법한 policy라고 부르지 않는다. Value Functions 다음 두 가지로 나뉜다. State Value Function vπ(st)v_{\\pi}(s_t)vπ​(st​) 한 상태 sts_tst​에서, 앞으로 어떤 policy π\\piπ를 따른다고 했을 때, 앞으로 얻을 수 있는 reward 기댓값을 의미한다. vπ(st)=∑atπ(at∣st)∑st+1,rt+1p(st+1,rt+1∣st,at)[rt+1+γ⋅vπ(st+1)]v_{\\pi}(s_t) = \\sum_{a_{t} } \\pi(a_t|s_t) \\sum_{s_{t+1},r_{t+1} } p(s_{t+1},r_{t+1}|s_{t},a_t)[r_{t+1} + \\gamma \\cdot v_{\\pi}(s_{t+1})] vπ​(st​)=at​∑​π(at​∣st​)st+1​,rt+1​∑​p(st+1​,rt+1​∣st​,at​)[rt+1​+γ⋅vπ​(st+1​)] Action Value Function qπ(st,at)q_{\\pi}(s_t, a_t)qπ​(st​,at​) 한 상태 sts_tst​에서, 어떤 액션 ata_tat​를 취하고 난 후, 앞으로 어떤 policy π\\piπ를 따른다고 했을 때, 앞으로 얻을 수 있는 reward의 기댓값을 의미한다. qπ(st,at)=∑st+1,rt+1p(st+1,rt+1∣st,at)[rt+1+γ⋅∑at+1π(at+1∣st+1)qπ(st+1,at+1)] q_{\\pi} (s_t, a_t) = \\sum_{s_{t+1}, r_{t+1}} p(s_{t+1}, r_{t+1}|s_t, a_t) [r_{t+1} + \\gamma \\cdot \\sum_{a_{t+1} } \\pi(a_{t+1}|s_{t+1})q_{\\pi}(s_{t+1}, a_{t+1})] qπ​(st​,at​)=st+1​,rt+1​∑​p(st+1​,rt+1​∣st​,at​)[rt+1​+γ⋅at+1​∑​π(at+1​∣st+1​)qπ​(st+1​,at+1​)] 둘 다 현재 어떤 상황에서 앞으로 얻을 수 있는 reward의 기댓값을 의미한다. Bellman Equation 현재 시간 ttt에서의 value와 다음 시간 t+1t+1t+1에서의 value와의 관계식을 의미한다. State-value Bellman equation과 action-value Bellman equation이 존재하며, reinforcement learning 알고리즘 구현에 있어서 가장 중요한 알고리즘 중 하나이다. Bellman Equation vs Value Function 두 용어의 개념에 대한 차이는 거의 없다. Bellman equation도 value function이다. 다만, ttt에서의 value function을 t+1t+1t+1에서의 value function에 대한 식으로 나타냈다 뿐. Recursive하게 표현한 value function을 Bellman equation이라고 부를 뿐이다. State-value Bellman Equation State-value function에 대한 Bellman equation으로, state-value function입장에서, ttt에서의 state value와 t+1t+1t+1에서의 state value와의 관계식이다. vπ(st)=∑aπ(a∣st)∑st+1,rp(st+1,r∣st,a)[r+γ⋅vπ(st+1)]v_{\\pi} (s_t) = \\sum_a \\pi(a|s_t) \\sum_{s_{t+1},r} p(s_{t+1}, r|s_{t}, a)[r + \\gamma \\cdot v_{\\pi}(s_{t+1})] vπ​(st​)=a∑​π(a∣st​)st+1​,r∑​p(st+1​,r∣st​,a)[r+γ⋅vπ​(st+1​)] Action-value Bellman Equation Action-value function에 대한 Bellman equation으로, action-value funciton입장에서, ttt에서의 actionvalue와 t+1t+1t+1에서의 action value와의 관계식이다. qπ(st,at)=∑st+1,rp(st+1,r∣st,at)[r+γ⋅∑at+1π(at+1∣st+1)⋅qπ(st+1,at+1)]q_{\\pi} (s_{t}, a_{t}) = \\sum_{s_{t+1}, r} p(s_{t+1}, r|s_t, a_t)[r + \\gamma \\cdot \\sum_{a_{t+1} } \\pi(a_{t+1}|s_{t+1}) \\cdot q_{\\pi}(s_{t+1}, a_{t+1})] qπ​(st​,at​)=st+1​,r∑​p(st+1​,r∣st​,at​)[r+γ⋅at+1​∑​π(at+1​∣st+1​)⋅qπ​(st+1​,at+1​)] Compute Value using Bellman Equation Bellman equation의 가장 큰 장점은, value function을 매우 효율적으로 계산할 수 있게 해 준다는 것이다. Value function은 정의에서 보다시피, 미래의 reward의 기댓값이다. 즉, 현재 시간 ttt이후의 모든 시간에서의 reward 기댓값인데, 이 정의로는 value를 계산할 수 없다. Bellman equation은 이 무한 수열 계산문제를 단순한 linear equation으로 바꿔준다. 다음 board를 생각해 보자. 보드에는 A,B,C,DA,B,C,DA,B,C,D라는 4개의 공간이 있으며, 말 하나를 이 공간 내에서 움직이려 한다. 즉, 각 공간이 곧 state이며, 총 4개의 state가 있는 environment이다. Action은 상,하,좌,우 4개의 움직임이 존재한다. Policy는 총 4개의 움직임에 대해 uniform distribution이다. 말이 BBB로 들어오거나 BBB에 머무는 움직임에 대해서만 reward +5를 부여하고 나머지는 0을 부여한다. Discount factor는 0.7로 하자. State AAA에서의 value는 무한 수열식이지만, Bellman equation을 이용한다면, 다음 state의 value를 이용해서 계산이 가능하다. V(A)=∑aπ(a∣A)∑s′,rp(s′,r∣a,A)[r+γ⋅V(s′)]V(A) = \\sum_{a} \\pi(a|A) \\sum_{s&#x27;,r} p(s&#x27;,r|a,A)[r + \\gamma \\cdot V(s&#x27;)] V(A)=a∑​π(a∣A)s′,r∑​p(s′,r∣a,A)[r+γ⋅V(s′)] 그런데, action이 정해지면, state는 확정(deterministic)이므로, 위 Bellman equation을 다음처럼 변경할 수 있다. V(A)=∑aπ(a∣A)[0+0.7⋅V(s′)]V(A) = \\sum_a \\pi (a|A) [0 + 0.7 \\cdot V(s&#x27;)] V(A)=a∑​π(a∣A)[0+0.7⋅V(s′)] V(A)=14⋅0.7⋅V(C)+12⋅0.7⋅V(A)+14⋅(5+0.7⋅V(B)))V(A) = \\frac{1}{4} \\cdot 0.7 \\cdot V(C) + \\frac{1}{2} \\cdot 0.7 \\cdot V(A) + \\frac{1}{4} \\cdot (5 + 0.7 \\cdot V(B))) V(A)=41​⋅0.7⋅V(C)+21​⋅0.7⋅V(A)+41​⋅(5+0.7⋅V(B))) V(B),V(C),V(D)V(B), V(C), V(D)V(B),V(C),V(D)도 유사하게 V(A),V(B),V(C),V(D)V(A), V(B), V(C), V(D)V(A),V(B),V(C),V(D)에 대한 식으로 표현이 가능하며, 일차 연립방정식으롤 표현이 가능하다. 즉, 무한 수열을 푸는 문제가 일차 연립 방정식을 푸는 문제로 바뀐 것이다. 하지만, 현실에서는 approximation방법을 많이 이용한다. State개수가 많아서 그런가? Optimality Reinforcement learning의 목적은 단순히 value function과 policy를 계산하는게 아니라, optimal policy와 optimal value function, 즉, reward를 최대화하는 policy와 value function을 찾는 것이다. Optimal Policies Optimal policy란, 모든 state에서 가장 높은 value를 반환하게 하는 policy를 말한다. 즉, 다음 그림처럼 어떤 여러개의 policies들보다 항상 큰 value를 반환하게 하는 policy는 항상 존재한다. 즉, π1,π2\\pi_1, \\pi_2π1​,π2​보다 항상 크거나 같은 value를 반환하는 policy는 항상 존재한다는 건데, 방법은 간단하다. π1≤π2\\pi_1 \\leq \\pi_2π1​≤π2​인 state에서는 π2\\pi_2π2​의 policy를 따르고, π1&gt;π2\\pi_1 &gt; \\pi_2π1​&gt;π2​인 state에서는 policy π1\\pi_1π1​을 따르도록 하는 새로운 policy π3\\pi_3π3​를 만들면 된다. 이와 같은 방법으로, 언제나 모든 policy보다 크거나 같은 value를 반환하는 policy를 만들 수 있으며, 이런 policy는 unique할 수도 있고 여러개가 될 수도 있다(모든 state에 걸쳐서 똑같은 value를 반환하는 policy가 여러개일 수 있음). 어쨌든, 이런 과정을 거쳐서 가장 높은 value를 모든 state에 걸쳐서 반환하는 policy를 optimal policy라고 부른다. 방금 말했듯이, optimal policy는 반드시 존재하며, 여러개일 수 있다. 또 하나 생각할 점은, 위 그림에서 π3\\pi_3π3​은 분명히, π1,π2\\pi_1, \\pi_2π1​,π2​중 하나를 선택한 policy에 불과하므로, π1,π2\\pi_1, \\pi_2π1​,π2​둘 중 value가 높은 policy의 value와 같아야 할 것이데, 어느 지점에서는 π1,π2\\pi_1, \\pi_2π1​,π2​ 모두의 value보다 높다. 이것은, future value까지 반영해서 생기는 현상으로, 미래 state에서도 최선 policy인 π3\\pi_3π3​을 따르므로, value는 재료가 된 policy들보다 커질수도 있다. Optimal Values 보통 optimal policy는 unknown으로, 바로 계산할 수 없다. 애초에 reinforcement learning의 목적은 optimal policy를 찾는 것이다. Optimal policy를 계산할때는 opimal value function를 이용하게 된다. Optimal value function이란, 현재 state에서 가능한 모든 액션과 그에 다른 다음 value를 보고, 다음 value가 가장 높은 action을 deterministic하게 선택했을 때의 value function을 의미한다. v∗(s)=maxa ∑s′,rp(s′,r∣s,a)[r+γ⋅v∗(s′)]v_* (s) = \\underset{a}{ \\text{max} } ~ \\sum_{s&#x27;,r} p(s&#x27;,r|s,a)[r + \\gamma \\cdot v_* (s&#x27;) ] v∗​(s)=amax​ s′,r∑​p(s′,r∣s,a)[r+γ⋅v∗​(s′)] 보다시피, action의 분포(policy)가 사라지고, 그냥 다음 state인 s′s&#x27;s′의 value v∗(s′)v_* (s&#x27;)v∗​(s′)가 가장 높은 action을 무조건(deterministically) 취하게 한다. 또한, 이 value v∗(s′)v_* (s&#x27;)v∗​(s′)만으로 v∗(s)v_* (s)v∗​(s)를 계산하도록 한다. 앞서, value function은 두 가지가 있고, 두 가지 value function 모두 Bellman equation 형태로 바꿀 수 있었다. Optimal value function도 마찬가지이며, optimal한 value function을 Bellman equation형태로 바꾼 것을 Bellman optimality equation이라고 부른다. Bellman optimality equation for state value function v∗(s)=maxa ∑s′,rp(s′,r∣s,a)[r+γ⋅v∗(s′)]v_* (s) = \\underset{a}{\\text{max} } ~ \\sum_{s&#x27;,r} p(s&#x27;,r|s,a) [r + \\gamma \\cdot v_* (s&#x27;)] v∗​(s)=amax​ s′,r∑​p(s′,r∣s,a)[r+γ⋅v∗​(s′)] Bellman optimality equation for action value function q∗(s,a)=maxa∑s′,rp(s′,r∣s,a)[r+γ⋅maxa′ q∗(s′,a′)] q_* (s,a) = \\underset{a}{\\text{max} } \\sum_{s&#x27;,r} p(s&#x27;,r|s,a)[r + \\gamma \\cdot \\underset{a&#x27;}{\\text{max} } ~ q_* (s&#x27;,a&#x27;)] q∗​(s,a)=amax​s′,r∑​p(s′,r∣s,a)[r+γ⋅a′max​ q∗​(s′,a′)]","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"03. Markov Decision Process","date":"2020-03-03T01:00:02.000Z","path":"studynotes/reinforcement-learning/03-Markov-Decision-Process/","text":"Markov Decision Process 참고: Coursera Reinforcement Learning (Alberta Univ.) 마르코프 결정 과정. 이름에서 유추할수 있다시피, Markov assumption에 기반한 decision process로, t+1t+1t+1에서의 상태 st+1s_{t+1}st+1​는 오직 현재 ttt에서의 상태인 sts_{t}st​에 있을 때, agent의 decision인 ata_{t}at​에 의해서만 결정된다는 것이다. Markov Decisoin Process vs K-arm Bandit Problems Markov decision process에서는 K-arm bandit에서 가정했던 여러가지 조건을 해제한 application에 적용이 가능하다. K-arm bandit problem에서는 항상 하나의 state만 존재했지만, Markov decision process는 액션을 취함에 따라 state가 변하는 application에도 적용이 가능하다. K-arm bandit problem에는 언제나 선택할 수 있는 action 리스트가 고정되어 있었다. 하지만, Markov decision process가 적용되는 application은 그럴 필요가 없다. 각 state에 서로 다른 action list가 있을 수 있다. K-arm bandit problem에는 state와 선택 가능한 액션 리스트가 하나임과 동시에 매 time마다 optimal action은 항상 고정되어 있었다. 하지만, 이번에 이야기할 reinforcement learning environment는 state마다 optimal action이 다를 수 있다. Finite Markov Decision Process Agent와 상호작용하는 environment에는 여러 state가 있을 수 있는데, 이 state의 개수가 finite 하며, 각 state에서 존재하는 action 개수도 finite한 경우에 적용되는 Markov decision process를 finite Markov decision process라고 한다. 물론 finite 하지 않는 경우가 매우 많다. 현재 상태를 sss, 이 상태를 기준으로 내린 decision aaa, 그리고, 그 결정에 의해 변한 상태를 s′s&#x27;s′, 그로인해 받는 reward를 rrr라고 했을 때, Markov decision process의 state transition probability는 다음과 같다. p(s′,r∣s,a)p(s&#x27;,r|s,a) p(s′,r∣s,a) Episodic Tasks vs Continuous Tasks Episodic Tasks 바둑, 스타크래프트와 같은 게임처럼, &quot;한번의 판(한 판), stage&quot;이 존재하는 problem을 가리킨다. 따라서, terminal state 라는 것이 존재하며, 하나의 stage를 시작해서 끝난 후 최종 reward까지 받을 때 까지를 하나의 episode라고 부른다. Agent는 여러 episode를 체험해보면서 학습하게 된다. 한 episode에서 이런 선택을 했다면 다음 episode에서 다른 선택을 하면서 다른 결말 및 reward를 획득하면서 학습하게 되는 task이다. Episode는 이전의 모든 episode와 독립적이다. 즉, 이전 episode가 어떻게 끝났던 간에, 현재 episode는 이전 episode에 의해 영향을 받지 않는다. 매 게임이 독립이라는 이야기이다. Continuous Tasks 일반적인 로봇이 수행하는 작업들이 보통 continuous task이다. 이 경우, terminal state가 없으며, 그냥 life를 살아가면서 마주치는 state에서 action을 수행하면서 학습을 진행하게 된다. Goals of MDP MDP의 목적은 당장 action을 선택했을 때의 reward를 최대화 하는 것이 아닌, 현재 어떤 action을 선택한 후, 미래의 모든 reward들 합의 기댓값을 최대하하도록 하는 action을 선택하는 것이다. 즉, 다음과 같은 action $a_t^* $를 선택한다. at∗=argmaxa E[Gt]=argmaxa E[Rt+1+⋯+RT]a_t^* = \\underset{a}{\\text{argmax} } ~ \\mathbb{E}[G_t] = \\underset{a}{\\text{argmax}} ~ \\mathbb{E}[R_{t+1} + \\cdots + R_T] at∗​=aargmax​ E[Gt​]=aargmax​ E[Rt+1​+⋯+RT​] 이때, TTT는 final state에서의 time 이다. 즉, 한 episode의 끝일때의 time이다. GtG_tGt​는 random variable인데, RtR_tRt​들이 random variable이고, random variable의 합이기 때문이다. 따라서, random variable GtG_tGt​의 기댓값을 최대화하는 action aaa를 선택하도록 한다. Goals of MDP for Continuous Tasks 위에서 소개한 action 선택법은 episodic task에만 적용이 가능하다. 미래의 모든 reward의 합의 기댓값이므로, terminal state가 존재해야 E[Gt]\\mathbb{E}[G_t]E[Gt​]가 finite(∞\\infty∞가 아님)하다. continuous task의 경우에는, RTR_TRT​가 없고 무한히 더해지기 때문에, E[Gt]≈∞\\mathbb{E}[G_t] \\approx \\inftyE[Gt​]≈∞가 된다. 따라서 discounting이라는 것을 통해 액션을 선택한다. at∗=argmaxa Gt=argmaxa [Rt+1+γRt+2+γ2Rt+3+⋯ ]=argmaxa [Rt+1+γGt+1]a_t^* = \\underset{a}{\\text{argmax} } ~ G_t = \\underset{a}{\\text{argmax} } ~ [R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots] = \\underset{a}{\\text{argmax} } ~ [R_{t+1} + \\gamma G_{t+1} ] at∗​=aargmax​ Gt​=aargmax​ [Rt+1​+γRt+2​+γ2Rt+3​+⋯]=aargmax​ [Rt+1​+γGt+1​] Discounting을 하는 이유는 GtG_tGt​를 finite하게 만들기 위함이며, 다음과 같기 때문에 finite하다. 이때, 0≤γ&lt;10 \\leq \\gamma &lt; 10≤γ&lt;1이어야 한다. RmaxR_{max}Rmax​를 agent가 한 액션을 취했을때 얻을 수 있는 액션의 최대치라고 하자. Gt=Rt+1+γRt+2+γ2Rt+3+⋯G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots Gt​=Rt+1​+γRt+2​+γ2Rt+3​+⋯ Gt≤Rmax+γRmax+γ2Rmax+⋯G_t \\leq R_{max} + \\gamma R_{max} + \\gamma^2 R_{max} + \\cdots Gt​≤Rmax​+γRmax​+γ2Rmax​+⋯ Gt≤Rmax(1+γ+γ2+⋯ )G_t \\leq R_{max}(1 + \\gamma + \\gamma^2 + \\cdots) Gt​≤Rmax​(1+γ+γ2+⋯) Gt≤Rmax⋅11−γ iff 0≤γ&lt;1G_t \\leq R_{max} \\cdot \\frac{1}{1 - \\gamma} ~ \\text{iff } 0 \\leq \\gamma &lt; 1 Gt​≤Rmax​⋅1−γ1​ iff 0≤γ&lt;1 따라서, 0≤γ&lt;10 \\leq \\gamma &lt; 10≤γ&lt;1을 만족하면, GtG_tGt​는 Rmax⋅11−γR_{max} \\cdot \\frac{1}{1 - \\gamma}Rmax​⋅1−γ1​보다 작다. 그리고, finite하다(∞\\infty∞가 아니다). 굳이 episodic task라고 해서 discounting을 사용하지 말라는 법은 없다. discount rate γ\\gammaγ를 통해 미래 reward 지향적일지, 즉각적인 reward 지향적일지 정할 수있기 때문에 discounting 방법은 episodic task에서도 많이 이용된다. Summary MDP란, 현재 상태만을 바탕으로 action을 취하고 reward를 얻는 환경에서의 reinforcement learning 방법 또는 decision process중 하나이다. 액션은 다음과 같이 취한다. a∗(t)=argmaxa(t) E[Gt]=argmaxa(t) E[Rt+1+γ⋅Gt+1]a^* (t) = \\underset{a(t)}{\\text{argmax} } ~ \\mathbb{E}[G_t] = \\underset{a(t)}{\\text{argmax} } ~ \\mathbb{E}[R_{t+1} + \\gamma \\cdot G_{t+1} ] a∗(t)=a(t)argmax​ E[Gt​]=a(t)argmax​ E[Rt+1​+γ⋅Gt+1​]","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"02. K-arm Bandits Problems","date":"2020-03-03T01:00:01.000Z","path":"studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/","text":"K-arm Bandits Problems 참고: Coursera Reinforcement Learning (Alberta Univ.) Reinforcement learning에서 가장 기본적인 문제로, K-arm bandit problem을 들 수 있다. K개의 arm이 있고, agent는 그 arm 중 하나를 골라야 하며, 하나를 골랐다면, environment는 그 arm의 reward를 반환한다. Multi-armed Bandits Problem 슬롯 머신문제, 의사의 치료법 선택 문제 등을 통칭해서 multi-armed bandits 종류의 문제라고 부른다. 슬롯 머신 문제 내가 어떤 슬롯 머신을 선택하면 분명히 해당 슬롯 머신의 stationary distribution에 의해 어떤 결과를 얻을 것이고 그 결과를 바탕으로 reward를 얻게 될 것이다. 하지만, stationary distribution은 인공지능이 알 수 없다. 하지만, 어떤 슬롯 머신을 많이 돌려봤다면 해당 머신에 대해서는 stationary distribution을 어느정도 추정할 수 있다. 추정한 놈들중 높은 reward를 주는 결과를 뱉는 머신을 선택할 수도 있고 새로운 머신을 돌려서 그 머신의 stationary distribution을 추정해 볼 수 있다. 그 머신이 지금까지 알고 있는 머신보다 더 좋은 결과를 줄 수도 있기 때문에 새로운 머신도 조사해봐야 한다. 의사의 치료법 선택 문제(이름은 신경쓰지 말자) 의사는 심각한 환자에게 신생 치료법을 제안할 수도 있고 최선이라고 알려진 방법을 선택할 수도 있다. 신생 치료법은 최선이라고 알려진 방법보다 나을수도, 쪽박일 수도 있다. 이런 종류의 문제를 K-armed bandits 문제라고 부른다. K-arm Bandit vs Reinforcement Learning K-arm bandit problem은 reinforcement learning에서 다음과 같은 제약조건을 걸어버린, 쉬운 문제에 속한다. 오직 하나의 state만 존재한다. 즉, 어떤 arm을 선택하는 액션을 취한다고 해서 state가 변하지는 않는다. 취할 수 있는 action 목록은 항상 동일하다. 시간이 지난다고 해서 가능한 액션이 늘어난다거나 줄어들지 않는다. 항상 optimal action이 일정하다. 시간에 따라 optimal action이 변하지 않는다. 유한한 환경이다. 즉, 선택 가능한 액션 개수, state개수 등이 finite하다. 일반적으로 reinforcement learning은 위 제약조건들이 없다. Terminology AtA_tAt​: 어떤 시간 ttt에서 취한 액션 RtR_tRt​: 어떤 시간 ttt에서 취한 액션으로 얻은 reward q∗(a)q_* (a)q∗​(a): 어떤 임의 액션 aaa를 취해서 얻는 reward의 기댓값 즉, q∗(a)=E(Rt∣At=a)q_* (a) = \\mathbb{E}(R_t|A_t=a) q∗​(a)=E(Rt​∣At​=a) (당연히 q∗(a)q_* (a)q∗​(a)는 인공지능이 알 수 없는 값이다. 이걸 알면 기댓값이 높은 길만 선택하면 된다.) Q∗(a)Q_* (a)Q∗​(a): 인공지능이 exploitation, exploration을 바탕으로 추정해낸 분포로 계산한, 임의 액션 aaa를 취했을 때의 reward 기댓값. 인공지능은 적절한 exploration을 통해 Q∗(a)Q_* (a)Q∗​(a)를 업데이트해서 q∗(a)q_* (a)q∗​(a)와 가깝게 추정해야 한다. Exploitation vs Exploration Value function을 말하기 앞서서, exploitation과 exploration은 서로 균형을 이뤄야 한다. 이 둘의 균형을 맞춰야 적절히 최대 reward를 찾아가는 결정을 하면서 새로운 길을 개척할 수 있다. 이것을 달성하는 방법으로 다음과 같은 것들이 있다. ϵ\\epsilonϵ-greedy methods (Epsilon-greedy) Optimal Initial Values UCB(Upper Confidence Bound) Methods Bayesian/Thompson Sampling Trade-off Between Exploration and Exploitation 일단, agent는 어떤 결정을 할때, exploration과 exploitation을 동시에 수행할 수 없다. 반드시 exploration을 할지, exploitation을 할지 선택하고 액션을 취해야 하는데, exploration을 하게 되면 당장 최선의 결과를 얻을 수 없으며, exploitation을 하게 되면 당장 최선의 결과를 얻을 수 있으나, 더 최상이 되는 다른 것을 찾아나설 수 없다. ϵ\\epsilonϵ-greedy Methods 매번 action을 선택할 때, ϵ\\epsilonϵ의 확률로 exploration을 하고, 1−ϵ1-\\epsilon1−ϵ의 확률로 exploitation을 하게 구현한 방법이다. 이 방식의 단점은 충분히 수렴한 뒤에도 ϵ\\epsilonϵ의 확률로 exploration을 하게 된다는 것이다(하지만 non-stationary distribution인 환경에서는 이게 더 도움이 된다). ϵ\\epsilonϵ이 크면 빠르게 optimal한 action 기댓값을 찾도록 exploration을 할 수 있지만, 지나치게 exploration을 많이 하고 수렴 이후에는 greedy action을 해야 하는데 exploration을 하고 있는 경우를 볼 수 있다. 그렇다고 ϵ\\epsilonϵ이 작으면 global optimal을 찾는 속도가 너무 느리다. Optimistic Initial Values ϵ\\epsilonϵ-greedy 방법에서, 각 액션의 초기 기댓값을 어떻게 정하느냐에 따라 알고리즘의 성능이 달라지기도 한다. 가장 기본적인 방법은 초기 기댓값을 0으로 두는 것으로, 이렇게 되면, ϵ\\epsilonϵ값을 0보다 크게 둬서 어느정도 exploration을 유도해야 한다. 하지만, 최대 reward를 알고, initial value값을 최대 reward보다 높은 값을 설정해 두면, 오직 greedy하게 선택하게 하는 것으로도 초기 iteration에서 적절히 exploration이 가능하다. 하지만, 어느정도 지나게 되면 exploration을 하지 않는다. 초기 reward 기댓값을 높게 설정하면, 아무리 좋은 reward를 얻는 액션이라 하더라도, 기댓값이 감소하기에, 다른 액션의 기댓값이 높아지게 된다. 따라서 자연스럽게 exploration을 하게 된다. 하지만, 초창기 iteration이 거의 exploration이 차지하기 때문에 수렴이 다소 느릴 수 있으나, 그 다음부터는 매우 빠르게 optimal에 수렴한다. optimal에 수렴한 이후부터는 exploitation만 수행하게 되기 때문에(이미 최적 expectation을 계산해서 낮은 기대치를 갖는 액션은 취하지 않는다.) 만약, optimal reward expectation이 변하는 환경, 즉, stationary distirbution이 변하는 환경에선 이 방법은 적합하지 않다. 시간이 지남에 따라 최적의 액션이 바뀌게 되면 이 방법이 소용이 없어진다. 다음을 만족하는 action을 선택한다. a∗=argmaxa q∗(a)a^* = \\underset{a}{\\text{argmax}} ~ q^* (a) a∗=aargmax​ q∗(a) Initial value estimation이 높기 때문에 오직 greedy하게 액션을 선택한다. UCB (Upper Confidence Bound) Confidence interval을 이용해서 액션을 선택하는 방법으로, 각 action의 value 기댓값을 추정한 후, 그 기댓값의 confidence interval를 계산한다. 그리고, 특정한 p-value에 대해 confidence interval의 upper bound를 구한 후, 가장 높은 upper bound를 가지는 action을 선택하는 방식이다. 이 방법의 장점은, 굳이 ϵ\\epsilonϵ의 확률을 정해놓고 exploration을 하게 하는 것이 아니라, exploration이 얼마 이루어지지 않아 불확실한 confidence boundary를 가지는 것을 알아서 선택하게 하고, 많이 exploration되어 확실하지만, 높은 기댓값으로 확실한 값을 선택하게 함으로써, exploration과 exploitation을 자동으로 조절해서 선택하게 한다. 시간이 지날수록 수렴하게 되면 exploration은 자동으로 줄어들게 된다. 즉 ,다음을 만족하는 action을 선택한다. a∗=argmaxa[q∗(a)+cln NNa]a^* = \\underset{a}{\\text{argmax} } [q^* (a) + c\\sqrt{\\frac{\\text{ln} ~ N} {N_a} } ] a∗=aargmax​[q∗(a)+cNa​ln N​​] 이때, argmaxa q∗(a)\\underset{a}{\\text{argmax}} ~ q^* (a)aargmax​ q∗(a)는 greedy 한 선택을 위한 term, 즉, exploitation을 위한 term이고, a∗=argmaxa clnNNaa^* = \\underset{a}{ \\text{argmax} } ~ c \\sqrt{ \\frac{ \\text{ln} N } {N_a} }a∗=aargmax​ cNa​lnN​​은 exploration을 위한 term이다. 즉, confidence interval인데, 다음 식을 이용해서 유도할 수 있다고 한다. P(∣Xˉ−μ∣≥ϵ)≤2e−2ϵ2NP(|\\bar{X} - \\mu| \\geq \\epsilon) \\leq 2e^{-2 \\epsilon^2 N} P(∣Xˉ−μ∣≥ϵ)≤2e−2ϵ2N Bayesian/Thompson Sampling UCB와 매우 유사하지만, value 기댓값의 confidence interval의 upper bound가 가장 큰 액션을 선택하는 것이 아니라, value의 posterior를 구하고 거기서 샘플링 한 후, 가장 큰 샘플을 가지는 액션을 선택하게 된다. a∗=argmaxa [xˉ∼Posterior(q∗(a))]a^* = \\underset{a}{\\text{argmax}} ~ [\\bar{x} \\sim \\text{Posterior}(q^*(a))] a∗=aargmax​ [xˉ∼Posterior(q∗(a))] 이것은 UCB와 마찬가지로 confidence/credible interval과 관련되어 있는데, value 샘플링을 할 때, 가장 높은 value가 나온 경우는 두 가지로 생각할 수 있다. 그 action에 대한 value estimation이 매우 불확실한 경우. 즉, credible interval이 매우 넓은 경우. 그 action의 value estimation이 그냥 높은 경우. 즉, optimal인 경우. 하는 방법은 다음과 같다. 한 액션의 value를 파라미터로 생각한다. Value의 likelihood를 모델링하고, conjugate prior를 설정한다. Value의 posterior를 계산한다. Posterior를 계산했으면, posterior를 이용해서 value하나를 샘플링한다. Conjugate prior가 아니라면? MCMC를 써서 수렴시킨 후에 나온 샘플 하나를 가저오면 되나? 이것을 각 액션에 대해서 반복하고 가장 높은 value 샘플을 가지는 액션을 취한다. Action-value Methods 어떤 액션을 골라야 할 때, 지금 현재 가지고 있는 지식만으로 각 액션을 취했을 때의 얻어지는 기댓값을 각각 계산하고, 가장 높은 기댓값을 가지는 액션을 취하는 방식이다. 즉, value function을 &quot;가장 큰 액션의 기댓값을 가지는 액션&quot;이라고 정의하는 것이다. 액션에 따른 reward의 기댓값 Q∗(a)Q_* (a)Q∗​(a)을 계산하는 방법은, 다음과 같이 할 수도 있고, 다른 방법을 사용할 수도 있다. Q∗(a)≈∑i=1t−1Ri⋅IAi=a∑i=1t−1IAi=a Q_* (a) \\approx \\frac{ \\sum_{i=1}^{t-1} R_i \\cdot I_{A_i=a} }{ \\sum_{i=1}^{t-1} I_{A_i=a} } Q∗​(a)≈∑i=1t−1​IAi​=a​∑i=1t−1​Ri​⋅IAi​=a​​ 즉, 이때까지 aaa라는 액션을 취했을 때, 얻었던 reward들의 평균값으로 aaa의 reward 기댓값이라고 삼는 것이다. 그리고, 다음을 만족하는 액션 AtA_tAt​를 선택한다. At=argmaxa Qt(a)A_t = \\text{argmax}_a ~ Q_t(a) At​=argmaxa​ Qt​(a) Action-value methods는 greedy한 방식으로, ϵ\\epsilonϵ-greedy와 함께 사용해서 exploration과의 균형을 맞추려고 시도해 볼 수 있다. Associative Search (Contextual Bandits) K-arm bandit problem은 매우 간단한 reinforcement learning 예제이다. 보통 흔히 이야기하는 reinforcement learning에 다음과 같은 제약조건을 걸면 K-arm bandit problem이 된다. 단 한 가지의 situation만 존재한다. 그에 따라, 액션이 situation에 영향을 미치지 않는다.(액션을 취한다고 해서 다음 time의 situation이 다른 situation으로 바뀌지는 않는다.) **상황(situation 또는 state)**이란, agent가 상호작용하는 environment의 한 객체라고 생각해도 되며, 상황이 바뀌면 reward를 샘플링하는 value function도 바뀐다. 따라서, 이 문제는 non-stationary problem이며, 각 상황들에 할당되어 있는 value function 역시 non-stationary일 수도 있다. Associate search를 통한 reinforcement learning은 contextual bandit problem이라고도 부른다. 반대로 이야기하면, full reinforcement learning은 다음과 같은 점때문에 K-arm bandit problem이랑 다르다. Environment안에 여러개의 situation이 state로 존재한다. 각 state에는 서로 다른 value function이 있다. 즉, 어떤 state에서는 액션 AiA_iAi​을 취하는게 optimal이지만, 다른 state에서는 액션 AjA_jAj​를 취하는게 optimal이 되기도 한다. state마다 취할 수 있는 액션 집합이 다를 수도 있다. 하나의 state입장에서, value function은 non-stationary distribution이 될 수 있다. Agent가 취하는 액션은 본인이 받는 reward에도 영향을 미치지만, 다음 time에서의 state에도 영향을 줄 수 있다. 즉, 액션이 state transition을 야기하기도 한다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"ReinforcementLearning","slug":"ReinforcementLearning","permalink":"https://wayexists02.github.io/tags/ReinforcementLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Reinforcement Learning","slug":"Study-Notes/Reinforcement-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Reinforcement-Learning/"}]},{"title":"Lagrangian Multiplication","date":"2020-03-01T23:51:55.000Z","path":"studynotes/machine-learning/Lagrangian-Multiplier/","text":"Lagrangian Multiplier Constraint optimization. 어떤 objective function을 파라미터에 대해 최대화하거나 loss function을 최소화하려고 하는데, 파라미터가 가질 수 있는 값에 제약조건이 있는 경우, Lagrangian multiplier을 사용할 수 있다. Background Lagrangian multiplier은 gradient vector의 방향 특성을 이용한, constraint optimization을 푸는 방법론중 하나이다. Lagrangian을 알기 위해서는 gradient vector의 특성을 파악해야 한다. Gradient Vectors 원 함수 F(x,y)=x2+y2−4=0F(x, y) = x^2 + y^2 - 4 = 0F(x,y)=x2+y2−4=0을 생각해보자. 이 함수의 differential dF(x,y)d F(x, y)dF(x,y)는 다음과 같다. dF(x,y)=∂F(x,y)∂xΔx+∂F(x,y)∂yΔyd F(x, y) = \\frac{\\partial F(x, y)}{\\partial x} \\Delta x + \\frac{\\partial F(x, y)}{\\partial y} \\Delta y dF(x,y)=∂x∂F(x,y)​Δx+∂y∂F(x,y)​Δy 왜냐하면, ∂F(x,y)∂xΔx\\frac{\\partial F(x, y)}{\\partial x} \\Delta x∂x∂F(x,y)​Δx만을 봤을 때, ∂F(x,y)∂x\\frac{\\partial F(x, y)}{\\partial x}∂x∂F(x,y)​는 yyy가 고정되어 있고, xxx만 변화시켰을때의 F(x,y)F(x, y)F(x,y)의 변화량이다. 즉, xxx방향의 기울기이다. 즉, ∂F(x,y)∂x\\frac{\\partial F(x, y)}{\\partial x}∂x∂F(x,y)​는 Δx\\Delta xΔx 양 만큼의 미세한 변화를 xxx축에 가했을 때, F(x,y)F(x, y)F(x,y)의 변화량이다. 이건 ∂F(x,y)∂y\\frac{\\partial F(x, y)}{\\partial y}∂y∂F(x,y)​도 마찬가지로 해석이 가능하다. 따라서, dF(x,y)d F(x, y)dF(x,y)는 x,yx, yx,y방향으로 각각 Δx,Δy\\Delta x, \\Delta yΔx,Δy만큼 변화를 가했을 때의 F(x,y)F(x, y)F(x,y)의 변화량이라고 볼 수 있다. 위 식은 다음처럼 표현할 수 있다. dF(x,y)=[∂F(x,y)∂x∂F(x,y)∂y]⋅[ΔxΔy]=∇F(x,y)⋅Δvd F(x, y) = \\begin{bmatrix} \\frac{\\partial F(x, y)}{\\partial x} \\newline \\frac{\\partial F(x, y)}{\\partial y} \\end{bmatrix} \\cdot \\begin{bmatrix} \\Delta x \\newline \\Delta y \\end{bmatrix} = \\nabla F(x, y) \\cdot \\Delta v dF(x,y)=[∂x∂F(x,y)​∂y∂F(x,y)​​]⋅[ΔxΔy​]=∇F(x,y)⋅Δv 이때, $$\\cdot$$은 내적이고, ∇F(x,y)\\nabla F(x, y)∇F(x,y)는 gradient vector, Δv\\Delta vΔv는 x,yx, yx,y가 변화하는 방향 벡터이다. Gradient vector ∇F(x,y)\\nabla F(x, y)∇F(x,y)는 F(x,y)F(x, y)F(x,y)의 surface(tangent plane)에 수직인 특징이 있다. Gradient Descent 어떤 함수 F(x,y)F(x, y)F(x,y)를 최소화하는 x,yx, yx,y를 찾고 싶을 때는, 최소가되는 그 지점에서의 gradient vector가 ∇F(x,y)=0\\nabla F(x, y)=0∇F(x,y)=0를 만족한다는 성질을 이용하면 된다. 이렇게 계산된다면, closed form으로 minimum점을 계산할 수 있다. 하지만, parameter(여기선 x,yx, yx,y)의 수가 많거나 식이 복잡해지면 그게 쉽지가 않고, 적절한 시간 안에 계산불가능할 수 있다. Gradient descent는 어떤 함수 F(x,y)F(x, y)F(x,y)가 있을 때, 이 함수의 최솟값을 iterative한 방식으로 계산하는 방법 중 하나이다. 특히, 최솟값을 찾기 위해 gradient vector를 이용해서 함수 F(x,y)F(x, y)F(x,y)의 그래프를 따라 하강하게 된다. 다음의 과정을 통해 gradient descent가 동작한다. 일단, x0,y0x_0, y_0x0​,y0​을 설정(초기값) (x0,y0)(x_0, y_0)(x0​,y0​)에서의 gradient vector ∇F(x0,y0)\\nabla F(x_0, y_0)∇F(x0​,y0​)을 계산한다. 여기서 F(x,y)F(x, y)F(x,y)의 변화량이 가장 작은(가장 큰 음수) 방향으로 x,yx, yx,y를 이동시켜야 하는데, 즉, Δv=[ΔxΔy]\\Delta v = \\begin{bmatrix} \\Delta x \\newline \\Delta y \\end{bmatrix}Δv=[ΔxΔy​]를 구해야 한다. Gradient vector에서 나온 식에서, dF(x,y)dF(x, y)dF(x,y)를 최소화하는, 내적을 구해야 하는데, gradient vector는 이미 계산했고, Δv\\Delta vΔv의 step size가 정해졌을 때, Δv\\Delta vΔv의 방향을 gradient vector와 180도 반대방향으로 가게 한다면, dF(x,y)dF(x, y)dF(x,y)가 절댓값이 가장 큰 음수가 될 것이다. 즉, Δv=−∇F(x,y)\\Delta v = -\\nabla F(x, y) Δv=−∇F(x,y) 하지만, step size를 1로 두면 너무 크다. 따라서, 작은 수 η\\etaη를 곱해준다. Δv=−η∇F(x,y)\\Delta v = - \\eta \\nabla F(x, y) Δv=−η∇F(x,y) Constraint Optimization 다음과 같이, 어떤 함수 F(x,y)F(x, y)F(x,y)를 최소화 또는 최대화하는데, 파라미터 x,yx, yx,y의 범위에 조건이 걸린 경우를 말한다. minx,y[F(x,y)=x2+y2] s.t x+y=1\\underset{x, y}{ \\text{min} } [F(x, y) = x^2 + y^2] ~ \\text{ s.t } ~ x + y = 1 x,ymin​[F(x,y)=x2+y2] s.t x+y=1 즉, x+y=1x + y = 1x+y=1을 만족하는 x,yx, yx,y중에서 x2+y2x^2+ y^2x2+y2를 최소화하는 x,yx, yx,y를 찾아야 한다는 것. 이 경우는 매우 간단하게 constraint 식을 F(x,y)F(x, y)F(x,y)에 대입해주면 된다. F(x,y)=(1−y)2+y2F(x, y) = (1 - y)^2 + y^2 F(x,y)=(1−y)2+y2 따라서, 이를 미분하고 gradient vector가 0이 되는 지점을 찾으면 될 것이다. 하지만, F(x,y)F(x, y)F(x,y)가 복잡하고, constraint 식 역시 복잡하며, 심지어 constraint가 여러개일 경우, 이렇게 closed form으로 구하는게 불가능해진다. 이를 좀 더 보편적으로 해결하기 위한 방법이 Lagrangian multiplication을 이용하는 것이다. Lagrangian Multiplication 어떤 objective function F(x,y)F(x, y)F(x,y)가 있고, constraint 함수인 g(x,y)g(x, y)g(x,y)가 있을 때, g(x,y)g(x, y)g(x,y)를 만족하면서 F(x,y)F(x, y)F(x,y)를 최대화하는 지점 (x′,y′)(x&#x27;, y&#x27;)(x′,y′)에서는, FFF의 gradient vector ∇F(x′,y′)\\nabla F(x&#x27;, y&#x27;)∇F(x′,y′)와 ggg의 gradient vector ∇g(x′,y′)\\nabla g(x&#x27;, y&#x27;)∇g(x′,y′)의 방향은 같거나 180도 방향이다. 그리고, 그 외 지점에서는 이게 성립되지 않는다. 따라서, 다음을 만족하는 (x′,y′)(x&#x27;, y&#x27;)(x′,y′)은 maximum 또는 minimum point라고 볼 수 있다. ∇F(x′,y′)=λ∇g(x′,y′)\\nabla F(x&#x27;, y&#x27;) = \\lambda \\nabla g(x&#x27;, y&#x27;) ∇F(x′,y′)=λ∇g(x′,y′) λ\\lambdaλ는 상수이며, 두 gradient vector가 반드시 같은 크기일 필요는 없다는 의미로 해석될 수 있다. 하지만, 방향은 반드시 평행하다. 이 수식을 이용해서 constraint optimization을 해결하는 방식을 lagrangian multiplication이라고 부르며, λ\\lambdaλ는 lagrangian constant라고 부른다. 예를들어, 다음을 만족하는 점을 찾는다고 가정한다. minx,y[F(x,y)=xy] s.t x2+y2−4=0\\underset{x, y}{ \\text{min} } [F(x, y) = xy] ~ \\text{ s.t } ~ x^2 + y^2 - 4 = 0 x,ymin​[F(x,y)=xy] s.t x2+y2−4=0 즉, g(x,y)=x2+y2−4=0g(x, y) = x^2 + y^2 - 4 = 0g(x,y)=x2+y2−4=0이다. 이 수식은 대입법을 이용해서 closed form으로 바로 풀 수 있지만, Lagrangian multiplier방법으로 풀어볼 수도 있다. ∇F(x,y)=[∂F(x,y)∂x∂F(x,y)∂y]=λ⋅[∂g(x,y)∂x∂g(x,y)∂y]\\nabla F(x, y) = \\begin{bmatrix} \\frac{\\partial F(x, y)}{\\partial x} \\newline \\frac{\\partial F(x, y)}{\\partial y} \\end{bmatrix} = \\lambda \\cdot \\begin{bmatrix} \\frac{\\partial g(x, y)}{\\partial x} \\newline \\frac{\\partial g(x, y)}{\\partial y} \\end{bmatrix} ∇F(x,y)=[∂x∂F(x,y)​∂y∂F(x,y)​​]=λ⋅[∂x∂g(x,y)​∂y∂g(x,y)​​] 이므로, [yx]=[2λx2λy]\\begin{bmatrix} y \\newline x \\end{bmatrix} = \\begin{bmatrix} 2 \\lambda x \\newline 2 \\lambda y \\end{bmatrix} [yx​]=[2λx2λy​] 일 것이다. 또한, 3변수 연립방정식을 풀기 위해 g(x,y)=x2+y2−4=0g(x, y) = x^2 + y^2 - 4 = 0g(x,y)=x2+y2−4=0도 같이 이용한다. 이 세 가지 수식을 이용한 연립방정식을 풀면, constraint를 만족하는 극점(극대, 극소)을 얻을 수 있다. 요약하면, 다음을 만족하는 점 (x,y)(x, y)(x,y)는 optimal point이다. 따라서, 다음 연립방정식을 풀면 된다. [∂F(x,y)∂x∂F(x,y)∂y]=[∂g(x,y)∂x∂g(x,y)∂y]\\begin{bmatrix} \\frac{\\partial F(x, y)}{\\partial x} \\newline \\frac{\\partial F(x, y)}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial g(x, y)}{\\partial x} \\newline \\frac{\\partial g(x, y)}{\\partial y} \\end{bmatrix} [∂x∂F(x,y)​∂y∂F(x,y)​​]=[∂x∂g(x,y)​∂y∂g(x,y)​​] g(x,y)=0g(x, y) = 0 g(x,y)=0 Constraint to non-Constraint Problem Lagrangian multiplication을 푸는 것은 다음 식을 만족하는 x⃗\\vec{x}x를 구하는 것이다. ∇F(x⃗)=λ⋅∇g(x⃗)\\nabla F(\\vec{x}) = \\lambda \\cdot \\nabla g(\\vec{x}) ∇F(x)=λ⋅∇g(x) 이 식을 조금 변형해보면, ∇F(x⃗)=∇λg(x⃗)\\nabla F(\\vec{x}) = \\nabla \\lambda g(\\vec{x}) ∇F(x)=∇λg(x) ∇F(x⃗)−∇λg(x⃗)=0\\nabla F(\\vec{x}) - \\nabla \\lambda g(\\vec{x}) = 0 ∇F(x)−∇λg(x)=0 ∇(F(x⃗)−λg(x⃗))=0\\nabla (F(\\vec{x}) - \\lambda g(\\vec{x})) = 0 ∇(F(x)−λg(x))=0 Q(x⃗,λ)=F(x⃗)−λg(x⃗)Q(\\vec{x}, \\lambda) = F(\\vec{x}) - \\lambda g(\\vec{x})Q(x,λ)=F(x)−λg(x)라고 정의해보면, ▽Q(x⃗,λ)=0\\bigtriangledown Q(\\vec{x}, \\lambda) = 0 ▽Q(x,λ)=0 으로 정리할 수 있다. 이것은, Q(x⃗,λ)Q(\\vec{x},\\lambda)Q(x,λ)를 non-constraint optimization을 한 식이 된다. 즉, F(x⃗)F(\\vec{x})F(x)를 어떤 constraint g(x⃗)g(\\vec{x})g(x)에 맞게 optimization을 한다는 것은, F(x⃗)−λg(x⃗)F(\\vec{x}) - \\lambda g(\\vec{x})F(x)−λg(x)를 non-constraint 환경에서 optimization하는 것과 같다. Neural network regularization도 해당 constraint (l1norm,l2norml_1 norm, l_2 norml1​norm,l2​norm) 에 맞게 losslossloss함수를 최적화하는 것이라고 해석할 수도 있지 않을까. 다만, 차이점은, Lagrangian 에선, λ\\lambdaλ도 파라미터이고, x⃗\\vec{x}x뿐 아니라 λ\\lambdaλ에 대해서도 최적화를 수행한다. Neural network regularization에서는 x⃗\\vec{x}x에 대해서만 최적화를 하며, λ\\lambdaλ는 하이퍼파라미터로 한다. 제약조건을 완전히 지키지는 않고, 약간의 제제만 가하는 것이라고 볼 수 있겠다. Multi-constraint Optimization 만약, F(x⃗)F(\\vec{x})F(x)를 최적화하는데, constraint가 g1(x⃗),g2(x⃗),⋯ ,gk(x⃗)g_1(\\vec{x}), g_2(\\vec{x}), \\cdots, g_k(\\vec{x})g1​(x),g2​(x),⋯,gk​(x) 등 kkk개가 있다고 해 보자. 이때, F(x⃗)F(\\vec{x})F(x)의 극점이면서, 위 constraint들을 만족시키는 x⃗\\vec{x}x를 구하는 것은 다음의 식을 푸는 것과 같다. ∇(F(x⃗)−λ1g1(x⃗)−λ2g2(x⃗)−⋯−λkgk(x⃗))=0\\nabla (F(\\vec{x}) - \\lambda_1 g_1(\\vec{x}) - \\lambda_2 g_2 (\\vec{x}) - \\cdots - \\lambda_k g_k (\\vec{x})) = 0 ∇(F(x)−λ1​g1​(x)−λ2​g2​(x)−⋯−λk​gk​(x))=0 또는 Q(x⃗,λ)=F(x⃗)−λ1g2(x⃗)−⋯−λkgk(x⃗)Q(\\vec{x}, \\lambda) = F(\\vec{x}) - \\lambda_1 g_2(\\vec{x}) - \\cdots - \\lambda_k g_k(\\vec{x})Q(x,λ)=F(x)−λ1​g2​(x)−⋯−λk​gk​(x)의 극점을 구하는 것과 같다. ∇Q(x⃗,λ)=0 w.r.t x⃗,λ\\nabla Q(\\vec{x}, \\lambda) = 0 ~ \\text{w.r.t} ~ \\vec{x}, \\lambda ∇Q(x,λ)=0 w.r.t x,λ","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"Principal Component Analysis","date":"2020-03-01T13:28:55.000Z","path":"studynotes/machine-learning/Principal-Component-Analysis/","text":"Principal Component Analysis (PCA) 데이터는 signal과 noise로 구성되어 있다. PCA의 핵심 목표는 데이터로부터 signal만 분리해 내는 것이다. PCA는 데이터 feature space를 회전시켜서 signal에 가까운 축과 noise에 가까운 축을 찾아준다. 그리고 데이터를 signal 축에 사영시킴으로써, 다른 축을 날려버리는데, 이때 데이터 정보 손실이 발생할 수 있다. 그러나, 그걸 감수하는 뛰어난 이득이 있기 때문에 PCA는 매우 유용하다. PCA의 주 활용 목적은 다음과 같이 크게 두 가지로 분류가 가능하다. (둘 다 dimensionality reduction이다) Feature Transformation Feature 공간의 축을 변환시켜서 signal축과 noise축을 구분되게 한다. 이때, 주의할 점은, PCA는 feature 공간을 non-linear하게 변환하는게 아니라 회전만 시킨다. 정확히 말하면, 축들만 회전시켜서 signal축과 noise축을 찾겠다는 것이다. Data Whitening Data whitening이란, 각 feature의 scale을 맞춰 주는 것을 말한다. PCA로 공간을 회전시킨 후엔, 각 축이 독립적이다. 그리고, 각 축의 variance가 구해지므로, 각 축을 standard deviation으로 나눠주면 whitening이 가능하다. Visualization PCA를 통해 signal에 가까운 축과 noise에 가까운 축을 찾아냈다면, signal에 가까운 축이 있을 것이다. Visualization을 위해 가장 signal다운 축 2개만 선택하는 방법도 있는데, 이럴 경우, 데이터에 대한 정보 상당수를 잃어버리지만, 데이터의 정보 상당량을 유지한 체, visualization을 할 수 있다. 다음은 2차원 데이터를 PCA를 통해 축 2개를 찾은 것을 보여준다. 파란색 방향의 축이 signal이 되고, 빨간색 방향의 축이 noise가 될 수 있다(이것은 상대적인 것으로, 절대적으로 어떤것이 noise라고는 판단할 수 없다). 여기서 만약에, 모든 데이터포인트를 파란색 축으로 사영시킨다고 하자. 그러면, 빨간색 방향의 정보는 사라지고, 데이터를 파란색 축 1차원으로만 표현이 가능하다. 빨간색 방향이 사라졌으므로 어느 정도의 데이터 분포에 대한 정보 손실이 있지만, 파란색 축은 대다수 정보를 보전하고 있는 축이다. 이렇게 데이터의 dimension을 reduction할 수 있는데, 이건 머신러닝 알고리즘의 속도를 크게 향상시킬 수 있다. 이건 매우 중요한 요소이다. 데이터의 모든 정보를 다 머신러닝에 때려넣기보단, 가장 메이저한 정보만 줘도 성능 하락없이 빠른 속도로 비슷한 결과를 달성할 수 있다. 이건 머신러닝에서 매우 중요하게 여기는, latent vector를 찾는다는 방향성과 일치한다. PCA Methods 그럼 signal 축은 무엇으로 정의해야 할지에 대해서 고민해야 한다. 정답을 말하면, signal 축은 데이터 분포가 가장 널리 퍼진 방향이다. 그래야, 사영 후에, 퍼진 정도의 정보를 최대한 보존할 수 있다. 즉, 데이터들을 이 축에 사영시키면, 다른 어느 축에 사영시키는 것보다 분산이 커야 한다. 즉, 분산이 가장 큰 축에 해당하는 벡터를 찾고, 데이터포인트를 그 벡터에 정사영시킨다. 참고로, 정보 이론을 잠깐 가져오면, 분산이 클수록 정보량이 많다. 즉, 정보량이 많은 축을 찾는 것이다. Data Rotation PCA는 데이터 feature space안에서 데이터간의 관계를 건들지 않는다. 그저 rotation만 시킴으로써, 축만 변화시킬 뿐이다. 변화시킨 축이 signal이라고 할 수 있는 축들이 있고, noise 라고 할 수 있는 축이 있어서, 축을 골라서 사영시킬 수 있을 뿐이다. 데이터 자체를 non-linear하게 변형시키지는 않는다. PCA Derivation 우리가 원하는 축의 단위 벡터를 eee라고 하자. 기본적으로 transpose가 없으면 column vector로 간주한다. 데이터들을 xxx라고 한다. 먼저, 데이터를 0-centerize시킨다. xi′=xi−μx_i&#x27;= x_i - \\mu xi′​=xi​−μ 그리고 난 후, 데이터 x′x&#x27;x′를 축 eee에 사영시킨 결과는, eTx′e^Tx&#x27;eTx′ 또는 x′Tex&#x27;^Tex′Te가 될 것이다. 이 사영시킨 결과로 나온 데이터들의 분포의 분산이 최대화되도록 하는 벡터 eee를 찾아야 한다. 또한, 여기서, eee는 단위 벡터로 제한하고자 한다. 즉, maxe1N∑i=1N(eTxi′)2=maxe1N∑i=1NeTxi′xi′Te\\underset{e}{ \\text{max} } \\frac{1}{N} \\sum_{i=1}^N (e^Tx_i&#x27;)^2 = \\underset{e}{ \\text{max} } \\frac{1}{N} \\sum_{i=1}^N e^Tx_i&#x27;x_i&#x27;^Te emax​N1​i=1∑N​(eTxi′​)2=emax​N1​i=1∑N​eTxi′​xi′T​e s.t eTe=1\\text{s.t} ~ e^Te = 1 s.t eTe=1 참고로, eTx′=x′Te=scalar valuee^Tx&#x27; = x&#x27;^Te = \\text{scalar value}eTx′=x′Te=scalar value이므로, (eTxi′)2(e^T x_i&#x27;)^2(eTxi′​)2을 위 식처럼 표현이 가능하다. 여기서, eee는 ∑\\sum∑에 영향을 받지 않는 변수이므로 밖으로 뺄 수 있다. maxe eT(1N∑i=1Nxi′xi′T)e\\underset{e}{\\text{max}} ~e^T (\\frac{1}{N}\\sum_{i=1}^N x_i&#x27; x_i&#x27;^T) e emax​ eT(N1​i=1∑N​xi′​xi′T​)e s.t eTe=1\\text{s.t} ~ e^T e = 1 s.t eTe=1 그런데, x′x&#x27;x′는 column vector이므로, ()()()안의 값은 x′x&#x27;x′의 covariance matrix와 같다는 것을 알 수 있다. 이를 Σ\\SigmaΣ라고 하자. maxe eTΣe\\underset{e}{ \\text{max} } ~ e^T \\Sigma e emax​ eTΣe s.t eTe=1\\text{s.t} ~ e^Te = 1 s.t eTe=1 이를 Lagrangian multiplier를 이용해서 식을 변형한다. maxe eTΣe−λ(eTe−1)=maxe F(e)\\underset{e}{\\text{max}} ~ e^T \\Sigma e - \\lambda (e^Te - 1) = \\underset{e}{\\text{max}} ~F(e) emax​ eTΣe−λ(eTe−1)=emax​ F(e) 이제 미분을 수행해서 그 결과가 0이 나오는 지점을 찾아야 한다. dF(e)de=2Σe−2λe=0\\frac{dF(e)}{de} = 2\\Sigma e - 2\\lambda e = 0 dedF(e)​=2Σe−2λe=0 Σe=λe\\Sigma e = \\lambda e Σe=λe 즉, 우리가 찾던 벡터 eee는 0-centered 된 데이터 x′x&#x27;x′의 covariance matrix의 eigen vector라는 사실을 알 수 있다. 그런데, 그럼, Σ\\SigmaΣ에겐 eigen vector가 많을 것인데, 어떤 eigen vector에다가 사영할 것인지 결정해야 할 것이다. 다음으로 다시 돌아가보면, maxe eTΣe\\underset{e}{\\text{max}} ~ e^T \\Sigma e emax​ eTΣe 위 식은 벡터 eee 방향으로의 분산에 대한 식을 변형시킨 결과로 얻었던 것이었다. 그런데, eee가 Σ\\SigmaΣ의 eigen vector라는 사실을 알았으므로, maxe eTΣe=maxe eTλe\\underset{e}{\\text{max}} ~ e^T \\Sigma e = \\underset{e}{\\text{max}} ~ e^T \\lambda e emax​ eTΣe=emax​ eTλe 가 되고, λ\\lambdaλ는 eigen value, 즉 스칼라값이므로, 맨 앞으로 올 수 있다. maxe λeTe\\underset{e}{\\text{max}} ~ \\lambda e^Te emax​ λeTe 근데, eTe=1e^Te = 1eTe=1이라고 이미 제약을 걸어놓았다. 즉, maxe λ\\underset{e}{\\text{max}} ~ \\lambda emax​ λ 다시말해, eigen value가 가장 큰 eigen vector를 고르면, 분산이 가장 큰 축을 찾을 수 있다는 말이 된다. 이 축은 데이터의 첫번째 principal component라고 부르며, 데이터의 가장 많은 정보를 포함하는 축이다. Principal Components After 1st Component 그래서, 가장 정보량이 많은 방향을 구했다. 그런데, dimensionality reduction을 하려고 할 때, 1차원으로만 압축해버리면 잃어버리는 정보가 매우 많다. 그래서, 첫번째 principal component(PC)를 제외하고 다른 방향으로 가장 많은 정보를 포함하는 축을 찾으려고 한다. 즉, 두번째 principal component를 찾을 것이다. 그런데 다음의 제한 사항이 있어야 한다. PC들은 모두 서로 직교해야 한다. 직교한다는 의미는 각 축이 서로 캡쳐하지 못하는 순수한 정보만 캡쳐할 수 있다는 이야기이다. 첫번째 PC와 비슷한 방향 축을 고른다고 해서 그 축이 정보를 많이 포함할까? 그 축은 첫번째 PC와 정보가 매우 많이 겹칠 것이다. 그래서, 첫번째 PC에 수직인 sub-space에서 다시 분산이 가장 큰 방향을 구하게 된다. 두 번째 PC를 e2e_2e2​라고 했을 때, e1Te2=0,e2Te2=1e_1^Te_2 = 0, e_2^Te_2 = 1e1T​e2​=0,e2T​e2​=1이라는 제약 조건을 만족시키면서 다음 variance를 최대화시켜야 한다. (XXX는 zero-centered 시켰다고 가정) Var(e2)=(Xe2)T(Xe2)=e2TΣe2\\text{Var}(e_2) = (Xe_2)^T(Xe_2) = e_2^T \\Sigma e_2 Var(e2​)=(Xe2​)T(Xe2​)=e2T​Σe2​ Lagrangian multiplier에 의해, argmaxe2 e2TΣe2−c1(e2Te2−1)−c2(e1Te2)\\underset{e_2}{ \\text{argmax} } ~ e_2^T \\Sigma e_2 - c_1(e_2^Te_2 - 1) - c_2(e_1^Te_2) e2​argmax​ e2T​Σe2​−c1​(e2T​e2​−1)−c2​(e1T​e2​) 이를 미분한 결과가 0이 나와야 하므로, 2Σe2−2c1e2−c2e1=02 \\Sigma e_2 - 2c_1e_2 - c_2 e_1 = 0 2Σe2​−2c1​e2​−c2​e1​=0 양변에 e1Te_1^Te1T​를 곱하면, 2e1TΣe2−2c1e1Te2−c2e1Te1=02 e_1^T \\Sigma e_2 - 2 c_1 e_1^T e_2 - c_2 e_1^T e_1 = 0 2e1T​Σe2​−2c1​e1T​e2​−c2​e1T​e1​=0 2e1TΣe2−c2=02 e_1^T \\Sigma e_2 - c_2 = 0 2e1T​Σe2​−c2​=0 그런데 이때, Σ\\SigmaΣ는 eigen decomposition에 의해 다음과 같다. (ddd는 데이터의 차원이라고 하자) Σ=(v1v2⋯vd)(λ10⋯00λ2⋯0⋯00⋯λd)(v1Tv2T⋯vdT)\\Sigma = \\begin{pmatrix} v_1 &amp; v_2 &amp; \\cdots v_d \\end{pmatrix} \\begin{pmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\cdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_d \\end{pmatrix} \\begin{pmatrix} v_1^T \\\\ v_2^T \\\\ \\cdots \\\\ v_d^T \\end{pmatrix} Σ=(v1​​v2​​⋯vd​​)⎝⎜⎜⎛​λ1​0⋯0​0λ2​0​⋯⋯⋯​00λd​​⎠⎟⎟⎞​⎝⎜⎜⎛​v1T​v2T​⋯vdT​​⎠⎟⎟⎞​ 따라서 위 식을 다음처럼 고칠 수 있다. 2e1T(v1v2⋯vd)(λ10⋯00λ2⋯0⋯00⋯λd)(v1Tv2T⋯vdT)e2−c2=02 e_1^T \\begin{pmatrix} v_1 &amp; v_2 &amp; \\cdots v_d \\end{pmatrix} \\begin{pmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\cdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_d \\end{pmatrix} \\begin{pmatrix} v_1^T \\\\ v_2^T \\\\ \\cdots \\\\ v_d^T \\end{pmatrix} e_2 - c_2 = 0 2e1T​(v1​​v2​​⋯vd​​)⎝⎜⎜⎛​λ1​0⋯0​0λ2​0​⋯⋯⋯​00λd​​⎠⎟⎟⎞​⎝⎜⎜⎛​v1T​v2T​⋯vdT​​⎠⎟⎟⎞​e2​−c2​=0 2(10⋯0)(λ10⋯00λ2⋯0⋯00⋯λd)(v1Tv2T⋯vdT)e2−c2=02 \\begin{pmatrix} 1 &amp; 0 &amp; \\cdots 0 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\cdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_d \\end{pmatrix} \\begin{pmatrix} v_1^T \\\\ v_2^T \\\\ \\cdots \\\\ v_d^T \\end{pmatrix} e_2 - c_2 = 0 2(1​0​⋯0​)⎝⎜⎜⎛​λ1​0⋯0​0λ2​0​⋯⋯⋯​00λd​​⎠⎟⎟⎞​⎝⎜⎜⎛​v1T​v2T​⋯vdT​​⎠⎟⎟⎞​e2​−c2​=0 (2λ10⋯0)(v1Tv2T⋯vdT)e2−c2=0\\begin{pmatrix} 2 \\lambda_1 &amp; 0 &amp; \\cdots 0 \\end{pmatrix} \\begin{pmatrix} v_1^T \\\\ v_2^T \\\\ \\cdots \\\\ v_d^T \\end{pmatrix} e_2 - c_2 = 0 (2λ1​​0​⋯0​)⎝⎜⎜⎛​v1T​v2T​⋯vdT​​⎠⎟⎟⎞​e2​−c2​=0 2λ1v1Te2−c2=02\\lambda_1 v_1^T e_2 - c_2 = 0 2λ1​v1T​e2​−c2​=0 근데, v1v_1v1​는 첫 번째 eigen vector로, e1e_1e1​과 같다. 2λ1e1Te2−c2=0c2=02 \\lambda_1 e_1^T e_2 - c_2 = 0 \\\\ c_2 = 0 2λ1​e1T​e2​−c2​=0c2​=0 다시 원래 최대화 식으로 돌아가보자. argmaxe2 e2TΣe2−c1(e2Te2−1)−c2(e1Te2)\\underset{e_2}{ \\text{argmax} } ~ e_2^T \\Sigma e_2 - c_1(e_2^Te_2 - 1) - c_2(e_1^Te_2) e2​argmax​ e2T​Σe2​−c1​(e2T​e2​−1)−c2​(e1T​e2​) 이건 다음처럼 변경된다. argmaxe2 e2TΣe2−c1(e2Te2−1)\\underset{e_2}{ \\text{argmax} } ~ e_2^T \\Sigma e_2 - c_1(e_2^Te_2 - 1) e2​argmax​ e2T​Σe2​−c1​(e2T​e2​−1) 얘네를 미분하는 것? 첫 번째 PC를 구할때 지나왔던 길과 같다. 따라서, 다음처럼 유도될 것이다. Σe2=λ2e2\\Sigma e_2 = \\lambda_2 e_2 Σe2​=λ2​e2​ 즉, e2e_2e2​ 역시, eigen vector이며, 이 e2e_2e2​방향으로도 e1e_1e1​ 방향을 제외하고 분산이 가장 커야 한다. 다음의 식에 의해 e2e_2e2​방향의 분산은 eigen value λ2\\lambda_2λ2​와 같다. Var(e2)=e2TΣe2=e2Tλ2e2=λ2\\text{Var}(e_2) = e_2^T \\Sigma e_2 = e_2^T \\lambda_2 e_2 = \\lambda_2 Var(e2​)=e2T​Σe2​=e2T​λ2​e2​=λ2​ 따라서, e1e_1e1​방향의 분산인 λ1\\lambda_1λ1​보다 작으면서 가장 큰 eigen value, 즉, 두번째로 큰 eigen value가 두 번째 PC의 분산값이 된다. 즉, 두번째로 큰 eigen value에 해당하는 eigen vector가 두 번째 PC축이다. (사실, 두 번째 PC를 다음처럼 추측도 가능하다. 왜냐하면, covariate matrix는 symmetric matrix이다. 그런데, symmetric matrix의 eigen vector들은 모두 서로 직교한다. 따라서, 첫 번째 PC가 eigen vector임이 밝혀진 상황에서, 두 번째 PC는 첫 번째 PC와 직교하므로, 두 번째 PC 역시 eigen vector라는 사실을 알 수 있다.) Covariance between PCs 데이터를 각 PCs(covariate matarix의 eigen vector들)에 사영시켰을 때, 사영된 데이터들간 covariance 또는 correlation은 0이다. 편의를 위해 2차원이라고 생각해보자. 2차원에 분포된 데이터들 xix_ixi​가 있고, 그들의 principal component를 e1,e2e_1, e_2e1​,e2​라고 했을 때, x1x_1x1​을 e1e_1e1​에 사영시킨 것은 (xiTe1)e1(x_i^T e_1)e_1(xiT​e1​)e1​이고, e2e_2e2​에 사영시킨 것은 (xiTe2)e2(x_i^T e_2)e_2(xiT​e2​)e2​이다. 따라서 이들의 covariance가 0이라는 것은 다음을 만족한다는 의미이다. ((xiTe1)e1)T⋅(xiTe2)e2=0((x_i^Te_1)e_1)^T \\cdot (x_i^T e_2)e_2 = 0 ((xiT​e1​)e1​)T⋅(xiT​e2​)e2​=0 이때, ()()()안에 있는 term들은 모두 scalar값이므로(내적이니까), (xiTe1)e1T⋅(xiTe2)e2=0(x_i^T e_1)e_1^T \\cdot (x_i^T e_2) e_2 = 0 (xiT​e1​)e1T​⋅(xiT​e2​)e2​=0 (xiTe1)(xiTe2)e1Te2=0(x_i^T e_1)(x_i^T e_2)e_1^Te_2 = 0 (xiT​e1​)(xiT​e2​)e1T​e2​=0 (xiTe1)(xiTe2)0=0(x_i^T e_1)(x_i^T e_2)0 = 0 (xiT​e1​)(xiT​e2​)0=0 0=00 = 0 0=0 Symmetric matrix의 eigen vector끼리는 orthogonal하므로 e1Te2=0e_1^Te_2 = 0e1T​e2​=0이다. 따라서, 등식이 성립하고, 데이터 포인트에 대해 PC축들은 서로 covariance가 0이다. 그러니까, covariance matrix Cov(i,j)\\text{Cov}(i,j)Cov(i,j)에서, Cov(i,j)=0 if i≠j\\text{Cov}(i,j) = 0 ~ \\text{if} ~ i \\not = jCov(i,j)=0 if i​=j라는 의미. Implementation of PCA (Dimensionality Reduction) 구현에서는 데이터 행렬 XXX의 각 데이터 포인트는 row-vector임을 명시한다. 나머지는 모두 column-vector이다. PCA의 구현은 다음과 같이 요약이 가능하다. 데이터를 0-centered 한다. Z=X−μXZ = X - \\mu_XZ=X−μX​ ZZZ의 covariate matrix를 계산한다. Σ=ZTZ\\Sigma = Z^TZΣ=ZTZ 또는, np.cov(X)로 바로 계산 Σ\\SigmaΣ의 eigen decomposition을 계산한다. VΛVTV\\Lambda V^TVΛVT, np.linalg.eig(cov_mat)로 계산 Eigen value를 내림차순으로 정렬한다. 당연히 eigen vector들도 동반 정렬되어야 한다. 1234evalues, evectors = np.linalg.eig(cov_mat)lst = sorted(zip(evalues, evectors), key=lambda item: item[0], reverse=True)# result:# [(e1, v1), (e2, v2), ...] 최상위 kkk개의 eigen vector를 뽑아낸다. 12evectors = list(map(lambda item: item[1], lst))[:k]E = np.hstack(evectors) 데이터 포인트를 각 eigen vector에 사영시킨다. inner prod(xi,ej)\\text{inner prod}(x_i, e_j)inner prod(xi​,ej​) 1X_reduced = np.matmul(X, E) Implementation of Data Whitening 데이터 whitening이란, 데이터의 분포를 타원형에서 원형으로 re-scaling해주는 것을 말한다. 각 feature들의 scale을 일치시킨다. PCA를 이용하면 이를 수행할 수 있다. 단, feature는 eigen vector로 변환된 상태로 whitening이 이루어진다. 방법은, PCA로 데이터를 rotation시킨 후(basis가 eigen vector들이 된다), 각 eigen vector축 방향을 그 방향의 standard deviation으로 나눠준다. PCA로 변환된(rotation된) 데이터들을 ZZZ라고 했을 때, (1λ10⋯001λ2⋯0⋯00⋯1λd)Z\\begin{pmatrix} 1 \\over \\sqrt{\\lambda_1} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 \\over \\sqrt{\\lambda_2} &amp; \\cdots &amp; 0 \\\\ \\cdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\over \\sqrt{\\lambda_d} \\end{pmatrix} Z ⎝⎜⎜⎜⎛​λ1​​1​0⋯0​0λ2​​1​0​⋯⋯⋯​00λd​​1​​⎠⎟⎟⎟⎞​Z 왜냐하면, 각 축 방향의 variance는 eigen vector들이기 때문. 구현은 다음과 같다. 데이터를 0-centered 한다. Z=X−μXZ = X - \\mu_XZ=X−μX​ ZZZ의 covariate matrix를 계산한다. Σ=ZTZ\\Sigma = Z^TZΣ=ZTZ 또는, np.cov(X)로 바로 계산 Σ\\SigmaΣ의 eigen decomposition을 계산한다. VΛVTV\\Lambda V^TVΛVT, np.linalg.eig(cov_mat)로 계산 데이터 XXX를 eigen vector에 사영시킨다. (정렬은 해도되고 안해도되는데, dimensionality reduction까지 하려면 정렬하고 kkk개만 뽑고 거기에 사영시킨다) 123evalues, evectors = np.linalg.eig(cov_mat)E = np.hstack(evectors)X_transformed = np.matmul(X, E) 얘네에다가 Λ−12\\Lambda^{-\\frac{1}{2}}Λ−21​를 곱해준다. (Dimensionality reduction했으면 kkk개만 있는 diagonal matrix이다) 12LAMBDA = np.sqrt(np.diag(evalues))X_whitened = np.matmul(LAMBDA, X_transformed)","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]},{"title":"Appendix 2. Empirical Bayes","date":"2020-03-01T13:08:16.000Z","path":"studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/","text":"Empirical Bayes 하이퍼파라미터 또한 데이터로 구하자. 베이지안 모델링을 하게 되면, likelihood를 모델링하고, 그 파라미터를 prior로 모델링을 하게 된다. D∼Lieklihood(θ) [P(D∣θ)]D \\sim \\text{Lieklihood} ( \\theta ) ~ [ P(D|\\theta) ] D∼Lieklihood(θ) [P(D∣θ)] θ∼prior(λ)\\theta \\sim \\text{prior} ( \\lambda ) θ∼prior(λ) 그런데, 이때, prior의 파라미터(위에서는 λ\\lambdaλ)는 하이퍼파라미터로, 사용자가 직접 constant로 세팅해 주게 된다. Empirical bayes는 이 하이퍼파라미터를 사용자 대신, 데이터를 이용해서 MAP으로 추론하는 것을 말한다(point estimation). 즉, λ^=argmaxλ P(λ∣D)\\hat{\\lambda} = \\underset{\\lambda}{ \\text{argmax} } ~ P(\\lambda|D) λ^=λargmax​ P(λ∣D) 하이퍼파라미터의 posterior를 구하는 방법은 다음과 같다. P(λ∣D)≈P(λ)∫P(D∣θ)P(θ∣λ)dθP(\\lambda|D) \\approx P(\\lambda) \\int P(D|\\theta)P(\\theta|\\lambda) d\\theta P(λ∣D)≈P(λ)∫P(D∣θ)P(θ∣λ)dθ 그런데, 이때, λ\\lambdaλ의 prior가 필요해지는데, 그냥 uniform prior로 둔다. 그럼 다음과 같다. P(λ∣D)≈∫P(D∣θ)P(θ∣λ)dθ≈P(D∣λ)P(\\lambda|D) \\approx \\int P(D|\\theta)P(\\theta|\\lambda) d\\theta \\approx P(D|\\lambda) P(λ∣D)≈∫P(D∣θ)P(θ∣λ)dθ≈P(D∣λ) 그리고, 이놈을 최대화하는 λ\\lambdaλ를 구하는 것이다. λ^=argmaxλ ∫P(D∣θ)P(θ∣λ)dθ\\hat{\\lambda} = \\underset{\\lambda}{ \\text{argmax} } ~ \\int P(D|\\theta)P(\\theta|\\lambda) d\\theta λ^=λargmax​ ∫P(D∣θ)P(θ∣λ)dθ Examples: beta-binomial Model 어떤 데이터에 대해 다음처럼 모델링했다 치자. xi∼binom(xi∣Ni,θi)x_i \\sim \\text{binom}(x_i|N_i, \\theta_i) xi​∼binom(xi​∣Ni​,θi​) θi∼beta(θi∣a,b)\\theta_i \\sim \\text{beta}(\\theta_i|a, b) θi​∼beta(θi​∣a,b) 이때, 각 데이터 샘플이 서로 다른 binomial distribution에서 왔다는 것에 주목한다. 즉, Ni,θiN_i, \\theta_iNi​,θi​가 데이터마다 모두 다르다. 따라서, likelihood는 다음과 같다. Likelihood(X∣Θ)=∏ibinom(xi∣Ni,θi)\\text{Likelihood}(X|\\Theta) = \\prod_i \\text{binom}(x_i|N_i, \\theta_i) Likelihood(X∣Θ)=i∏​binom(xi​∣Ni​,θi​) 그럼 다음처럼 EB(Empirical Bayes)를 이용해서 a,ba, ba,b에 대한 posterior에 비례(approximate)하는 함수를 구할 수 있다. P(a,b∣D)≈∏i∫binom(xi∣Ni,θi)⋅beta(θi∣a,b) dθiP(a, b|D) \\approx \\prod_i \\int \\text{binom}(x_i|N_i, \\theta_i) \\cdot \\text{beta}(\\theta_i|a, b) ~ d\\theta_i P(a,b∣D)≈i∏​∫binom(xi​∣Ni​,θi​)⋅beta(θi​∣a,b) dθi​ 이 식을 최대화하는 a,ba, ba,b를 구하면 된다. =∏ibeta(a+xi,b+Ni−xi)beta(a,b)= \\prod_i \\frac{\\text{beta}(a + x_i, b + N_i - x_i)}{\\text{beta}(a, b)} =i∏​beta(a,b)beta(a+xi​,b+Ni​−xi​)​ (왜 저렇게 나오지??)","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"Appendix 1. Maximize a Posterior","date":"2020-03-01T13:08:15.000Z","path":"studynotes/bayesian-statistics/APPENDIX-1-MAP/","text":"Maximize a Posterior 사후확률 최대화 추정법(MAP Estimation). Likelihood를 최대화하는 추정법(MLE - Maximum Likelihood Estimation)은 오직 데이터만을 이용해서 파라미터를 point estimation하는 방법이다. 하지만, MAP는 선행 지식 또는 개인의 믿음을 파라미터 추정에 집어 넣고, 데이터의 정보와 합해서 파라미터를 point estimation하는 방법이다. 방법은 다음과 같다. 데이터에 대한 likelihood P(D∣θ)P(D|\\theta)P(D∣θ)를 모델링한다. Likelihood P(D∣θ)P(D|\\theta)P(D∣θ)의 파라미터 θ\\thetaθ의 prior P(θ)P(\\theta)P(θ)를 모델링한다. Prior P(θ)P(\\theta)P(θ)와 likelihood P(D∣θ)P(D|\\theta)P(D∣θ)를 이용해서 posterior P(θ∣D)P(\\theta|D)P(θ∣D)를 직접 계산할 수 있으면 계산하되, 불가능하다면, posterior와 비례하는 함수 g(θ∣D)g(\\theta|D)g(θ∣D)를 계산한 P(θ∣D)=P(D∣θ)P(θ)∑θ′P(D∣θ′)P(θ′)P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{\\sum_{\\theta&#x27;}P(D|\\theta&#x27;)P(\\theta&#x27;)} P(θ∣D)=∑θ′​P(D∣θ′)P(θ′)P(D∣θ)P(θ)​ 분모인 normalization constant가 계산 불가능할 수도 있다. 그럼, g(θ∣D)g(\\theta|D)g(θ∣D)를 대신 구한다. P(θ∣D)≈g(θ∣D)=P(D∣θ)P(θ)P(\\theta|D) \\approx g(\\theta|D) = P(D|\\theta)P(\\theta) P(θ∣D)≈g(θ∣D)=P(D∣θ)P(θ) 보통, 파라미터의 prior P(θ)P(\\theta)P(θ)가 likelihood P(D∣θ)P(D|\\theta)P(D∣θ)와 conjugate를 이루면, posterior를 직접 계산할 수 있을 것이다. 그렇지 못하면서 가능한 θ\\thetaθ의 개수가 많다면, g(θ∣D)g(\\theta|D)g(θ∣D)를 계산해야 하는 경우가 많다. Posterior P(θ∣D)P(\\theta|D)P(θ∣D)가 가장 커지는 θ\\thetaθ를 구한다. θ^=argmaxθ P(θ∣D)\\hat{\\theta} = \\underset{\\theta}{\\text{argmax}} ~ P(\\theta|D) θ^=θargmax​ P(θ∣D) 또는 g(θ∣D)g(\\theta|D)g(θ∣D)가 가장 커지는 θ\\thetaθ를 구한다. θ^=argmaxθ g(θ∣D)\\hat{\\theta} = \\underset{\\theta}{\\text{argmax}} ~ g(\\theta|D) θ^=θargmax​ g(θ∣D) 미분을 통한 극점을 이용하자. (Gradient descent 같은) Is MAP a Bayesian Method? Bayes 통계의 특징은, 파라미터를 하나의 값이 아닌 분포로 본다는 것이다. 하지만, MAP는 파라미터의 분포를 추정하는게 최종 목표가 아니라 파라미터를 point estimation하는 것이 최종 목표이다. MAP의 계산 과정상 파라미터의 posterior를 계산하게 되지만, 결국은 point estimation으로, 파라미터를 하나의 값으로 본다는 점에서 완전한 베이지안 통계적 방법이라고 보지 않는 경우가 많다고 한다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"14. Predictive Simulations","date":"2020-03-01T13:08:14.000Z","path":"studynotes/bayesian-statistics/14_Predictive_Simulation/","text":"Predictive Simulation 어떤 예측해야 할 변수 λ\\lambdaλ의 분포에서 샘플링하는 시뮬레이션을 말한다. λ\\lambdaλ를 데이터를 관찰하기 전의 분포 p(λ)p(\\lambda)p(λ)에서 시뮤레이션하는가, 관찰한 후의 분포 p(λ∣D)p(\\lambda|D)p(λ∣D)에서 시뮬레이션 하는가에 따라 다음과 같이 나뉜다. Prior Predictive Simulation Posterior Predictive Simulation Prior Predictive Simulation Prior를 설정한 후, predictive simulation을 수행하는 것을 말한다. 다음처럼 모델링한 모델이 있다고 하자. yi∣λj∼Pois(λj)y_i|\\lambda_{j} \\sim \\text{Pois}(\\lambda_{j}) yi​∣λj​∼Pois(λj​) λj∣α,β∼Gamma(α,β)\\lambda_j|\\alpha,\\beta \\sim \\text{Gamma}(\\alpha, \\beta) λj​∣α,β∼Gamma(α,β) α∼p(α),β∼p(β)\\alpha \\sim p(\\alpha), \\beta \\sim p(\\beta) α∼p(α),β∼p(β) 이때, α,β\\alpha,\\betaα,β에 대한 prior를 각각 설정했다면, 그 prior를 바탕으로 $\\alpha^* , \\beta^* $를 샘플링할 수 있다. 그런 다음, $\\lambda^* $를 샘플링한다. 이때, $\\lambda^* $를 샘플링하는 확률분포는 다음처럼 표시할 수 있다. p(λ∗)=∫p(λ∗∣α,β)p(α)p(β) dα dβp(\\lambda^* ) = \\int p(\\lambda^* |\\alpha,\\beta) p(\\alpha) p(\\beta) ~d\\alpha ~d\\beta p(λ∗)=∫p(λ∗∣α,β)p(α)p(β) dα dβ 위 확률 분포에 따라 $\\lambda^* $를 샘플링하는 것을 prior predictive simulation이라고 부르고, 위 확률 분포를 prior predictive distribution이라고 부른다. 이 분포는 likelihood와 prior의 곱의 합으로 이루어진다. $\\lambda^* $를 샘플링했다면, λ\\lambdaλ와 마찬가지로 $y^* $를 샘플링할 수 있다. 일단 $\\lambda^* $를 얻었다면, 다음 식에 의해 $y^* $를 샘플링할 수 있다. p(y∗)=∫p(y∗∣λ)p(λ) dλp(y^* ) = \\int p(y^* |\\lambda)p(\\lambda) ~d\\lambda p(y∗)=∫p(y∗∣λ)p(λ) dλ 이렇게 계층을 올라가면서 각 파라미터와 예측값에 대해 prior predictive simulation을 할 수 있다. Posterior Predictive Simulation 데이터를 관측해서 prior를 credential distribution(posterior)로 수정한 이후, predictive simulation하는 것을 말한다. 이때, λ\\lambdaλ에 대한 시뮬레이션은 다음과 같다. p(λ∣D)=∫p(λ∣D,α,β)p(α∣D)p(β∣D) dα dβp(\\lambda|D) = \\int p(\\lambda|D,\\alpha,\\beta)p(\\alpha|D)p(\\beta|D) ~d\\alpha ~d\\beta p(λ∣D)=∫p(λ∣D,α,β)p(α∣D)p(β∣D) dα dβ 위 식을 posterior predictive distribution이라고 부르는데, prior predictive distribution과 다른 점은 각 파라미터 분포에 prior 대신 posterior가 쓰였다는 점이다. yyy의 경우도 마찬가지. p(y∣D)=∫p(y∣D,λ)p(λ∣D) pλp(y|D) = \\int p(y|D,\\lambda)p(\\lambda|D) ~p\\lambda p(y∣D)=∫p(y∣D,λ)p(λ∣D) pλ 시뮬레이션할 때는, posterior로부터 α∗,β∗\\alpha^* ,\\beta^*α∗,β∗를 샘플링하고, 그 α∗,β∗\\alpha^* , \\beta^*α∗,β∗를 이용해서 λ∗\\lambda^*λ∗를 샘플링한다. 그리고 그 λ∗\\lambda^*λ∗를 이용해서 y∗y^*y∗를 샘플링하면 된다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"13. Hierarchical Models","date":"2020-03-01T13:08:13.000Z","path":"studynotes/bayesian-statistics/13_Hierarchical_models/","text":"Hierarchical Models 데이터 생성 프로세스를 계층적으로 모델링한 것을 의미한다. 즉, likelihood의 파라미터는 또 다른 파라미터를 갖는 분포를 가지는 형태. 예를들어 likelihood는 poisson 분포를 따른다고 모델링하고, 그 파라미터 λ\\lambdaλ는 또 다른 파라미터 α,β\\alpha, \\betaα,β를 가지는 gamma 분포를 따른다고 모델링했다고 하자. 이 경우가 계층적 모델링에 속한다. yi∣λj∼Pois(λj)y_i|\\lambda_{j} \\sim \\text{Pois}(\\lambda_{j}) yi​∣λj​∼Pois(λj​) λj∣α,β∼Gamma(α,β)\\lambda_j|\\alpha,\\beta \\sim \\text{Gamma}(\\alpha, \\beta) λj​∣α,β∼Gamma(α,β) α∼p(α),β∼p(β)\\alpha \\sim p(\\alpha), \\beta \\sim p(\\beta) α∼p(α),β∼p(β) 이때, 각 λ\\lambdaλ는 여러개가 있고, 그중 하나에서 yyy가 생성되지만, λ\\lambdaλ는 모두 같은 분포에서 나온 녀석들이라고 모델링 한 것이다. p(α),p(β)p(\\alpha),p(\\beta)p(α),p(β)는 각각 α,β\\alpha,\\betaα,β의 prior이다. 이 경우의 장점은, 데이터가 모두 독립이지 않고 같은 성질을 갖는 놈들(같은 λ\\lambdaλ에서 나온 놈들에 해당)은 비슷하고 다른 성질은 갖는 놈들은 조금 다르다는, 약간의 correlation이 있는 데이터를 모델링할 수 있다. α,β\\alpha,\\betaα,β는 고정적으로 줘도 될 것인데, 왜 궂이 prior를 할당해서 샘플링하는가? 이건 α,β\\alpha,\\betaα,β에 대한 uncertainty(불확실성) 때문이다. α,β\\alpha,\\betaα,β는 독립적으로 샘플링된다. 그러나, 샘플링된 λ\\lambdaλ들 끼리는 독립이 아니다. 대신, α,β\\alpha,\\betaα,β가 주어진다면, 각 λ\\lambdaλ끼리는 해당 특정한 α,β\\alpha, \\betaα,β를 파라미터로 하는 분포에서 독립적으로 샘플링됬을 것이므로 조건부 독립이다. yyy끼리도 독립이 아니지만, λ\\lambdaλ가 주어진다면 yyy들 끼리 독립(조건부 독립)이다. λ\\lambdaλ가 주어졌다는 의미는 어느 한 그룹으로 좁혔고, 그 그룹 내에서 샘플들끼리는 독립이기 때문이다(그렇게 모델링 했으니까). 이렇게 함으로써, 다른 그룹은 다른 모델로 모델링하기 보다는 계층적인 하나의 모델로 모델링할 수 있다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"12. Prior Sensitivity Analysis","date":"2020-03-01T13:08:12.000Z","path":"studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/","text":"Prior Sensitivity Analysis Prior sensitivity analysis란, 하나의 파라미터에 대해 여러가지 prior 분포를 적용해보고 만든 여러 모델의 성능을 분석하는 과정을 말한다. 여러가지 prior를 적용해서 여러 모델을 만들었다고 하자. 그럼 여러 모델들이 내놓는 결과를 바탕으로 다음과 같이 해석할 수 있다. (Prior sensitivity analysis의 결과로 다음 두 가지의 경우가 나온다) 결과가 prior-sensitive하다. 어떤 prior를 선택하느냐에 따라 추정 성능 및 결과가 크게 달라지는 경우를 말한다. 즉, 데이터보다는 prior가 결과에 영향을 많이 미치는 경우이다. 이 경우, 내가 왜 이 prior를 선택해야 하고 이 모델을 선택해야 하는지 팀에게 설명할 필요가 있다. 결과가 prior-insensitive하다. 이 경우, prior보다는 데이터가 결과에 지대한 영향을 미치는 경우로, prior의 선택에 큰 힘을 쏟을 필요가 없음을 보일 수 있다. Prior sensitivity analysis는 내가 선택한 prior가 적절하다는 가정을 증명하기 위해서도 사용한다(즉, 가설검정에 사용할 수 있음). 내가 원하는 prior를 선택해서 모델을 구성하고, 내가 원하지 않는 prior를 선택해서 모델을 구성하게 된다. 이때, 내가 원하지 않는 prior를 skeptical prior라고 한다. 만약, skeptical prior로 시도해본 여러 모델들이 모두 내가 원한 prior 모델보다 일정 이상 성능이 좋지 않으면 나의 prior 선택을 증명 또는 설명할 수 있다. Prior sensitivity analysis를 할때, 해당 prior로 posterior estimation을 통해 성능을 측정하게 된다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"11. Linear Regression","date":"2020-03-01T13:08:11.000Z","path":"studynotes/bayesian-statistics/11_Linear_Regression/","text":"Linear Regression 선형 회귀라고도 불리며, 예측해야할 dependent variable이 continuous할때, 유용하다. 선형 회귀는 다음과 같다. yi∼N(μi,σ2)y_i \\sim N(\\mu_i, \\sigma^2) yi​∼N(μi​,σ2) μi=β0+β1x1+⋯+βkxk\\mu_i = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k μi​=β0​+β1​x1​+⋯+βk​xk​ 또는 다음과 같이 표현할 수 있다. yi=β0+β1x1+⋯+βkxk+ϵy_i = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_k x_k + \\epsilon yi​=β0​+β1​x1​+⋯+βk​xk​+ϵ ϵ∼N(0,σ2)\\epsilon \\sim N(0, \\sigma^2) ϵ∼N(0,σ2) 즉, 예측값 yiy_iyi​는 일직선 값에 오차값(residual) ϵ\\epsilonϵ을 더한 값이며, 이 residual값은 normal distribution을 따른다. 베이지안 통계에서는 β\\betaβ들은 분포를 갖는 random variable들이다. 따라서, 선형 회귀를 학습시킬때, 각 β\\betaβ에 대해 prior를 설정하고 posterior를 계산하게 된다. 각 β\\betaβ값에는 normal prior를 주는게 보통이지만, 다른 prior도 상관없다. 단, 어떤 variable xix_ixi​가 yiy_iyi​에 영향을 주는 놈인지 알고 싶다면, βi\\beta_iβi​의 prior로 Laplace prior를 설정하기도 한다. Laplace distribution은 double exponential 이라고도 불리며, 0점에서 뾰족한 모양이고 yyy축 대칭이다. 만약, 어떤 iii번째 βi\\beta_iβi​의 posterior 분포가 그냥 Laplace처럼 0점에 뾰족한 모양에 가깝다면, 그 βi\\beta_iβi​에 대응되는 xix_ixi​는 영향력이 거의 없다고 할 수 있다. 이런 방법을 Lasso 라고 부른다. JAGS 문법으로 표현하면 다음과 같다. 123456789101112131415161718# rjagsmod.string &lt;- \"model &#123; # likelihood for (i in 1:length(y)) &#123; y[i] ~ dnorm(mu[i], prec) mu[i] = b0 + b[1]*x1[i] + b[2]*x2[i] &#125; # prior # prec의 사후샘플들의 effective size가 5, variability가 2라고 기대하는 경우의 # inverse-gamma는 다음과 같다. prec ~ dgamma(5/2, 5*2/2) b0 ~ dnorm(0, 1e-6) for (i in 1:2) &#123; b[i] ~ dnorm(0, 1e-6) &#125;&#125;\" Poison Regression 선형 회귀의 일종으로, response variable인 yyy가 count 값인 경우에 사용되는 경우가 있다. poison 분포는 0보다 크거나 같은 값을 도메인으로 가지므로, yyy역시 0보다 같거나 큰 값이어야 한다. 반대로, yyy의 범위가 0이상이라면, Poison regression을 생각해 볼 수 있다. Poison regression은 likelihood가 poison distribution으로 모델링된 형태이다. 그런데, 이 경우, yyy가 0 이상 값이므로, 선형 회귀처럼 yi=β0+β1x1,iy_i = \\beta_0 + \\beta_1 x_{1,i}yi​=β0​+β1​x1,i​로 할 수 없다. 대신, yyy를 적절히 변형해서 [−∞,∞][-\\infty, \\infty][−∞,∞]범위로 만들어 준다면, 선형 함수로 적용이 가능할 것이다. 이때 사용하는 것이 log\\text{log}log함수이다. 즉, 다음과 같다. log yi=β0+β1x1,i+β2x2,i+⋯\\text{log}~y_i = \\beta_0 + \\beta_1x_{1,i} + \\beta_2 x_{2,i} + \\cdots log yi​=β0​+β1​x1,i​+β2​x2,i​+⋯ yi=exp(β0+β1x1,i+β2x2,i+⋯ )y_i = \\text{exp}(\\beta_0 + \\beta_1x_{1,i} + \\beta_2 x_{2,i} + \\cdots) yi​=exp(β0​+β1​x1,i​+β2​x2,i​+⋯) 위의 방법으로 모델링한다. Poison distribution의 파라미터 λ\\lambdaλ는 곧 분포의 기댓값이다. 즉, yi=λiy_i = \\lambda_iyi​=λi​로 생각하면 된다. JAGS 문법으로 표현하면 다음과 같다. 123456789101112131415# rjagsmod.string &lt;- \"model &#123; # likelihood for (i in 1:length(y)) &#123; y[i] ~ dpois(lambda[i]) log(lambda[i]) = b0 + b[1]*x1[i] + b[2]*x2[i] &#125; # prior b0 ~ dnorm(0, 1e-6) for (i in 1:2) &#123; b[i] ~ dnorm(0, 1e-6) &#125;&#125;\"","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"10. Markov Chain Monte Carlo","date":"2020-03-01T13:08:10.000Z","path":"studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/","text":"Markov Chain Monte Carlo MCMC라고도 불린다. 파라미터 θ\\thetaθ의 분포 p(θ)p(\\theta)p(θ)를 추정하고자 한다. 그러기 위해, bayesian inference를 하려고 하는데, 그러려면, 데이터를 수집한 후 posterior p(θ∣Y)p(\\theta|Y)p(θ∣Y)를 계산해야 한다. 그러나, 이 posterior를 계산하기에 상당히 어려울 수 있기 때문에(특히, normalization constant) 대신, posterior를 추정하기로 한다. 이 posterior를 추정할때 쓰일 수 있는 알고리즘 중 하나가 MCMC이다. Background 우리가 추정하고자 하는 파라미터 θ\\thetaθ의 분포인 p(θ)\\mathbb{p}(\\theta)p(θ)를 추정하고자 한다. 그래서 θ\\thetaθ를 파라미터로 하는 어떤 데이터 분포 y∣θy|\\thetay∣θ로부터 y1,...yky_1,...y_ky1​,...yk​를 샘플링했다. 이를 바탕으로 θ\\thetaθ의 prior p(θ)p(\\theta)p(θ)를 설정하고, 데이터로 posterior p(θ∣y1,...yk)p(\\theta|y_1,...y_k)p(θ∣y1​,...yk​)를 계산해서 p(θ)\\mathbb{p}(\\theta)p(θ)에 대한 베이지안 추론을 하고자 한다. 그런데, posterior인 p(θ∣y1,...,yk)p(\\theta|y_1,...,y_k)p(θ∣y1​,...,yk​)를 계산하는게 매우 어렵거나 불가능한 경우가 있다. 이때, posterior 분포 p(θ∣y1,...,yk)p(\\theta|y_1,...,y_k)p(θ∣y1​,...,yk​)를 추정하기 위한 방법으로 MCMC가 사용될 수 있다. MCMC MCMC는 posterior의 분포 p(θ∣y1,...yk)p(\\theta|y_1,...y_k)p(θ∣y1​,...yk​)를 추정하기 위해 마치 이 posterior로부터 샘플링됬을 법한 샘플 θ1∗,...,θm∗\\theta^* _1,...,\\theta^* _mθ1∗​,...,θm∗​을 생성해 준다. 이들은 posterior로부터 샘플링 되었을 거라고 가정하고 posterior를 monte carlo estimation으로 추정한다. MCMC의 알고리즘으로 여러 개가 있다고 하는데, 대표적으로 Metropolis-Hastings 알고리즘이 있다. Matropolis-Hastings Algorithms 알고리즘은 다음과 같다. 시작하기에 앞서 posterior p(θ∣y1,...,yk)p(\\theta|y_1,...,y_k)p(θ∣y1​,...,yk​)를 정확히 계산할 수는 없더라도, 이 posterior에 비례하는 어떤 함수는 알고 있어야 한다. 즉, 다음을 만족하는 g(θ)g(\\theta)g(θ)는 알고 있어야 한다. p(θ∣y1,...,yk)∝g(θ)p(\\theta|y_1,...,y_k) \\propto g(\\theta) p(θ∣y1​,...,yk​)∝g(θ) θ\\thetaθ와 도메인이 같거나 최대한 비슷한 분포 아무거나 고른다. 이 분포는 마르코프 체인을 만족하면 좋다. 즉, q(θ∗∣θi−1)q(\\theta^* |\\theta_{i-1})q(θ∗∣θi−1​). 적당히 큰 수 mmm번을 반복하는데, mmm개의 $\\theta^* $를 1개씩 샘플링할 것이다. $\\theta^* $를 q(θ∗∣θi−1)q(\\theta^* |\\theta_{i-1})q(θ∗∣θi−1​)로부터 1개를 샘플링한다. 다음을 계산한다. α=g(θ∗)q(θ∗∣θi−1)g(θi−1)q(θi−1∣θ∗)\\alpha = \\frac{g(\\theta^* )q(\\theta^* |\\theta_{i-1})}{g(\\theta_{i-1})q(\\theta_{i-1}|\\theta^* )} α=g(θi−1​)q(θi−1​∣θ∗)g(θ∗)q(θ∗∣θi−1​)​ α≥1\\alpha \\geq 1α≥1이면, $\\theta_i \\leftarrow \\theta^* $로 accept한다. 0≤α&lt;10 \\leq \\alpha &lt; 10≤α&lt;1이면, α\\alphaα의 확률로 $\\theta_i \\leftarrow \\theta^* $로 accept하고, reject되면 θi←θi−1\\theta_i \\leftarrow \\theta_{i-1}θi​←θi−1​한다. 분자에 g(θ∗)g(\\theta^* )g(θ∗)가 있고, 분모에 g(θi−1)g(\\theta_{i-1})g(θi−1​)가 있어서, 이전에 뽑은 θ\\thetaθ보다 현재 뽑은 θ\\thetaθ가 더 p(θ∣y1,...,yk)p(\\theta|y_1,...,y_k)p(θ∣y1​,...,yk​)에서 확률이 높다면, α≥1\\alpha \\geq 1α≥1이 되어서 accept된다. ggg가 ppp에 비례하기 때문에 그렇다. 이렇게 뽑은 θ∗\\theta^*θ∗는 초반 샘플링된 놈들을 제외하면, posterior p(θ∣y1,...,yk)p(\\theta|y_1,...,y_k)p(θ∣y1​,...,yk​)에서 샘플링된 것처럼 역할을 할 수 있다. 분포로부터 샘플링된 놈이 있으므로 posterior에 대해 monte carlo estimation이 가능해진다. Random Walk Algorithm Matropolis-hastings 알고리즘에서, proposal distribution q(θ∗∣θi−1)q(\\theta^* |\\theta_{i-1})q(θ∗∣θi−1​)를 θi−1\\theta_{i-1}θi−1​을 평균으로 하는 normal distribution으로 놓은 것을 말한다. Normal distribution은 대칭 분포이기 때문에, α=g(θ∗)g(θi−1)\\alpha=\\frac{g(\\theta^* )}{g(\\theta_{i-1})}α=g(θi−1​)g(θ∗)​이 된다. Gibbs Sampling 파라미터가 여러개라면, gibbs sampling이 Metropolis-hastings 알고리즘 보다 편할 수 있다. Metropolis-hastings 알고리즘에서는 파라미터 θ1,...,θk\\theta_1, ..., \\theta_kθ1​,...,θk​에 대해 proposal distribution을 각 파라미터마다 정의하고, accept, reject과정을 거칠 테지만, gibbs sampling과정에서는 이 과정을 없앴다. 대신 다음의 과정이 있다. 이때, parameter θ1,...,θk\\theta_1,...,\\theta_kθ1​,...,θk​를 모두 업데이트 1번씩 하는 과정을 1번의 iteration이라고 하자. 일단, p(θ1,...,θk∣y)∝g(θ1,...,θk)p(\\theta_1,...,\\theta_k|y) \\propto g(\\theta_1,...,\\theta_k)p(θ1​,...,θk​∣y)∝g(θ1​,...,θk​)를 만족하는 g(θ1,...,θk)g(\\theta_1, ..., \\theta_k)g(θ1​,...,θk​)를 알고 있어야 한다. p(θ1,...,θk∣y)∝p(y∣θ1,...,θk)p(θ1,...,θk)p(\\theta_1,...,\\theta_k|y) \\propto p(y|\\theta_1,...,\\theta_k)p(\\theta_1,...,\\theta_k)p(θ1​,...,θk​∣y)∝p(y∣θ1​,...,θk​)p(θ1​,...,θk​)를 활용. 하나의 parameter에 대한 full conditional distribution의 proportion을 계산해야 하는데, 다음과 같이 posterior 분포에 비례하므로(Bayes’ rule에 의해), ggg에 비례한다. p(θi∣θ1,...,θi−1,θi+1,...,θk,y)∝p(θ1,...,θk∣y)∝g(θ1,...,θk)p(\\theta_i|\\theta_1,...,\\theta_{i-1},\\theta_{i+1},...,\\theta_k,y) \\propto p(\\theta_1,...,\\theta_k|y) \\propto g(\\theta_1,...,\\theta_k) p(θi​∣θ1​,...,θi−1​,θi+1​,...,θk​,y)∝p(θ1​,...,θk​∣y)∝g(θ1​,...,θk​) 그리고, 나머지 파라미터는 모두 주어진 것으로 가정한다. 나머지 파라미터는 초기값이거나 가장 최근에 업데이트한 값으로 들어간다. 그렇게 되면, ggg에서 θi\\theta_iθi​에 의해 parameterize되지 않는 항은 모두 constant로 취급할 수 있으며, proportion에서 제외할 수 있다. 그럼 ggg가 간소화된다. 이렇게 되면, θi\\theta_iθi​에 대한 full conditional distribution이 우리가 아는 분포, 즉 샘플링이 가능한 분포가 되는 경우가 있다. 이럴 경우, 그냥 그 분포에서 샘플링하면 되기 때문에 accept, reject과정이 필요가 없다. 하나를 샘플링하고 θi\\theta_iθi​를 업데이트한다. 파라미터 θi+1\\theta_{i+1}θi+1​에 대해 같은 과정을 반복하는데, θ1,...i\\theta_{1,...i}θ1,...i​은 이전 iteration의 값이 아니라, 현재 iteration값을 이용한다. 만약, 4번 과정에서 샘플링이 가능한 표준적인 분포가 아니라면, 그 안에서, θi\\theta_iθi​ 하나에 대해서 matropolis-hastings 알고리즘의 방식을 사용해서, 하나의 샘플을 accept 혹은 reject로 업데이트한다. 업데이트 이전 값은 어디다가 저장해두자. 그 값들이 샘플들이다. Assessing Convergence of MCMC MCMC알고리즘에서 샘플링한 샘플들 θ1∗,...,θk∗\\theta^* _1, ..., \\theta^* _kθ1∗​,...,θk∗​의 평균값 θ∗ˉ\\bar{\\theta^* }θ∗ˉ이 θ\\thetaθ의 posterior 분포 p(θ∣Y)p(\\theta|Y)p(θ∣Y)를 잘 추정하려면, 마르코프 체인이 충분히 수렴해야 하고, 수렴한 체인으로부터 θ∗\\theta^*θ∗가 충분히 샘플링되어야 한다. 하지만, 마르코프 체인이 언제 수렴할지를 모르기 때문에 몇 개의 샘플까지가 수렴이 안된 상태의 샘플인지, 몇 개가 유용한 샘플인지 알 수가 없다. Stationary Distribution 마르코프 체인이 추정하고자 하는 target distribution(parameter의 posterior가 된다)을 최대한 추정한 distribution을 의미하며, 마르코프 체인이 충분히 수렴한 상태에서의 distribution을 의미한다. 당연히 알 수 없으며, 여기서 마르코프체인으로 샘플링만 가능하다. Monte Carlo Effective Sample Size 진짜 Stationary distribution으로부터 독립적으로 샘플링한 샘플을 θeff\\theta_{eff}θeff​이라고 하자. 즉, 이들은 실제로 posterior로부터 샘플링한 샘플과 매우 유사할 것이다. 우리가 마르코프 체인으로부터 샘플링한 샘플의 개수를 nnn이라고 하자. 하지만, 수렴이 제대로 되지 않은 상태에서 뽑은 것은 독립적인 샘플일 수가 없고, 마르코프 체인이기에, 완전히 독립적이기는 어렵다. 따라서, 유용한 샘플들은 일부일 것이다. 이 nnn개의 샘플이 가지고 있는 정보가 과연 몇 개의 θeff\\theta_{eff}θeff​들이 가지는 정보와 같은지를 나타내는게 monte carlo effective sample size이다. 즉, n=1000000n=1000000n=1000000개의 샘플을 뽑았는데, effective sample size neff=500n_{eff}=500neff​=500이라고 하면, 이 100만개의 샘플들은 실제로 posterior에서 500개를 샘플링한 것과 같은 정보를 가진다. 이는, 마르코프 체인이 posterior를 완전히 추정하지 못하기 때문이다. 또한, neffn_{eff}neff​이 너무 작다면, 수렴 속도가 느린 것일수도 있다. Auto-correlation 마르코프 체인에서 샘플링한 한 샘플이 과거 샘플들과 얼마나 많은 dependency가 있는가를 나타낸다. [-1,1] 범위의 값을 가지며, 0에 가까울수록 그 샘플은 과거 샘플들과 관계없는, 독립적인 샘플들이다. monte carlo sample size를 증가시키려면, 이 독립적인 샘플들이 필요하다. 마르코프 체인으로부터 샘플링을 하면, 초기 몇 개의 샘플까지는 수렴이 되지 않아서 correlation이 높다. 초기 correlation이 0에 가까운 값이 되는 지점까지의 샘플은 버리는 것도 방법(burn-in 이라고 부름). Auto-correlation이 0과 가까운 값이 적으면 effective sample size가 감소한다. Gelman-Rubin Diagnostic 마르코프 체인으로부터 샘플링한 샘플들을 주면, 실수값을 반환하는데, 1에 가가우면 수렴이 된 것이고, 1보다 많이 크면, 수렴이 아직 안된 것이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"09. Monte Carlo Estimation","date":"2020-03-01T13:08:09.000Z","path":"studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/","text":"Monte Carlo Estimation 쉽게 말하면, 몬테 카를로 추정법이란, 어떤 특정한 파라미터를 얻기 위해서, 파라미터의 distribution으로부터 많은 샘플링 시뮬레이션을 한 후, 그 샘플들의 평균을 계산한 것을 파라미터의 기댓값으로 추정하는 것이다. 예를 들어, 어떤 파라미터 θ\\thetaθ는 어떤 분포 p(θ)p(\\theta)p(θ)를 따른다. 우리는 파라미터 θ\\thetaθ의 기댓값 E[θ]E[\\theta]E[θ]를 계산하고 싶다. 이를 계산하기 위해서는 원래 E[θ]=∫p(θ)⋅θ dθE[\\theta] = \\int p(\\theta) \\cdot \\theta ~~ d\\thetaE[θ]=∫p(θ)⋅θ dθ를 계산해야 한다. 하지만, 이 계산은 불가능하거나 매우 힘들 수 있다. E[θ]E[\\theta]E[θ]를 계산하는 대신 추정하는 방법으로 몬테 카를로 추정법을 이용한다. 우선, 컴퓨터로 p(θ)p(\\theta)p(θ)로부터 θ\\thetaθ를 많이 샘플링한다. 그리고 그들의 평균 θˉ=1m∑imθi\\bar{\\theta} = \\frac{1}{m}\\sum_i^m \\theta_iθˉ=m1​∑im​θi​를 계산하고 θˉ\\bar{\\theta}θˉ를 E[θ]E[\\theta]E[θ]로 추정하는 것이다. 분포 p(θ)p(\\theta)p(θ)로부터 높은 확률의 θ\\thetaθ가 많이 샘플링되고 낮은 확률의 θ\\thetaθ는 적게 샘플링 되었을 것이다. 따라서 이 추정법은 유효할 수 있다. 다른 방식으로 해석하면, central limit theorem에 의해 샘플평균 θˉ\\bar{\\theta}θˉ는 실제 평균인 E[θ]E[\\theta]E[θ]를 평균으로 하고 1mVar[θ]\\frac{1}{m}Var[\\theta]m1​Var[θ]를 분산으로 하는 normal distribution을 따른다. 특히, 샘플수가 많아질수록, 계산한 샘플평균은 실제 평균값과 매우 유사할 확률이 높다. h(θ)h(\\theta)h(θ)의 기댓값 E[h(θ)]E[h(\\theta)]E[h(θ)]를 추정하고 싶다. 그러면, θ\\thetaθ를 많이 샘플링해서 각 샘플로 h(θ)h(\\theta)h(θ)를 계산하고 평균을 내면 E[h(θ)]E[h(\\theta)]E[h(θ)]의 추정값이 된다. Monte Carlo Error CLT(Central Limit Theorem)에 의해 파라미터 θ\\thetaθ에 대해 모은 샘플들은 N(E[θ],Var[θ]m)\\mathbb{N}(E[\\theta],\\frac{Var[\\theta]}{m})N(E[θ],mVar[θ]​)를 따른다. Var[θ]Var[\\theta]Var[θ]는 θ\\thetaθ의 분산으로, 다음으로 대체한다. Var[θ]=1m∑i(θˉ−θi)2Var[\\theta] = \\frac{1}{m}\\sum_i (\\bar{\\theta} - \\theta_i)^2 Var[θ]=m1​i∑​(θˉ−θi​)2 그리고, Var[θ]m\\frac{Var[\\theta]}{m}mVar[θ]​값을 monte carlo error라고 한다. Monte carlo estimation 값(E[θ]E[\\theta]E[θ]의 추정값인 θˉ\\bar{\\theta}θˉ)이 진짜 E[θ]E[\\theta]E[θ]로부터 어느정도로 오차가 있을지에 대한 term이라고 볼 수 있다. Monte Carlo Marginalization Paremter가 hierarchical하게 연결된 경우도 있다. 예를들어, 데이터 YYY는 베르누이 분포 Bern(ϕ)\\text{Bern}(\\phi)Bern(ϕ)를 따르는데, 이 ϕ\\phiϕ가 또 베타분포 Beta(2,2)\\text{Beta}(2, 2)Beta(2,2)를 따른다고 하자. 데이터 YYY의 기댓값 E[Y]E[Y]E[Y]를 몬테 카를로 추정법으로 추정하기 위해서는 다음의 과정이 필요하다. Beta(2,2)\\text{Beta}(2, 2)Beta(2,2)로부터 ϕ\\phiϕ를 샘플링한다. 샘플링한 ϕ\\phiϕ를 가지고 Y∣ϕY|\\phiY∣ϕ를 샘플링한다. 이제, (Y,ϕY,\\phiY,ϕ)한 쌍이 생성되었다. 반복한다. 이 과정의 특징이, 샘플 (Y,ϕY,\\phiY,ϕ)가 자연스럽게 P(Y,ϕ)P(Y,\\phi)P(Y,ϕ)의 joint distribution을 반영한다는 것이다. 그런데, 위에서 샘플링한 ϕ\\phiϕ를 그냥 무시하고 YYY만 취하면 그게 ϕ\\phiϕ에 대해 marginalization한 것과 같다. 즉, prior predictive distribution을 취한 것이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"08. Bayesian Modeling","date":"2020-03-01T13:08:08.000Z","path":"studynotes/bayesian-statistics/08_Bayesian_Modeling/","text":"Bayesian Modeling Statistical Modeling Bayesian modeling은 statistical modeling의 일종이다. Statistical modeling이란, 데이터가 생성/샘플링되는 프로세스를 모델링하는 것을 의미한다. Bayesian modeling은 이러한 모델링을 할때, 베이지안 방법론을 적용한 것을 말한다. 전체적인 모델링 프로세스는 Bayesian modeling이나 Frequentist modeling이나 같다. 문제 이해 데이터 수집 데이터 관찰 모델 구성 모델의 구현 및 fit 샘플공간 분포 파라미터 추정 테스트 및 예측성능 검사 5~7번 반복 모델의 이용. 이때, Frequentist modeling과 Bayesian modeling에서의 차이는 모델의 구현과 fit, 파라미터 추정에 있다. Model Specification - 모델 구성 모델의 구성은 계층적으로 적어 내려가면서 파악하는게 어느정도 쉽다. 일단, likelihood를 적고 likelihood에 영향을 미치는 random variable 또는 parameter를 찾는다. 어느 학교의 학생들의 키(height)의 분포를 예로 들자. 키의 분포는 normal distribution을 따른다고 가정하고 likelihood를 만든다. f(y∣θ)=N(μ,σ2)f(y|\\theta) = \\mathbb{N}(\\mu, \\sigma^2) f(y∣θ)=N(μ,σ2) 그리고 μ\\muμ, σ2\\sigma^2σ2의 분포가 필요한데, 이들의 prior를 정한다. μ≈N(μ0,σ02)\\mu \\approx \\mathbb{N}(\\mu_0, \\sigma_0^2) μ≈N(μ0​,σ02​) σ2≈IG(ν0,β0)\\sigma^2 \\approx \\mathbb{IG}(\\nu_0, \\beta_0) σ2≈IG(ν0​,β0​) 여기서 IG\\mathbb{IG}IG는 inverse-gamma distribution을 뜻한다. 그리고 각 prior는 독립이라고 가정하면, p(μ,σ2)=p(μ)p(σ2)p(\\mu,\\sigma^2) = p(\\mu)p(\\sigma^2)p(μ,σ2)=p(μ)p(σ2)일 것이다. 모델로 그려보면 다음과 같다. 이렇게, 우선 데이터가 어떻게 생성되었을지에 대해 그 생성 과정을 모델링하는데, likelihood부터 적고, 아래 파라미터까지 노드를 뻗어 나간다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"07. Priors","date":"2020-03-01T12:08:07.000Z","path":"studynotes/bayesian-statistics/07_Priors/","text":"Priors Prior를 어떤 분포로 선택할지에 대해서는 conjugatation을 고려해야 한다. prior으로 선택한 distribution이 likelihood와 곱해져서 posterior가 되었을 때도 그 distribution이 되어야 한다는 의미이다. Effective Sample Size Effective sample size란, 설정한 prior의 영향이 posterior에 영향을 50%만 미치는 순간의 샘플 개수를 말한다. 샘플 개수가 적으면 prior는 posterior에 영향을 많이 미칠 테지만, effective sample size이상의 샘플을 모은다면, prior가 posterior에 미치는 영향이 50% 이내일 것이다. Priors in Binomial Likelihood 예를들어, binomial distribution으로 likelihood를 모델링하는 경우, 즉, 제품 생산과정에서 불량의 빈도수 같은 경우는 prior를 beta distribution으로 불량일 확률 θ\\thetaθ를 모델링한다. 이때, beta distribution의 parameter α\\alphaα와 β\\betaβ를 어떻게 정해야 할까, beta distribution의 평균은 αα+β\\frac{\\alpha}{\\alpha+\\beta}α+βα​라는 것을 기억한다. 만약, 우리가 0.3%확률로 불량이 나올 것이라고 믿는다면, αα+β=0.3\\frac{\\alpha}{\\alpha + \\beta}=0.3α+βα​=0.3이 되게끔 정하면 된다. 다만, 이러면 경우의 수가 많은데, α\\alphaα와 β\\betaβ값이 커지면 그 믿음에 자신감이 있는 것이다. 30%의 확률로 불량이 있다고 생각해서 불량률 θ\\thetaθ에 대해 θ≈beta(6,14)\\theta \\approx \\text{beta}(6, 14)θ≈beta(6,14)으로 prior를 설정했다고 하자. 그리고 10번의 생산 후 6개의 불량이 나왔다. 이때, θ\\thetaθ의 posterior는 beta(6+6,14+4)=beta(12,18)\\text{beta}(6+6, 14+4) = \\text{beta}(12, 18)beta(6+6,14+4)=beta(12,18)이 된다. α\\alphaα는 불량인 것의 개수와 관련있고, β\\betaβ는 불량이 아닌 것과 관련이 있는 것이다. 실제로 계산해봐도 그렇다. P(θ)=Γ(6+20)Γ(6)Γ(14)θ6−1(1−θ)14−1P(\\theta) = \\frac{\\Gamma(6 + 20)}{\\Gamma(6)\\Gamma(14)}\\theta^{6-1}(1-\\theta)^{14 - 1} P(θ)=Γ(6)Γ(14)Γ(6+20)​θ6−1(1−θ)14−1 P(X∣θ)=(106)θ6(1−θ)4P(X|\\theta) = \\begin{pmatrix} 10 \\\\ 6 \\end{pmatrix}\\theta^6(1-\\theta)^4 P(X∣θ)=(106​)θ6(1−θ)4 P(θ∣X)=P(X∣θ)P(θ)=25!10!5!19!6!4!θ12−1(1−θ)18−1P(\\theta|X) = P(X|\\theta)P(\\theta) = \\frac{25!10!}{5!19!6!4!}\\theta^{12-1}(1-\\theta)^{18-1} P(θ∣X)=P(X∣θ)P(θ)=5!19!6!4!25!10!​θ12−1(1−θ)18−1 P(θ∣X)∝beta(12,18)P(\\theta|X) \\propto \\text{beta}(12, 18) P(θ∣X)∝beta(12,18) 앞의 상수들은 다 상수일 뿐. 어쨌든 beta distribution에 근사된다. Effective Sample Size Binomial likelihood에서 beta distribution을 θ\\thetaθ의 prior로 했을 경우, effective sample size는 α+β\\alpha+\\betaα+β가 된다. Posterior는 다음과 같다. Posterior(θ∣X)=Beta(α+∑inxi,β+n−∑inxi)\\text{Posterior}(\\theta|X) = \\text{Beta}(\\alpha+\\sum_i^n x_i, \\beta + n - \\sum_i^n x_i)\\\\ Posterior(θ∣X)=Beta(α+i∑n​xi​,β+n−i∑n​xi​) 이때, posterior mean은 α+∑inxiα+β+n\\frac{\\alpha + \\sum_i^n x_i}{\\alpha + \\beta + n}α+β+nα+∑in​xi​​인데, 이를 더 decompose해보면, 다음 식이 나온다. α+∑inxiα+β+n=α+βα+β+n⋅αα+β+nα+β+n⋅∑inxin\\frac{\\alpha + \\sum_i^n x_i}{\\alpha + \\beta + n} = \\frac{\\alpha + \\beta}{\\alpha + \\beta + n} \\cdot \\frac{\\alpha}{\\alpha + \\beta} + \\frac{n}{\\alpha + \\beta + n} \\cdot \\frac{\\sum_i^n x_i}{n} α+β+nα+∑in​xi​​=α+β+nα+β​⋅α+βα​+α+β+nn​⋅n∑in​xi​​ 이것은 prior mean과 data mean과의 weighted sum으로 해석할 수 있다. 즉, posterior mean은 prior mean과 data mean에 의해 영향을 받는다. 그런데, 이때, 샘플 개수 nnn이 작으면, prior의 영향력이 커진다. 반면, nnn이 커지면, data의 영향력이 커진다. n≥α+βn \\geq \\alpha+\\betan≥α+β 일때, prior보다 데이터의 영향력이 커진다. 따라서, effective sample size는 α+β\\alpha + \\betaα+β이다. Prior를 정의할때, α,β\\alpha, \\betaα,β를 크게 잡던, 작게 잡던, α\\alphaα와 α+β\\alpha+\\betaα+β의 비율이 같으면, prior mean은 같지만, 값들이 크면, prior의 영향력이 강해지기 때문에 sample 개수를 많이 모아야 한다. Priors in Poisson Distribution Poisson distribution을 likelihood로 취하는 experiment에 대해서는 parameter가 λ\\lambdaλ가 된다. 즉, λ\\lambdaλ에 대한 prior가 필요한데, 이때는 Gamma distribution으로 λ\\lambdaλ의 prior를 모델링한다. Poisson distribution으로 likelihood를 모델링 할 수 있는 경우, Gamma distribution를 conjugate prior로 가지는 distribution이다. 이때, Gamma distribution의 두 파라미터 α\\alphaα와 β\\betaβ를 정할때, gamma distribution의 평균은 αβ\\frac{\\alpha}{\\beta}βα​인 것을 생각하자. α\\alphaα는 event 발생 횟수, β\\betaβ는 총 시행 횟수와 관련이 있다. 이때, poisson이므로, 1번의 시행에서 event가 여러번 발생할 수 있다. 특정 시간 안에 몇 번의 버스가 오는가? Effective Sample Size Beta distribution을 prior로 삼고, posterior도 역시 beta distribution이기 때문에, effective sample size는 α+β\\alpha+\\betaα+β이다. Priors in Exponential Distribution Exponential distribution도 역시 λ\\lambdaλ를 파라미터로 하며, gamma distribution를 conjugate로 가진다. Effective Sample Size Gamma prior로 conjugate인 likelihood의 경우, posterior도 gamma distribution이다. 이런 경우, effective sample size는 β\\betaβ이다. Posterior(λ∣X)=Gamma(α+∑inxi,β+n)\\text{Posterior}(\\lambda|X) = \\text{Gamma}(\\alpha + \\sum_i^n x_i, \\beta + n) Posterior(λ∣X)=Gamma(α+i∑n​xi​,β+n) mean(posterior)=α+∑inxiβ+n\\text{mean}(posterior) = \\frac{\\alpha + \\sum_i^n x_i}{\\beta + n} mean(posterior)=β+nα+∑in​xi​​ α+∑inxiβ+n=ββ+n⋅αβ+nβ+n⋅∑inxin\\frac{\\alpha + \\sum_i^n x_i}{\\beta + n} = \\frac{\\beta}{\\beta + n} \\cdot \\frac{\\alpha}{\\beta} + \\frac{n}{\\beta + n} \\cdot \\frac{\\sum_i^n x_i}{n} β+nα+∑in​xi​​=β+nβ​⋅βα​+β+nn​⋅n∑in​xi​​ Priors in Normal Distribution Normal distribution의 파라미터 μ\\muμ는 σ\\sigmaσ에 의존함과 동시에 normal distribution prior와 conjugate관계이다. σ\\sigmaσ는 주어젔다고 가정하는 경우가 많으며, 그렇지 않을 경우, inverse-gamma distribution와 conjugate관계이다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"06. Prior Predictive Distribution","date":"2020-03-01T12:08:06.000Z","path":"studynotes/bayesian-statistics/06_Prior_Posterior_predictive/","text":"Prior Predictive Distribution Prior정보만으로, 즉, 데이터 없이 데이터가 어떻게 분포할지,즉 p(D)p(D)p(D)를 추정해본 분포이다. 데이터 분포 p(D)p(D)p(D)는 다음과 같이 쓸 수 있다. p(d)=∫01p(d∣θ)p(θ) dθp(d) = \\int_0^1 p(d|\\theta)p(\\theta) ~d\\theta p(d)=∫01​p(d∣θ)p(θ) dθ 이때, 데이터 없이 사전 정보만으로 p(D)p(D)p(D)를 추정하는데, 사전정보로 추정한 데이터 분포 p(D)p(D)p(D)를 추정한 것을 prior predictive distribution이라고 부른다. Prior predictive distribution은 데이터 수집 전에, prior정보만을 이용해서 데이터 sample space distribution을 추정한 것이라고 할 수 있다. Posterior Predictive Distribution 데이터를 수집한 후, 데이터의 분포를 추정한 분포를 말한다. 데이터 d1d_1d1​를 수집했다고 치자. 그럼 다음에 샘플링될 d2d_2d2​의 확률분포는 다음과 같다. p(d2∣d1)=∫01p(d2∣d1,θ)p(θ∣d1)dθp(d_2|d_1) = \\int_0^1 p(d_2|d_1,\\theta)p(\\theta|d_1)d\\theta p(d2​∣d1​)=∫01​p(d2​∣d1​,θ)p(θ∣d1​)dθ 이때, d1⊥d2d_1 \\perp d_2d1​⊥d2​이므로, 다음과 같다. p(d2∣d1)=∫01p(d2∣θ)p(θ∣d1)dθp(d_2|d_1) = \\int_0^1 p(d_2|\\theta)p(\\theta|d_1)d\\theta p(d2​∣d1​)=∫01​p(d2​∣θ)p(θ∣d1​)dθ Prior predictive distribution과 다른 점은 prior 자리에 posterior가 들어갔다는 점이다. Posterior predictive distribution은 데이터를 관찰한 후, 그 정보를 이용해서 데이터 sample space 분포를 추정한 것이라고 할 수 있다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"05. Credible Intervals","date":"2020-03-01T12:08:05.000Z","path":"studynotes/bayesian-statistics/05-Credible-Intervals/","text":"Credible Intervals Prior로 θ\\thetaθ의 distribution p(θ)p(\\theta)p(θ)를 정한 후, 데이터를 이용해서 bayesian inference과정을 거쳤다고 하자. 따라서, p(θ∣D)p(\\theta|D)p(θ∣D)를 구했다. 그리고, 이 posterior속에서 파라미터 θ\\thetaθ가 어디쯤에 위치할지, credible interval을 계산할 수 있다. 이는 frequentist statistics에서의 confidence interval과 매우 유사하지만, 다음의 차이점이 있다. Confidence interval은 θ\\thetaθ는 고정되어 있고 bound 경계가 random variable이다. 반면, credible interval에서는 θ\\thetaθ가 random variable이고 bound가 고정된 값이다. confidence interval은 이 구간 사이에 모 파라미터 θ\\thetaθ가 있을 것이라는 자신감이 있을 뿐, θ\\thetaθ가 그 구간에 위치할 확률이 p-value가 되는 것이 아니다. Frequentist statistics에서는 θ\\thetaθ는 고정되어 있고 변하지 않는다. 반면, credible interval은 그 구간 내에 θ\\thetaθ가 있을 확률을 의미한다. Posterior를 충분한 데이터로 구했다면, 사전 지식과 합쳐서 credible interval을 계산하고 θ\\thetaθ가 어느 범위에 있을 확률을 구하는 것이다. Equal-Tailed Interval 95%의 credible interval을 구하고 싶다면, 한쪽 끝에서 2.5%의 bound를 계산하고 다른 한 쪽 끝에서 2.5%의 bound를 계산한다. 그리고 그 사이가 equal tailed interval이 된다. Highest Posterior Density 양쪽 끝을 같은 확률로 자르지말고, 확률이 높은 구간을 최대한 포함하자는 것이다. 만약, p(θ∣D)=2θp(\\theta|D) = 2\\thetap(θ∣D)=2θ의 경우, 오른쪽 꼬리는 매우 확률이 높은 구간인데, 자르기 아깝다는 것이다. 따라서 확률이 낮은 왼쪽 꼬리만 잘라서 interval을 구한다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"04. Bayesian Inference","date":"2020-03-01T12:08:04.000Z","path":"studynotes/bayesian-statistics/04-Bayesian-Inference/","text":"Bayesian Inference 우리가 추정하고자 하는 parameter θ\\thetaθ에 대해 θ\\thetaθ가 어떻게 분포되어있는지 사전 지식 또는 정보가 있다면 그것을 이용하는게 좋을 것이다. 하지만, frequentist inference에서는 사전 정보를 이용하기 어렵다. 사전 정보를 이용해서 p(θ)p(\\theta)p(θ)를 초기화한후(prior), 데이터를 수집하면서 얻은 정보(posterior)를 이용해서 p(θ)p(\\theta)p(θ)분포를 p(θ∣D)p(\\theta|D)p(θ∣D)로 업데이트한다. 이렇게 p(θ)p(\\theta)p(θ)을 추정해 가는 방식을 Bayesian inference라고 한다. 그리고, 얻은 데이터를 바탕으로 p(θ∣D)p(\\theta|D)p(θ∣D)를 최대화하는 θ^\\hat{\\theta}θ^를 선택하는 것을 Maximize A Posterior(MAP) 추정이라고 한다. 즉, 다음과 같다. θ^=argmaxθ p(θ∣D)\\hat{\\theta} = argmax_{\\theta} ~p(\\theta|D) θ^=argmaxθ​ p(θ∣D) 개와 고양이를 판별하는 classifier를 만든다고 치자. 역시 θ∈{개,고양이}\\theta \\in \\{개, 고양이\\}θ∈{개,고양이}이고, 사진을 주고 bayesian inference로 개인지, 고양이인지 판단을 한다고 하면, 사진처럼 생겼을 경우 개일 확률과 사진처럼 생겼을 경우 고양이일 확률을 비교한다. 사진처럼 생겼을때, 고양이일 확률이 개일 확률보다 높으면 고양이라고 추정하는 방식이 MAP이다. 그런데, p(θ∣D)p(\\theta|D)p(θ∣D)는 다음과 같이 계산한다. p(θ∣D)=p(D∣θ)p(θ)∑ip(D∣θi)p(θi)p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{\\sum_ip(D|\\theta_i)p(\\theta_i)} p(θ∣D)=∑i​p(D∣θi​)p(θi​)p(D∣θ)p(θ)​ 즉, 사후확률 p(θ∣D)p(\\theta|D)p(θ∣D)는 관찰된 데이터의 likelihood p(D∣θ)p(D|\\theta)p(D∣θ)와 사전확률 p(θ)p(\\theta)p(θ)을 이용해서 계산된다. 그리고 계산된 사후확률 p(θ∣D)p(\\theta|D)p(θ∣D)를 이용해서 p(θ)p(\\theta)p(θ)를 업데이트한다(단순 대입, p(θ)←p(θ∣D)p(\\theta) \\leftarrow p(\\theta|D)p(θ)←p(θ∣D)). 이렇게 데이터를 모은 정보를 바탕으로 prior를 posterior로 업데이트 해 가면서 θ\\thetaθ에 대한 분포 p(θ)p(\\theta)p(θ)를 추정해 나가는 방식을 bayesian inference라고 한다. 주의할 점은 prior p(θ)p(\\theta)p(θ)를 어느 특정 지점에서 0 또는 1로 설정하면, posterior에서도 그 지점은 0 또는 1이 된다. 따라서 왠만하면 0 또는 1을 어떤 지점에 할당하지 않도록 한다. p(θ∣D)∝p(D∣θ)p(θ)=p(θ)p(\\theta|D) \\propto p(D|\\theta)p(\\theta) = p(\\theta) p(θ∣D)∝p(D∣θ)p(θ)=p(θ)","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"03. Frequentist Inference","date":"2020-03-01T12:08:02.000Z","path":"studynotes/bayesian-statistics/03-Frequentist-Inference/","text":"Frequentist Inference Frequentist statistics에서는 sample space의 분포 파라미터 θ\\thetaθ를 추정할 때, 다음과 같은 방법으로 추론할 수 있다. 일단 데이터를 많이 모은다. X1=d1,X2=d2,...,Xn=dnX_1 = d_1, X_2 = d_2, ..., X_n=d_nX1​=d1​,X2​=d2​,...,Xn​=dn​ Central limit theorem을 이용해서 데이터의 평균치 또는 합을 계산한다. Xˉ=1n∑iXi\\bar{X} = \\frac{1}{n}\\sum\\limits_i X_iXˉ=n1​i∑​Xi​ 이 평균치는 θ\\thetaθ에 대한 함수일 것이고(애초에 XiX_iXi​가 θ\\thetaθ에 대한 함수임) 이 평균치는 N(Xˉ,σn)\\mathbb{N}(\\bar{X}, \\frac{\\sigma}{\\sqrt{n}})N(Xˉ,n​σ​)의 분포를 이룬다. (합의 경우는 N(nXˉ,σ)\\mathbb{N}(n\\bar{X},\\sigma)N(nXˉ,σ)) 이 분포에 대해 confidence interval을 계산하고, Xˉ\\bar{X}Xˉ주위 그 interval 안에 해당 confidence (p−valuep-valuep−value) 의 자신감으로, 진짜 μ\\muμ가 있다고 가정한다. 주의할 점은, p−valuep-valuep−value는 μ\\muμ가 그 confidence의 확률로 interval안에 있다는 것이 아니다. μ\\muμ는 고정되어 있는 값이라서 그 interval 안에 있을 확률은 0 아니면 1이다. 다만, μ\\muμ가 거기에 있을 것이라는 95%(p−value=95p-value=95p−value=95)의 자신감이 있을 뿐이다. Confidence Interval 동전던지기 시행에서 앞면이 나올 확률 ppp를 알고 싶다. 100번 던져본다. 각 시행은 XiX_iXi​이다. 이때, 100번의 시행을 모두 더한 random variable Y=∑iXiY=\\sum\\limits_i X_iY=i∑​Xi​를 정의한다. 그럼 YYY는 다음의 분포를 따른다. Y≈N(100p,100p(1−p))Y \\approx \\mathbb{N}(100p, 100p(1-p)) Y≈N(100p,100p(1−p)) Y=1n∑iXiY = \\frac{1}{n} \\sum\\limits_i X_iY=n1​i∑​Xi​라고 정의했다면, Y≈N(p,p(1−p)n)Y \\approx \\mathbb{N}(p, \\frac{p(1-p)}{\\sqrt{n}})Y≈N(p,n​p(1−p)​)가 되겠다. 어쨌든, 55번의 H, 45번의 T이 나왔다면, frequentist statistics의 확률 정의에 의해 p^=0.55\\hat{p}=0.55p^​=0.55이고 이 추정치는 95%, 97%, 99% confidence interval로 어느정도 true ppp에 가깝다고 확신을 내릴 수 있다. 95%를 예로 들면, 55−1.96∗100∗0.55∗0.45≤100p≤55+1.96∗100∗0.55∗0.4555 - 1.96 * 100 * 0.55 * 0.45 \\leq 100p \\leq 55 + 1.96 * 100 * 0.55 * 0.45 55−1.96∗100∗0.55∗0.45≤100p≤55+1.96∗100∗0.55∗0.45 로 100p100p100p에 대한 confidence interval을 계산할 수 있다. Maximum Likelihood Estimation 데이터를 확률분포 p(D∣θ)p(\\mathbb{D}|\\theta)p(D∣θ)로부터 샘플링했을 때, 가지고 있는 데이터가 샘플링 됬을 확률을 p(D∣θ)p(D|\\theta)p(D∣θ)라고 표현한다면, 이를 liklihood라고 한다. 이 likelihood를 최대화하는 파라미터 θ\\thetaθ를 찾으면, 즉, likelihood를 최대화하는 분포를 구하면, 그것이 sample space분포인 p(D∣θ)p(\\mathbb{D}|\\theta)p(D∣θ)와 매우 유사할 것이라는 것이라고 가정한다. 따라서 likelihood를 최대화하는 파라미터 θ\\thetaθ를 찾고, 나아가 sample space distribution을 추정하는 방법을 MLE(Maximum likelihood estimation)라고 부른다. Likelihood를 최대화하는 θ^\\hat{\\theta}θ^를 구하는데 이용하는 방법은 미분하고 derivatives를 0으로 하는 θ\\thetaθ를 구하는 것이다. 즉, 극점을 구하는 것이다. 예를 들어, 동전이 fair한지, loaded인지 구하려고 한다. 만약, fair한 동전이라면 앞 뒷면이 나올 확률은 0.5로 같다. loaded 동전이라면 앞면이 나올 확률은 0.7, 뒷면이 나올 확률은 0.3이라고 하자. 동전을 다섯 번 던져서 5개의 데이터를 얻었다. 이때, 2번은 앞면, 3번은 뒷면이 나왔다. 이때, liklihood는 동전이 fair일때와, loaded일때에 대해서 각각 구할 수 있다. p(D∣θ)={(52)∗0.55if θ is fair(52)∗0.72∗0.32if θ is loadedp(D|\\theta) = \\begin{cases} \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix} * 0.5^5 &amp; \\text{if } \\theta \\text{ is fair} \\\\ \\begin{pmatrix} 5 \\\\ 2 \\end{pmatrix} * 0.7^2 * 0.3^2 &amp; \\text{if } \\theta \\text{ is loaded} \\end{cases} p(D∣θ)=⎩⎪⎪⎨⎪⎪⎧​(52​)∗0.55(52​)∗0.72∗0.32​if θ is fairif θ is loaded​ 결과를 구해보면, θ\\thetaθ가 fair일때의 p(D∣θ)p(D|\\theta)p(D∣θ)가 더 높다는 것을 알 수 있다. 즉, θ\\thetaθ가 fair일때, likelihood가 더 높다. 따라서 MLE에 의해 likelihood가 최대화되는 θ=fair\\theta=\\text{fair}θ=fair 이라고 추정할 수 있다. 그런데, 동전은 물리적인 물체이므로 데이터가 주어졌을 때의 동전이 fair할 확률 p(θ=fair∣D)p(\\theta=\\text{fair}|D)p(θ=fair∣D)은 p(θ=fair)p(\\theta=\\text{fair})p(θ=fair)와 같다. 동전이 fair한지 안하는지는 변하지 않는 것이고 데이터셋과 상관없이 결정된 것이기 때문이다. 따라서 다음과 같다. p(θ=fair∣D)=p(θ=fair)∈{0,1}p(\\theta=\\text{fair}|D) = p(\\theta=\\text{fair}) \\in \\{0, 1\\} p(θ=fair∣D)=p(θ=fair)∈{0,1} 즉, frequentist inference는 다음과 같이 정리할 수 있다. θ^=argmaxθ p(D∣θ)\\hat{\\theta} = argmax_{\\theta} ~p(D|\\theta) θ^=argmaxθ​ p(D∣θ) 다른 예시로, 개와 고양이를 구분하는 classifier를 구현하고 싶다고 하자. MLE 방법에서는 θ∈{개,고양이}\\theta \\in \\{개, 고양이\\}θ∈{개,고양이}이고, 사진을 보여주고 frequentist inference를 한다고 하자. 만약, 개라면 사진처럼 생겼을 확률과 고양이라면 사진처럼 생겼을 확률을 비교하고, 개라면 사진처럼 생겼을 확률이 높으면 개라고 판단하고, 고양이라면 사진처럼 생겼을 확률이 높다면 고양이로 판단한다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"02. Distribution","date":"2020-03-01T12:08:01.000Z","path":"studynotes/bayesian-statistics/02_Distribution/","text":"Distribution Background Indicator Function 어떤 조건이 만족했을때, 1을 반환하고, 만족하지 못하면 0을 반환하는 함수이다. Icond(x)(x)={1if cond(x) is True0if cond(x) is False\\mathbb{I}_{\\text{cond}(x)}(x) = \\begin{cases} 1 &amp; \\text{if cond(x) is True} \\\\ 0 &amp; \\text{if cond(x) is False} \\end{cases} Icond(x)​(x)={10​if cond(x) is Trueif cond(x) is False​ Indicator function은 다음과 같이 동전 던지기에 대한 확률같은 것을 표현할 때 용이하다. P(X∣θ)=θ⋅Ihead+(1−θ)⋅ItailP(X|\\theta) = \\theta \\cdot \\mathbb{I}_{\\text{head}} + (1 - \\theta) \\cdot \\mathbb{I}_{\\text{tail}} P(X∣θ)=θ⋅Ihead​+(1−θ)⋅Itail​ Expected Values 기댓값이라고도 불리며, 확률적인 관점에서 본 **“평균”**이다. E[X]=∑xpxxE[X] = \\sum\\limits_{x} p_x x E[X]=x∑​px​x 각 샘플들에 비중치를 곱해서 모두 더한 것이다. 일반적으로 생각할 수 있는 평균값은 모든 샘플들에 같은 비중치를 둔 기댓값과 같다. Variance 분산이라고도 불리며, 샘플들이 평균 또는 기댓값으로부터 얼마나 떨어져서 분포하는가, 즉, 샘플들이 얼마나 넓게 퍼져있는가를 나타낸다. Var[X]=E[(X−μ)2]=∑xpx(x−μ)2\\text{Var}[X] = E[(X-\\mu)^2] = \\sum\\limits_{x} p_x(x - \\mu)^2 Var[X]=E[(X−μ)2]=x∑​px​(x−μ)2 평균과의 거리의 제곱을 평균한 것인데, 단위가 제곱이 된다. 따라서 단위를 일치시키기 위해 제곱근을 씌우는데, 이를 **standard deviation(표준편차)**라고 부른다. Std.[X]=σ(X)=Var[X]\\text{Std.}[X] = \\sigma(X) = \\sqrt{\\text{Var}[X]} Std.[X]=σ(X)=Var[X]​ Scaling Random Variable vs Many Random Variables Random variable을 scaling 한다는 것은, 분포를 넓게 피는것을 의미한다. random variable XXX를 ccc배 scaling하는 것은 cXcXcX로 표기한다. 반면, random variable XXX를 여러번 시행하는 것은 ∑inXi\\sum_i^n X_i∑in​Xi​로 표기한다. 둘이 분명히 다른데, cXcXcX의 경우, 1번 샘플링하는 것이고, ∑X\\sum X∑X는 여러번 샘플링 하는 것이다. 예를들어, XXX가 베르누이 분포를 따르고, 1일 확률이 0.7 이라면, 10X10X10X는 0일 확률이 0.3, 10일 확률이 0.7인 것이다. 반면, ∑i10Xi\\sum_i^{10} X_i∑i10​Xi​는 1이 나올 확률이 0.7인 분포에서 10번 샘플링하는 것이다. Discrete Distribution Bernoulli Distribution Sample space의 크기가 2(event 개수가 2개)라고 추정되는 random variable XXX가 있을때, 이 XXX는 Bernoulli distribution(베르누이 분포)을 따른다. 베르누이 분포를 따르는 random variable의 1회 experiment을 베르누이 시행이라고 한다. Bern(X∣θ)=θ∗IX=1+(1−θ)∗IX=0\\text{Bern}(X|\\theta) = \\theta*\\mathbb{I}_{X=1} + (1 - \\theta) * \\mathbb{I}_{X=0} Bern(X∣θ)=θ∗IX=1​+(1−θ)∗IX=0​ Expected value: E[X]=θE[X] = \\theta E[X]=θ Variance: σ2(X)=θ∗(1−θ)2+(1−θ)∗(0−θ)2=θ(1−θ)\\sigma^2(X) = \\theta * (1 - \\theta)^2 + (1-\\theta)*(0 - \\theta)^2 = \\theta(1-\\theta) σ2(X)=θ∗(1−θ)2+(1−θ)∗(0−θ)2=θ(1−θ) Binomial Distribution 베르누이 분포를 따르는 experiment를 여러번 시행했을 때, 한 결과가 몇 번이 나왔는가에 대한 분포이다. 흔히, 동전을 10번 던졌을때, 앞면이 몇 번 나오는지에 대한 분포라고 이해하면 편하다. 동전 던지기 1회는 베르누이 시행이다. Binom(n,x∣θ)=(nx)θx(1−θ)n−x\\text{Binom}(n, x|\\theta) = \\begin{pmatrix} n \\\\ x \\end{pmatrix} \\theta^x(1-\\theta)^{n-x} Binom(n,x∣θ)=(nx​)θx(1−θ)n−x Expected value: E[X]=nθE[X] = n\\theta E[X]=nθ Variance: σ2(X)=nθ(1−θ)\\sigma^2(X) = n\\theta(1-\\theta) σ2(X)=nθ(1−θ) Geometric Distribution 베르누이 시행을 여러번 반복하는데, 어떤 event가 최초로 일어날때 까지의 시행한 횟수는 geometric distribution을 따른다. (기하분포). θ\\thetaθ는 베르누이 시행 1회에서 그 event가 성공할 확률이다. Geom(x∣θ)=θ∗(1−θ)x−1\\text{Geom}(x|\\theta) = \\theta * (1-\\theta)^{x-1} Geom(x∣θ)=θ∗(1−θ)x−1 Expected value: E[X]=1θE[X] = \\frac{1}{\\theta} E[X]=θ1​ Multinomial Distribution 시행이 베르누이 시행이 아니라, 여러 개의 event가 나올 수 있는 시행일 때의 binomial distribution을 의미한다. Multinom(n,x1,...,xn∣θ1,θ2,...,θn)=(nx1,x2,...,xn)θ1x1∗θx2∗⋯∗θxn\\text{Multinom}(n, x_1,...,x_{n}|\\theta_1, \\theta_2, ..., \\theta_{n}) = \\begin{pmatrix} n \\\\ x_1, x_2, ..., x_n \\end{pmatrix}\\theta_1^{x_1}*\\theta^{x_2}* \\cdots*\\theta^{x_n} Multinom(n,x1​,...,xn​∣θ1​,θ2​,...,θn​)=(nx1​,x2​,...,xn​​)θ1x1​​∗θx2​∗⋯∗θxn​ Poisson Distribution 포아송 분포는, 어느 시간 간격 내에 그 event가 몇 번 일어날지에 대한 분포이다. 해당 시간 간격동안에 event가 발생하는 횟수를 λ\\lambdaλ라고 하면, 다음과 같다. Poisson(x∣λ)=λxe−λx!\\text{Poisson}(x|\\lambda) = \\frac{\\lambda^xe^{-\\lambda}}{x!} Poisson(x∣λ)=x!λxe−λ​ Expected value: E[X]=λE[X] = \\lambda E[X]=λ (애초에 λ\\lambdaλ 정의가 그냥 기댓값이다.) 여기서, 만약에 binomial distribution을 따르는데, 동전이 앞면이 나올 확률이 너무나도 희박하고, 동전 던지기 experiment를 무한번 한 경우, 그 무한번의 experiment를 일정 기간의 시간이라고 간주하게 되면 poisson distribution와 같다. Continuous Distribution Sample space가 continuous한 경우의 distribution을 말함. Exponential Distribution 특정 일이 일어날 때 까지 걸린 시간 또는 기다린 시간의 분포는 exponential distribution을 따른다. Exp(λ)=λe−λxIx≥0\\text{Exp}(\\lambda) = \\lambda e^{-\\lambda x} \\mathbb{I}_{ x \\geq 0 } Exp(λ)=λe−λxIx≥0​ 여기서 λ\\lambdaλ는 어떤 시간 동안에 사건이 발생하는 횟수의 비율을 말한다. 예를 들어, 10분 동안 버스가 3대 오면 λ=0.3\\lambda = 0.3λ=0.3이다(시간을 10분 단위로 했을 때). Gamma Distribution 버스가 올때까지 걸리는 시간을 측정하는 시행이 여러 번 있고, 그들의 총합 시간은 gamma distribution을 따른다. 쉽게 말해서, Gamma distribution을 따르는 YYY는 exponential distribution을 따르는 XiX_iXi​의 합과 같다. Y=∑iXiY = \\sum_i X_i Y=i∑​Xi​ p(y∣α,β)=βαΓ(α)yα−1e−βyIy≥0(y)p(y|\\alpha,\\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}y^{\\alpha-1}e^{-\\beta y} \\mathbb{I}_{y \\geq 0}(y) p(y∣α,β)=Γ(α)βα​yα−1e−βyIy≥0​(y) 감마분포는 α\\alphaα와 β\\betaβ를 파라미터로 삼으며, α=n\\alpha = nα=n, β=λ\\beta = \\lambdaβ=λ가 된다. α\\alphaα는 shape parameter로, α=1\\alpha=1α=1이면, exponential distribution이 된다. 또한, α\\alphaα가 0에 가까워질수록 right-skewed가 된다. α\\alphaα가 커질수록 normal distribution에 가까워지면서 skewness가 줄어든다(한쪽으로 치우치지 않는다). β=λ\\beta = \\lambdaβ=λ는 rate parameter로, θ=1β\\theta = \\frac{1}{\\beta}θ=β1​는 scale parameter이다. 서로 역수 관계이며, 감마 분포를 표기할때, (α,β)(\\alpha, \\beta)(α,β)로 parameterize하기도 하고 (k,θ)(k, \\theta)(k,θ)로 parameterize하기도 한다. α=k\\alpha=kα=k이지만, β=1θ\\beta = \\frac{1}{\\theta}β=θ1​이다. θ\\thetaθ는 scale parameter로, rate의 역수이다. Scale parameter는 분산의 scaling 정도이며, 클수록 넓게 퍼진다. 즉, rate가 작을수록 넓게 퍼지며, random variable XXX의 ccc배 scale인 cXcXcX는 (k,cθ)(k, c\\theta)(k,cθ)가 되는 셈. ∑inXi\\sum_i^{n} X_i∑in​Xi​는 (nk,θ)(nk, \\theta)(nk,θ) 효과를 얻는다! 이걸 보면 α\\alphaα가 exponential의 횟수와 관련이 있을지도 모른다. Expected Value: E[X]=αβE[X] = \\frac{\\alpha}{\\beta} E[X]=βα​ Variance: σ2(X)=αβ2\\sigma^2(X) = \\frac{\\alpha}{\\beta^2} σ2(X)=β2α​ Uniform Distribution 모든 sample space범위의 단위 interval에서 확률이 같다. Uni(X)=1b−aIa≤x≤b\\text{Uni}(X) = \\frac{1}{b-a}\\mathbb{I}_{ a \\leq x \\leq b } Uni(X)=b−a1​Ia≤x≤b​ Expected Value: E[X]=a+b2E[X] = \\frac{a+b}{2} E[X]=2a+b​ (a~b까지 나올 확률이 같으므로 샘플링 여러번 하다 보면 평균값은 중앙값인 a+b2\\frac{a+b}{2}2a+b​이 된다.) Variance: σ2(X)=(b−a)212\\sigma^2(X) = \\frac{(b-a)^2}{12} σ2(X)=12(b−a)2​ Beta Distribution Sample space가 0과 1 사이인 분포. 따라서 확률을 모델링할때 이용하기도 한다. Beta(α,β)=Γ(α+β)Γ(α)Γ(β)xα−1(1−x)β−1\\text{Beta}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} Beta(α,β)=Γ(α)Γ(β)Γ(α+β)​xα−1(1−x)β−1 여기서, Γ(x)=(x−1)!\\Gamma(x) = (x-1)!Γ(x)=(x−1)!이다. 앞의 Γ\\GammaΓ term들을 풀어보면 binomial coefficient와 비슷하게 생겨서 나중에 binomial distribution을 적분할때, gamma distribution을 이용하면 매우 유용하다. Expected Value: E[X]=αα+βE[X] = \\frac{\\alpha}{\\alpha + \\beta} E[X]=α+βα​ Variance: σ2(X)=αβ(α+β)2(α+β+1)\\sigma^2(X) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} σ2(X)=(α+β)2(α+β+1)αβ​ Normal Distribution 정규 분포. p(x∣μ,σ2)=12πσ2exp(−(x−μ)22σ2)p(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}) p(x∣μ,σ2)=2πσ2​1​exp(−2σ2(x−μ)2​) Expected Value: E[X]=μE[X] = \\mu E[X]=μ Variance: Var(X)=σ2\\text{Var}(X) = \\sigma^2 Var(X)=σ2 만약, iid(independent identical distribution)에서 샘플링된 여러 샘플들, 즉, 똑같은 분포로부터 독립적인 시행으로 샘플링한 여러 샘플들의 평균 Xˉ\\bar{X}Xˉ은 정규 분포를 따른다. 샘플의 개수를 nnn, 그 샘플들을 샘플링한, 즉, 하나의 샘플을 샘플링한 분포의 실제 평균을 μ\\muμ, 분산을 σ2\\sigma^2σ2이라고 했을 때, 다음을 만족한다. Xˉ∼N(μ,σ2n)\\bar{X} \\sim \\mathbb{N}(\\mu, \\frac{\\sigma^2}{n}) Xˉ∼N(μ,nσ2​) 이를 central limit theorem(CLT) 이라고 부른다. 평균 μ\\muμ는 추정 대상이라서 모르지만, σ2\\sigma^2σ2는 샘플들의 분산으로 대체한다. 즉, 우리가 샘플링한샘플들의 평균은 실제 샘플 평균으로부터 어느정도 가깝다는 것이다. 또한, 분산은 샘플수에 반비레하는데, 이는 샘플이 많을수록, 진짜 샘플 평균에 가까워진다는 것을 알 수 있다. t-Distribution Student-t 분포, test용 분포라고도 한다. CLT에서, 샘플 평균의 분포를 standardize시키면 standard normal distribution이 아니라, t-distribution이 나온다. 분산값인 σ2\\sigma^2σ2가 샘플 분산인 S2S^2S2으로 대체되기 때문이다. S2=∑i(Xˉ−Xi)2n−1S^2 = \\frac{\\sum_i (\\bar{X}-X_i)^2}{n-1} S2=n−1∑i​(Xˉ−Xi​)2​ 이러면, Xˉ\\bar{X}Xˉ의 분포는 더 이상 normal distribution이 아닌, t-distribution을 따른다. ν=n−1\\nu = n-1ν=n−1이라고 했을 때, t(x)=Γ(ν+12)Γ(ν2)νπ(1+x2ν)−(ν+12)\\text{t}(x) = \\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}}(1+\\frac{x^2}{\\nu})^{-(\\frac{\\nu+1}{2})} t(x)=Γ(2ν​)νπ​Γ(2ν+1​)​(1+νx2​)−(2ν+1​) 이때, ν\\nuν는 자유도, degree of freedom이라고 부른다. Expected Value: E[X]=0 if ν≥1E[X] = 0 ~~~\\text{if } \\nu \\geq 1 E[X]=0 if ν≥1 Variance: Var(X)=νν−2 if ν≥2\\text{Var}(X) = \\frac{\\nu}{\\nu-2} ~~~~\\text{if } \\nu \\geq 2 Var(X)=ν−2ν​ if ν≥2","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"01. Probability","date":"2020-03-01T12:08:00.000Z","path":"studynotes/bayesian-statistics/01_Probability/","text":"통계학에서는 확률론을 기초로 한다. 확률론은 확률에 대한 이론을 다루며, 확률은 불확실성을 수치화(정량화)함으로써, 불확실성을 수학으로 다룰 수 있도록 도와준다. Background확률론을 하기 앞서서, 필요한 용어들을 먼저 정리할 필요가 있다. Experiments 시행이라고도 불리며, 하나의 데이터 샘플을 얻는 행위를 말한다. Sample space Sample space란, 어떤 시행에 의해 나올 수 있는 모든 경우의 집합을 의미한다. 예를들어, 동전던지기라는 시행에서는 뒷면 또는 앞면만이 나올 수 있다. 이때, 동전던지기라는 experiment의 sample space는 ${\\text{HEAD}, \\text{TAIL}}$이 된다. 주사위를 던지는 experiment에 대해서의 sample space는 ${1, 2, 3, 4, 5, 6}$이 될 것이다. Events 사건이라고도 불리며, Sample space의 부분집합이다. 예를 들어, 주사위를 던지는 시행에서, sample space는 ${1, 2, 3, 4, 5, 6}$이다. 그리고, 다음과 같은 event $A$를 정의할 수 있다. $A$: 짝수가 나오는 경우 이때, $A$는 ${2, 4, 6}$이 된다. Random variables 확률변수라고도 불리며, 각 sample space를 어떤 다른 라벨로 매핑하는 함수를 의미한다. 예를들어, 주사위를 던지는 experiment가 있다고 가정해보자. 이때, sample space는 ${1, 2, 3, 4, 5, 6}$이 된다. 이때, 우리는 random variable $X$를 다음과 같이 정의할 수 있다. $X = x_1$: 주사위가 짝수인 경우 $X=x_2$: 주사위가 홀수인 경우 $X=x_3$: 주사위가 4보다 크거나 같은 경우 Random variable은 여러개의 이벤트중 하나의 이벤트를 취할 수 있으며, 주사위 던지기의 기존 라벨 ${1, 2, 3, 4, 5, 6}$을 ${x_1, x_2, x_3}$으로 매핑하게 된다. 이때, random variable의 sample space는 ${x_1, x_2, x_3}$이 된다. 각 이벤트 $x_1, x_2, x_3$은 서로 겹치는 부분이 있어도 상관없다(예제에서는 $x_3$과 $x_2$가 모두 4를 가지고 있다. Definition of Probability확률은 불확실성을 정량화하는 도구이다. 어떤 experiment(시행)에 대한 random variable(확률변수) X가 있고, X는 $x_1,…,x_k$의 event(사건, 경우)를 취할 수 있을 때, $x_i$가 일어날 확률은 대문자 $P$를 이용하여 $P(X=x_i)$로 정의한다. 또는 $p_X(X=x_i)$로 표기하거나 간단하게 $p(x_i)$로 표기하기도 한다(세 가지 표현법 모두 같은 의미임). 어떤 사건이 일어날 확률은 항상 0보다 크거나 같으며 1보다 작거나 같다. 0 \\leq P(X=x_i) \\leq 1Odds어떤 event a에 대한 odds는 $O(a) = \\frac{P(a)}{P(a^C)}$라고 정의한다. 즉, 동전던지기에서 앞면이 나올 확률이 0.3이라면, 앞면이 나올 event에 대한 odds는 $O(X=h)=\\frac{P(X=h)}{P(X \\not = h)} = \\frac{0.3}{0.7} = \\frac{3}{7}$이다. How to Define Probability방금전까지 확률은 불확실성을 정량화해주는 도구라고 정의했다. 그런데, 어떻게 정량화를 해야 할까. 불확실성을 정량화하는 방법은 크게 3가지로 나눌 수 있다. Classical method Frequentist method Bayesian method Classical MethodEqually Likely Probability Sample space에서 모든 event들은 일어날 확률이 같다고 정의하는 방법이다. 동전 1번 던지는 시행에서 sample space는 앞면, 뒷면만 있다고 가정한다. 그럼 앞면이 나올 확률은 0.5이고, 뒷면이 나올 확률 역시 0.5이라고 정의한다. 하지만, 이러한 정의에는 문제가 있는데, 내일 날씨가 비가오거나, 맑거나, 우박이 내리는 3가지 경우만 있다고 가정해보자. 이때, classical method에 따르면, 맑을 확률은 0.33, 비가 올 확률도 0.33, 우박이 내릴 확률도 0.33이 된다. 따라서, classical method 방법은 매우 조심스럽게 사용해야 한다. Probability in Frequentist StatisticsRelative Rates of Events in Infinite Sequence 어떤 event의 확률을 “수많은 시행 가운데 그 event가 일어난 비율”이라고 정의하는 방법으로, Frequentist statistics에서 확률을 정의하는 방법이다. 즉, 동전 던지기에서 앞면이 나올 확률을 계산하고 싶다면, 일단 동전을 무수히 많이 던져본다. 1000번을 던진 후, 651번의 앞면이 나왔다면, 동전 던지기 시행에서 앞면이 나올 확률은 0.651 로 정의한다. 하지만, 이러한 정의에도 문제점이 있다. 어떤 이벤트가 일어날 확률을 계산하기 위해서는 많은 수의 샘플이 필요하고, experiment를 많이 수행해야 한다. 하지만, experiment가 가능한 경우는 실제로 그렇게 많지가 않다. 예를들어, 내일 비가 올 확률을 구하고 싶다고 해 보자. Frequentist statistics에 따르면, 내일 날씨를 여러번 샘플링 해야 한다. 즉, 내일 날씨를 확인하고, 다시 오늘로 돌아온 후 내일이 되면 날씨를 확인하고, 다시 오늘로 돌아와서 내일이 되면 날씨를 확인하고를 반복해야 한다. 하지만, 알다시피, 이건 타임머신이 있어야 가능하다. Probability in Bayesian StatisticsPersonal Perspective Bayesian statistics에서 확률을 정의하는 방법으로, 그 event가 일어날것 같다는 개인의 견해와, 필요하다면 이전의 데이터를 바탕으로 확률을 정의한다. 예를 들어, 내일 비가 올 확률을 구하기 위해 오늘의 날씨를 보고 내일 비가 올 확률이 0.7정도 되겠구나 하는 개인의 믿음을 확률에 반영하게 된다. Bayesian statistics에서 확률의 특징은 모든 것은 deterministic하지 않다. 즉, 0%, 100%는 존재하지 않으며 불확실성이 항상 존재한다고 가정한다. 그렇다면, “개인의 견해”를 얼만큼 반영하고 어떻게 설정해야 할까. 이때 사용하는 방법 중 하나가 fair bet이라는 방법이다. 예를 들어, 만약, 앞면이 나오면 7달러를 얻고, 뒷면이 나오면 3달러를 잃는다고 하자. 그럼 다음을 만족해야 한다. E [ \\text{gain} ]=p*7 + (1−p)*(−3) = 0즉, 내가 앞면이 나올 확률에 어느정도 베팅을 할 수 있는지에 대한 자신감을 고려해서 최대한 공평하게 확률을 선정해야 한다. 이때 $p = 0.3$이 되야 한다. Bayes’ Rule두 random variable $X,Y$가 있을 때, 다음을 Bayes’ rule이라고 정의한다. p(X=x|y) = \\frac{p(Y|X=x)p(X=x)}{p(y)}Bayes rule의 특징이자 장점은, conditioning variable의 위치를 뒤바꿀 수 있다는 점이다. 이 Bayes rule은 Bayesian statistics에서 매우 중요한 역할을 차지한다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"BayesianStatistics","slug":"BayesianStatistics","permalink":"https://wayexists02.github.io/tags/BayesianStatistics/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Bayesian Statistics","slug":"Study-Notes/Bayesian-Statistics","permalink":"https://wayexists02.github.io/categories/Study-Notes/Bayesian-Statistics/"}]},{"title":"Init Blog","date":"2020-03-01T03:39:05.000Z","path":"Blog-Init/","text":"Blog Init 재영이 화이팅~~ jaeyoung−taeukjaeyoung - taeuk jaeyoung−taeuk","tags":[{"name":"Home","slug":"Home","permalink":"https://wayexists02.github.io/tags/Home/"}],"categories":[{"name":"Home","slug":"Home","permalink":"https://wayexists02.github.io/categories/Home/"}]},{"title":"Machine Learning","date":"2020-02-29T23:51:55.000Z","path":"studynotes/machine-learning/Machine-Learning/","text":"My Interpretation of Machine Learning 기본적으로 통계학의 궁극적인 목표 중 하나는 우리가 모르는, sample space distribution을 최대한 추정하는 것이다. 추정하는 방법은 먼저, 우리가 수집한 데이터셋 DDD의 likelihood를 최대화 하는 분포가 진짜 sample space를 가장 유사하게 추정한 분포라고 가정한다. 다시 한번 반복하자면, 통계학의 목표중 하나는 우리가 모르는, sample space의 분포를 추정하는 것이다. Machine Learning Frequentist statistics에서는 많은 데이터 샘플을 뽑는 시행을 한 후, 데이터를 이용해서 모분포를 추정하는 것이라는 목표가 있다. 이것을 하기 위해 데이터셋을 바탕으로 확률 분포가 대충 어떻게 생겼을지 모델링하게 되며, 이때, 확률 분포를 모델링하는데 쓰이는 것이 바로 머신러닝이다. 데이터셋 DDD가 있다고 하자. 이 데이터셋에 있는 각 샘플들 did_idi​들은 어쨌든 모 분포(sample space distribution)에서 나올 확률이 어느 정도 높으니까 샘플링되어 우리 손에 들어왔을 것이다. Frequentist statistics에서는 바로 이것에 주목한다. 우리가 모은 데이터셋은 모 분포를 반영해서, 확률이 높은 애들이 많이 뽑히고 낮은 애들이 적게 뽑힐 것이다. 따라서, 우리가 모은 데이터셋이 뽑혀왔을 확률, 즉, likelihood를 최대화 하는 분포를 찾는다면, 이 분포가 모분포와 매우 유사할 것이라고 가정한다. 그 전에, 조건이 있다. 각 데이터 샘플 did_idi​는 반드시 모든 샘플과 동일한 분포에서 샘플링되어야 한다. 즉, 서울에서 온도측정하고 북극가서 온도 측정하면 안 된다. -&gt; identical 각 데이터 샘플 did_idi​는 모두 독립적인 시행으로 인한 샘플링이어야 한다. 즉, 첫 번째 동전던지기가 세 번째 동전던지기에 영향을 주지 않는다. 이와 같이, 이 조건을 자동으로 만족시키는 샘플링도 있다. -&gt; independent 위 두 가지 조건을 합쳐서, &quot;각 데이터 샘플 did_idi​는 iid(Identical independent distribution) 하에서 샘플링 되어야 한다&quot;고 말한다. iid를 만족시킴으로써, 우리는 하나의 모분포를 추정하기만 하면 된다. 즉, 한 분포의 파라미터 θ\\thetaθ를 추정하기만 하면 된다. iid를 만족시키는 시행으로 얻어진 데이터셋의 joint distribution, 즉, 데이터셋을 얻었을 확률을 다음과 같이 계산할 수 있다. P(D∣θ)=P(d1,d2,...,dn∣θ)=P(d1∣θ)P(d2∣θ)⋯P(dn∣θ)​P(D|\\theta) = P(d_1, d_2,...,d_n|\\theta) = P(d_1|\\theta)P(d_2|\\theta)\\cdots P(d_n|\\theta)​ P(D∣θ)=P(d1​,d2​,...,dn​∣θ)=P(d1​∣θ)P(d2​∣θ)⋯P(dn​∣θ)​ Maximum Likelihood Estimation 앞서 말했듯이, frequentist statistics에서는 모분포(sample space distribution)을 추정하기 위해서, likelihood를 모델링하고 likelihood를 최대화 하는 확률분포를 계산한다. 이 확률분포가 모분포의 추정이 된다. Likelihood라고 함은, 우리의 데이터 셋이 샘플링되어왔을 확률이라고 보면 된다. Likelihood Dist.=P(d1∣θ)P(d2∣θ)⋯P(dn∣θ)=P(D∣θ)\\text{Likelihood Dist.} = P(d_1|\\theta)P(d_2|\\theta)\\cdots P(d_n|\\theta) = P(D|\\theta) Likelihood Dist.=P(d1​∣θ)P(d2​∣θ)⋯P(dn​∣θ)=P(D∣θ) 여기서 θ\\thetaθ는 모분포의 파라미터이다. 이 likelihood를 최대화하는 분포를 찾는다면, 즉, 모분포의 파라미터 θ\\thetaθ의 추정값 θ^\\hat{\\theta}θ^를 찾는다면, 이 분포가 모분포와 유사할 것이다 라는 것이다. θ^=argmaxθP(D∣θ)​\\hat{\\theta} = argmax_{\\theta} P(D|\\theta)​ θ^=argmaxθ​P(D∣θ)​ 이와 같이 likelihood를 최대화 시키는 분포 파라미터 θ^\\hat{\\theta}θ^를 찾고, 그것을 파라미터로 하는 분포는 모분포와 가깝다는 것이 frequentist statistics에서 모분포를 추정하는 대표적인 방법이다. 이것을 MLE(Maximum likelihood estimation)라고 부른다. 하지만, 어떻게 최대화 시킬까? 무언가를 최대화 최소화시키는데 가장 먼저 떠오르는건 미분을 통해 극점을 찾는 것이다. 하지만, likelihood가 주어지지 않아서 미분또한 할 수 없다. 따라서 우리는 먼저 likelihood를 모델링해야한다. 이것은 잠시 후에 설명한다. How to Maximize Likelihood Likelihood를 최대화하는 분포를 구한다면, 그것이 모분포와 비슷해질 것이라는 것은 알겠다. 그렇다면, 어떻게 최대화시키는지 알아야 할 것이다. 그에 앞서서 동전을 100번 던지는 시행을 예로 들자. 우리는 동전을 100번 던졌을 때, 앞면이 몇번 나올까에 대한 확률 분포를 추정하고 싶다. 이때, 각 시행은 동전 1번 던지는게 아니라 100번 던지는게 1번의 시행이다. 우리는 시행 1회에 대한 확률을 먼저 모델링해야 한다. 이것은 binomial distribution으로 모델링할 수 있을 것이다. i-th experiment=P(di∣θ)=(100ni)θni(1−θ)100−ni​\\text{i-th experiment} = P(d_i|\\theta) = \\begin{pmatrix} 100 \\\\ n_i \\end{pmatrix} \\theta^{n_i} (1 - \\theta)^{100-n_i}​ i-th experiment=P(di​∣θ)=(100ni​​)θni​(1−θ)100−ni​​ iii번째의 시행에서는 100번 동전을 던지고 nin_ini​회의 앞면이 나왔다. 그리고 동전이 앞면이 나올 확률은 θ\\thetaθ이다. 모든 시행은 iid조건을 만족한다면, identical한 distribution에서 샘플링된 데이터이므로 모든 시행에서 θ\\thetaθ는 같다. 이 시행을 1000회 해서 1000개의 데이터를 모았다고 가정한다. 그럼 likelihood는 이들을 곱한 것이다. P(D∣θ)=P(d1∣θ)P(d2∣θ)⋯P(d1000∣θ)P(D|\\theta) = P(d_1|\\theta)P(d_2|\\theta)\\cdots P(d_1000|\\theta) P(D∣θ)=P(d1​∣θ)P(d2​∣θ)⋯P(d1​000∣θ) 이때, likelihood는 파라미터 θ\\thetaθ에 대한 함수가 된다. Likelihood는 이처럼 분포 파라미터의 함수가 된다. 그런데, 동전던지기는 우리가 미리 잘 알고있다시피 베르누이 시행이고, 이들을 100번 던졌을때 앞면이 몇번 나올까에 대한 것은 binomial distribution을 따른다. 따라서 시행 1회를 binomial distribution으로 모델링할 수 있었지만, 일반적으로 데이터 샘플링을 모델링할때는 무슨 distribution으로 모델링을 해야 할지 알 수 없다. 이때 등장하는게 바로 Machine Learning이다. Machine Learning은 이 &quot;시행&quot;을 모델링하는데 사용한다. 더 나아가 likelihood를 모델링하는데 사용한다! Likelihood를 모델링했다면, 이 likelihood는 우리가 추정하고자 하는 parameter인 θ\\thetaθ에 대해 미분이 가능해진다(parameter인 θ\\thetaθ를 구한다는 것은 likelihood를 추정하는 것이다. θ\\thetaθ는 likelihood를 나타내는 파라미터이기 때문이다.). 미분이 가능하다면, θ\\thetaθ에 따른 likelihood의 극점을 찾을 수 있다는 것이다. 많은 분들이 아시다시피 다음 조건을 만족할때, 극점이라고 부른다. dP(D∣θ)dθ=0​\\frac{dP(D|\\theta)}{d\\theta} = 0​ dθdP(D∣θ)​=0​ 하지만, 이 식은 극점을 가르처주지만, 그 점이 극대인지, 극소인지 가르쳐주지는 않는다. 이 것을 해결하기 위한 것이 **gradient(기울기)**이다. Negative Log Likelihood(NLL) &amp; Gradient Descent 흔히, 우리는 gradient descent를 machine learning 알고리즘의 최적화 방법론으로 알고 있다. 그런데 이것이 왜 machine learning 알고리즘을 최적화 할 수 있는 것인지 알아보려고 한다. likelihood를 모델링했고, 극대점을 찾아야 한다는 것도 알았다. 그런데, 단순히 미분값이 0인 θ^\\hat{\\theta}θ^를 찾는 것만으로는 극대인지, 극소인지 알 수 없다. 이때 사용하는 것이 gradient인데, 방법은 다음과 같다. 일단 θ\\thetaθ를 임의로 초기화한다. 현재 θ\\thetaθ값에 대해서 likelihood를 미분해본다. 미분해서 나온 값**(gradient라고 부른다)**의 부호가 (+)이라는 의미는 θ\\thetaθ를 증가시키면 likelihood가 증가한다는 의미이다. 반대로, gradient가 (-)부호라는 것은 θ\\thetaθ를 감소시켜야 likelihood가 증가한다는 의미이다. 따라서 likelihood의 완전한 maximum 지점을 찾을 수는 없더라도 현재 위치에서 어느 방향으로 가야 likelihood를 증가시키는지 알 수 있다. gradient를 이용해서 likelihood 산을 오른다는 느낌으로, gradient ascent 라는 용어가 있을 수 있겠다. 하지만, 문제가 있다. Likelihood는 확률을 데이터 샘플 수만큼 곱한 것이다. 즉, 매우매우 0에 가까운 값으로, 일반적으로 데이터 샘플 수는 1만개, 10만개가 넘어가는 경우도 많다. 이들을 다 곱하면 컴퓨터에게는 그냥 0이다. 따라서 미분을 하기도 전에 이미 likelihood는 표현조차 불가능하다. 이것을 해결하기 위해 likelihood에 로그를 씌워서 log likelihood를 만든다. 확률값은 0과 1 사이값이므로, log를 씌우면 상당히 절댓값이 큰 (-)값이 나올 가능성이 높다. 즉, 너무너무 작아서 컴퓨터로 표시되지 않는 문제를 해결할 수 있다. 또한, log를 씌운다고 해서 씌우기 전에 대소관계가 씌운 후에 바뀐다거나 하지 않는다. (log함수는 monotonically 증가하는 함수이다.) 다른 의미로, log likelihood를 최대화하는 θ^\\hat{\\theta}θ^와 likelihood를 최대화하는 θ^\\hat{\\theta}θ^가 같다. 좋은 예시로, y=−(x−1)2+1y=-(x-1)^2+1y=−(x−1)2+1를 최대화 하는 x값이나, log[y]=log[−(x−1)2+1]\\text{log}[y] = \\text{log}[-(x-1)^2+1]log[y]=log[−(x−1)2+1]를 최대화하는 x값은 똑같이 1이다. 그런데, likelihood는 확률이므로, 0과 1사이 값이다. 따라서 log를 씌우면 무조건 0보다 작거나 같다. 이 모양이 조금 이상하니까, -1를 곱해서 negative log likelihood를 도입한다. 즉 다음과 같다. NLL=−log[P(D∣θ)]​\\text{NLL} = -\\text{log}[P(D|\\theta)]​ NLL=−log[P(D∣θ)]​ 그런데, 정보 이론을 공부해보신 분이라면 어디서 많이 본 모양일 것이다. 정보 이론에서 entroy와 cross entropy라는 개념이 있다. 다음과 같다. Entropy=Ep[−log p],Cross Entropy=Ep[−log q]​\\text{Entropy} = E_p[-\\text{log} ~ p], \\\\ \\text{Cross Entropy} = E_p[-\\text{log} ~ q]​ Entropy=Ep​[−log p],Cross Entropy=Ep​[−log q]​ 정보 이론에서 엔트로피란, 불확실성의 높고 낮음을 나타낸다. (다른 의미로 정보량이 적고 많음을 의미한다) 즉, 확률 분포 p가 uniform distribution과 같이 뭐가 샘플링될지 전혀 알 수 없을수록, 엔트로피는 증가한다. 반대로, 어느 지점에서 확률이 매우 높은(분산이 매우 작은 normal distribution을 떠올리자) 분포는 우리가 어느 정도 뭐가 나올지 알고, 여러개 샘플링해보면 데이터의 다양성이 떨어진다. 따라서 엔트로피가 감소한다. Cross entropy란, 어느 분포 p에 대해서 다른 분포 q의 기댓값이다. 무슨 의미냐면, p분포와 q분포가 많이 다르게 생기면 생길수록 cross entropy가 증가한다. 반면, p분포와 q분포가 비슷하게 생길수록 cross entropy가 감소한다. 이는 두 분포간의 거리를 계산한다는 KL-divergence의 개념과도 거의 유사하며, 사실상 다 이어져 있는 개념이다. KL-divergence는 실제로 entropy와 cross entropy의 합이다. 왜 이 개념을 말했냐면, negative log likelihood에서, 모분포를 P\\mathbb{P}P라고 했을 때, 우리 손에 들어온 모든 데이터 샘플이 나왔을 확률을 모두 같다고 가정해보자. 왜 모두 같다고 해도 되냐면, 데이터 샘플에는 중복되어 샘플링 된 샘플이 많을 것이다. 동전 던지기를 10번 해서 앞면이 7번 나왔다면, 그 10번 시행에 모두 같은 비중치를 뒀지만, 다 더해보면 앞면의 비중치는 0.7, 뒷면의 비중치는 0.3으로, 많이 샘플링된 얘들은 높은 비중치를 가지게 된다. 따라서, 모든 데이터 샘플들에 비중치를 같다고 둬도, 중복된 샘플들 덕분에 실제로 비중치는 데이터 확률 분포를 반영하게 되는 것이다. 그렇게 되면 각 비중치 α\\alphaα를 통해 다음과 같이 표현될 수 있다. NLL=Σi−αlogP(di∣θ)=[예시]:Σi[−0.1⋅logθ]=−0.7⋅logθ−0.3⋅log(1−θ)\\text{NLL} = \\Sigma_i-\\alpha\\text{log}P(d_i|\\theta) = [\\text{예시}]: \\Sigma_i[-0.1 \\cdot \\text{log} \\theta] = -0.7 \\cdot \\text{log} \\theta - 0.3 \\cdot \\text{log}(1-\\theta) NLL=Σi​−αlogP(di​∣θ)=[예시]:Σi​[−0.1⋅logθ]=−0.7⋅logθ−0.3⋅log(1−θ) 각 비중치 위 모양은 cross entropy와 정확히 일치한다. 참고로 α\\alphaα를 곱해준다고 해서 likelihood를 최대화시키는 θ^\\hat{\\theta}θ^값은 변하지 않는다. −(x−1)2-(x-1)^2−(x−1)2를 최대화 시키는 x나, −0.1∗(x−1)2-0.1*(x-1)^2−0.1∗(x−1)2을 최대화 시키는 x는 모두 1이다. 같은 이유이다. 따라서, negative log likelihood는 cross entropy라고도 부른다. 종합해보면 다음 문장들은 모두 같은 의미이다. Likelihood를 최대화 시키는 파라미터를 구한다. Log likelihood를 최대화 시키는 파라미터를 구한다. Negative log likelihood를 최소화 시키는 파라미터를 구한다. Cross entropy를 최소화 시키는 파라미터를 구한다. 그리고, likelihood를 최대화 시키기 위해서 gradient ascent를 해야 했지만, negative로 만듦으로써, 이번엔 negative likelihood를 하강해야 하므로, gradient descent를 해야 하는 것이다. 하지만, 보다시피, gradient descent 방법으로는 theta를 증가시킬지, 감소시킬지는 가르쳐주지만, 어느정도 감소시켜야 할지, 증가시켜야 할지는 알려주지 않는다. 따라서 theta를 조금씩 증감하면서 gradient가 0이 되는 극점을 찾는 것이 gradient descent optimization이고, 극점을 찾는 과정을 training/learning(학습)이라고 부른다. Machine Learning in Frequentist Statistics 앞서 말했듯이, 데이터 샘플을 뽑는 1회 시행은 우리가 알고 있는 normal distribution이나, bernoulli distribution같은 것이 아닐 가능성이 있다. 따라서 우리는 유연하게 &quot;시행&quot;을 모델링해야 할 필요가 있다. Machine learning이라고 하면, weights www로 parameterize되며, 데이터 샘플 하나 xxx를 입력으로 받으면, 그 데이터 샘플이 어느 부류인지에 대한 확률 P(x∣w)P(x|w)P(x∣w)을 계산한다. 즉, machine learning이라는 이름의 방법론 속에 숨어있는 확률 분포로부터 입력으로 넣은 데이터 샘플이 뽑힐 확률을 계산하는 것이다. 이들을 모든 데이터 샘플들에 반복해서 모두 곱하면, P(x1∣w)P(x2∣w)⋯P(xn∣w)=P(X∣w)​P(x_1|w)P(x_2|w)\\cdots P(x_n|w) = P(X|w)​ P(x1​∣w)P(x2​∣w)⋯P(xn​∣w)=P(X∣w)​ 즉, likelihood를 모델링한 것이다. machine learning은 데이터 샘플 1개가 샘플링되는 모 확률 분포라고 가정하고, 그 모 확률분포를 모델링한 것에 지나지 않는다. Generalization Issue 머신 러닝으로 데이터 샘플들의 확률 분포를 모델링했다(말 그대로 모델링한 것이지, 진짜 모 확률 분포가 아니다. 추정일 뿐이다.). 그런데, 우리는 수집된 데이터 셋만을 이용해서 모 확률분포를 추정했는데, 우리가 모은 샘플들이 모 분포에서 나올 수 있는 모든 샘플들을 포함할까? 절대 아니다. 그럼, 샘플링되지 않은 놈들이 있을 수 있는데, 이들에 대해서도 잘 작동하는지는 어떻게 보장하나? 지금 우리는 frequentist statistics에서의 machine learning을 말하고 있다. 답은 frequentist statistics에 있다. Frequentist statistics에서는 가능한 많은 데이터 샘플을 뽑고 분포를 추정하게 된다. 여기서, **“가능한 많은 데이터 샘플”**이 핵심이다. 가능한 많은 데이터 샘플들을 뽑게 되면, 어쨌든 샘플링될 확률이 높은 데이터샘플은 많이 뽑힐 것이고, 적은 확률로 샘플링된 녀석들도 적은 개수나마 샘플링될 것이다. 즉, 데이터셋을 바탕으로 확률 분포를 모델링할때, 실제로 높은 확률을 가지는 샘플은 수가 많아서 모델링된 분포에도 높은 확률을 가질 것이고, 실제로 낮은 확률을 가지는 샘플은 수가 적어서 모델링된 분포에도 낮은 확률을 가질 것이다. 즉, 데이터 샘플수가 충분히 많다면, 모 확률분포와 매우매우 유사해진다. 그래서 머신러닝에서 “데이터셋을 많이 모아라~” 하는 것이다. 또한, 데이터 샘플들의 수가 많아질수록 매우 다양한 샘플들이 샘플링되어 있을 것이며, 이들 만으로도 충분히 모 확률분포에서 나올 수 있는 샘플들을 커버할 수 있다는 것이다. 따라서, 데이터 샘플 수가 적당히 많다면, 이들을 이용하면 모 확률분포와 매우 유사하게 모델링이 가능하고, 그럼, 미처 샘플링되지 못한 샘플들에 대해서도 잘 작동할 것이라는 이론이 있다. Summary 정리해보면, frequentist statistics에서의 machine learning이란, 일단은 함수이다. 그런데, 확률값을 반환하는 확률 함수이다. 좀 더 정확히 말해보면 데이터 샘플들의 진짜 모 확률분포를 모델링한, 확률분포이다. Machine learning은 parameter(weight)를 이용해서 이 확률 분포를 모델링하고, likelihood를 모델링한다. Likelihood를 최대화하는 parameter를 계산(정확히는 “추정”)한다. 이때, gradient descent를 이용한다. parameter를 계산하는 과정을 학습이라고 한다. 학습이 끝나면, machine learning은 모분포를 잘 추정한다고 가정하며, 일반화도 잘 이루어 졌을 것이라고 가정한다.","tags":[{"name":"StudyNotes","slug":"StudyNotes","permalink":"https://wayexists02.github.io/tags/StudyNotes/"},{"name":"MachineLearning","slug":"MachineLearning","permalink":"https://wayexists02.github.io/tags/MachineLearning/"}],"categories":[{"name":"Study Notes","slug":"Study-Notes","permalink":"https://wayexists02.github.io/categories/Study-Notes/"},{"name":"Machine Learning","slug":"Study-Notes/Machine-Learning","permalink":"https://wayexists02.github.io/categories/Study-Notes/Machine-Learning/"}]}]}