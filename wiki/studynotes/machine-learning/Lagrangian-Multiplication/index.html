<!DOCTYPE html>
<html lang="ko">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>Lagrangian Multiplication | Jaeyoung&#39;s Blog</title>
    
    
        <meta name="keywords" content="StudyNotes,MachineLearning">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Lagrangian MultiplicationConstraint optimization. 어떤 objective function을 파라미터에 대해 최대화하거나 loss function을 최소화하려고 하는데, 파라미터가 가질 수 있는 값에 제약조건이 있는 경우, Lagrangian multiplication을 사용할 수 있다. BackgroundLagrang">
<meta name="keywords" content="StudyNotes,MachineLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="Lagrangian Multiplication">
<meta property="og:url" content="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Lagrangian-Multiplication/index.html">
<meta property="og:site_name" content="Jaeyoung&#39;s Blog">
<meta property="og:description" content="Lagrangian MultiplicationConstraint optimization. 어떤 objective function을 파라미터에 대해 최대화하거나 loss function을 최소화하려고 하는데, 파라미터가 가질 수 있는 값에 제약조건이 있는 경우, Lagrangian multiplication을 사용할 수 있다. BackgroundLagrang">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200206200220651.png">
<meta property="og:updated_time" content="2020-03-06T03:01:31.173Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lagrangian Multiplication">
<meta name="twitter:description" content="Lagrangian MultiplicationConstraint optimization. 어떤 objective function을 파라미터에 대해 최대화하거나 loss function을 최소화하려고 하는데, 파라미터가 가질 수 있는 값에 제약조건이 있는 경우, Lagrangian multiplication을 사용할 수 있다. BackgroundLagrang">
<meta name="twitter:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200206200220651.png">
    

    
        <link rel="alternate" href="/atom.xml" title="Jaeyoung&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">
    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Jaeyoung&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Main</a>
                
                    <a class="main-nav-link" href="/archives">TimeLine</a>
                
                    <a class="main-nav-link" href="/categories">Category</a>
                
                    <a class="main-nav-link" href="/tags">Tag</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '포스트',
            PAGES: 'Pages',
            CATEGORIES: '카테고리',
            TAGS: '태그',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Main</a></td>
                
                    <td><a class="main-nav-link" href="/archives">TimeLine</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Category</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tag</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>카테고리</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Log
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/Blog-Init/">Init Blog</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Study Notes
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Bayesian Statistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/01_Probability/">01. Probability</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/02_Distribution/">02. Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/">03. Frequentist Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/">04. Bayesian Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/">05. Credible Intervals</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/">06. Prior Predictive Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/07_Priors/">07. Priors</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/">08. Bayesian Modeling</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/">09. Monte Carlo Estimation</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/">10. Markov Chain Monte Carlo</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/11_Linear_Regression/">11. Linear Regression</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/">12. Prior Sensitivity Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/">13. Hierarchical Models</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/">14. Predictive Simulations</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/">Appendix 1. Maximize a Posterior</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/">Appendix 2. Empirical Bayes</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Machine Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/">Machine Learning</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis/">Principal Component Analysis</a></li>  <li class="file active"><a href="/wiki/studynotes/machine-learning/Lagrangian-Multiplication/">Lagrangian Multiplication</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Distillation-Methods/">Distillation Methods</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st/">Restrict Boltzmann Machines 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd/">Restrict Boltzmann Machines 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/">Hidden Markov Models 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/">Hidden Markov Models 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/KL Divergence/">KL Divergence</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Reinforcement Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/01_Introduction/">01. Introduction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/">02. K-arm Bandits Problems</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/">03. Markov Decision Process</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">04. Policies and Value Functions</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">05. Policy Evaluation & Control</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/">06. Sample-based Reinforcement Learning</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-studynotes/machine-learning/Lagrangian-Multiplication" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/Study-Notes/">Study Notes</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Study-Notes/Machine-Learning/">Machine Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/MachineLearning/">MachineLearning</a>, <a class="tag-link" href="/tags/StudyNotes/">StudyNotes</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/studynotes/machine-learning/Lagrangian-Multiplication/">
            <time datetime="2020-03-01T23:51:55.000Z" itemprop="datePublished">2020-03-02</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/raw/writing/source/_posts/studynotes/machine-learning/Lagrangian-Multiplication.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/edit/writing/source/_posts/studynotes/machine-learning/Lagrangian-Multiplication.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/commits/writing/source/_posts/studynotes/machine-learning/Lagrangian-Multiplication.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Lagrangian Multiplication
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">카탈로그</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Lagrangian-Multiplication"><span class="toc-number">1.</span> <span class="toc-text">Lagrangian Multiplication</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Background"><span class="toc-number">1.1.</span> <span class="toc-text">Background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Vectors"><span class="toc-number">1.1.1.</span> <span class="toc-text">Gradient Vectors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Descent"><span class="toc-number">1.1.2.</span> <span class="toc-text">Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contraint-Optmization"><span class="toc-number">1.1.3.</span> <span class="toc-text">Contraint Optmization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lagrangian-Multiplication-1"><span class="toc-number">1.2.</span> <span class="toc-text">Lagrangian Multiplication</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Constraint-to-non-Constraint-Problem"><span class="toc-number">1.2.1.</span> <span class="toc-text">Constraint to non-Constraint Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-constraint-Optimization"><span class="toc-number">1.2.2.</span> <span class="toc-text">Multi-constraint Optimization</span></a></li></ol></li></ol></li></ol>
                </div>
            
        
        
            <h1 id="Lagrangian-Multiplication"><a href="#Lagrangian-Multiplication" class="headerlink" title="Lagrangian Multiplication"></a>Lagrangian Multiplication</h1><p>Constraint optimization.</p>
<p>어떤 objective function을 파라미터에 대해 최대화하거나 loss function을 최소화하려고 하는데, 파라미터가 가질 수 있는 값에 제약조건이 있는 경우, Lagrangian multiplication을 사용할 수 있다.</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Lagrangian multiplication은 gradient vector의 방향 특성을 이용한, constraint optimization을 푸는 방법론중 하나이다. Lagrangian을 알기 위해서는 gradient vector의 특성을 파악해야 한다.</p>
<h3 id="Gradient-Vectors"><a href="#Gradient-Vectors" class="headerlink" title="Gradient Vectors"></a>Gradient Vectors</h3><p>원 함수 $F(x, y) = x^2 + y^2 - 4 = 0$을 생각해보자. 이 함수의 differential $d F(x, y)$는 다음과 같다.<br>$$<br>d F(x, y) = \frac{\partial F(x, y)}{\partial x} \Delta x + \frac{\partial F(x, y)}{\partial y} \Delta y<br>$$<br>왜냐하면, $\frac{\partial F(x, y)}{\partial x} \Delta x$만을 봤을 때, $\frac{\partial F(x, y)}{\partial x}$는 $y$가 고정되어 있고, $x$만 변화시켰을때의 $F(x, y)$의 변화량이다. 즉, $x$방향의 기울기이다. 즉, $\frac{\partial F(x, y)}{\partial x}$는 $\Delta x$ 양 만큼의 미세한 변화를 $x$축에 가했을 때, $F(x, y)$의 변화량이다. 이건 $\frac{\partial F(x, y)}{\partial y}$도 마찬가지로 해석이 가능하다. 따라서, $d F(x, y)$는 $x, y$방향으로 각각 $\Delta x, \Delta y$만큼 변화를 가했을 때의 $F(x, y)$의 변화량이라고 볼 수 있다.</p>
<p>위 식은 다음처럼 표현할 수 있다.<br>$$<br>d F(x, y) = \begin{bmatrix}<br>\frac{\partial F(x, y)}{\partial x} \newline<br>\frac{\partial F(x, y)}{\partial y}<br>\end{bmatrix} \cdot \begin{bmatrix}<br>\Delta x \newline<br>\Delta y<br>\end{bmatrix} =<br>\nabla F(x, y) \cdot \Delta v<br>$$<br>이때, $$\cdot$$은 내적이고, $\nabla F(x, y)$는 gradient vector, $\Delta v$는 $x, y$가 변화하는 방향 벡터이다.</p>
<p>Gradient vector $\nabla F(x, y)$는 $F(x, y)$의 surface(tangent plane)에 수직인 특징이 있다.</p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>어떤 함수 $F(x, y)$를 최소화하는 $x, y$를 찾고 싶을 때는, 최소가되는 그 지점에서의 gradient vector가 $\nabla F(x, y)=0$를 만족한다는 성질을 이용하면 된다. 이렇게 계산된다면, closed form으로 minimum점을 계산할 수 있다. 하지만, parameter(여기선 $x, y$)의 수가 많거나 식이 복잡해지면 그게 쉽지가 않고, 적절한 시간 안에 계산불가능할 수 있다.</p>
<p>Gradient descent는 어떤 함수 $F(x, y)$가 있을 때, 이 함수의 최솟값을 iterative한 방식으로 계산하는 방법 중 하나이다. 특히, 최솟값을 찾기 위해 gradient vector를 이용해서 함수 $F(x, y)$의 그래프를 따라 하강하게 된다.</p>
<p>다음의 과정을 통해 gradient descent가 동작한다.</p>
<ol>
<li><p>일단, $x_0, y_0$을 설정(초기값)</p>
</li>
<li><p>$(x_0, y_0)$에서의 gradient vector $\nabla F(x_0, y_0)$을 계산한다.</p>
</li>
<li><p>여기서 $F(x, y)$의 변화량이 가장 작은(가장 큰 음수) 방향으로 $x, y$를 이동시켜야 하는데, 즉, $\Delta v = \begin{bmatrix} \Delta x \newline \Delta y \end{bmatrix}$를 구해야 한다.</p>
</li>
<li><p>Gradient vector에서 나온 식에서, $dF(x, y)$를 최소화하는, 내적을 구해야 하는데, gradient vector는 이미 계산했고, $\Delta v$의 step size가 정해졌을 때, $\Delta v$의 방향을 gradient vector와 180도 반대방향으로 가게 한다면, $dF(x, y)$가 절댓값이 가장 큰 음수가 될 것이다. 즉,<br>$$<br>\Delta v = -\nabla F(x, y)<br>$$<br>하지만, step size를 1로 두면 너무 크다. 따라서, 작은 수 $\eta$를 곱해준다.<br>$$<br>\Delta v = - \eta \nabla F(x, y)<br>$$</p>
</li>
</ol>
<h3 id="Contraint-Optmization"><a href="#Contraint-Optmization" class="headerlink" title="Contraint Optmization"></a>Contraint Optmization</h3><p>다음과 같이, 어떤 함수 $F(x, y)$를 최소화 또는 최대화하는데, 파라미터 $x, y$의 범위에 조건이 걸린 경우를 말한다.<br>$$<br>\underset{x, y}{ \text{min} } [F(x, y) = x^2 + y^2] ~ \text{ s.t } ~ x + y = 1<br>$$<br>즉, $x + y = 1$을 만족하는 $x, y$중에서 $x^2+ y^2$를 최소화하는 $x, y$를 찾아야 한다는 것.</p>
<p>이 경우는 매우 간단하게 contraint 식을 $F(x, y)$에 대입해주면 된다.<br>$$<br>F(x, y) = (1 - y)^2 + y^2<br>$$<br>따라서, 이를 미분하고 gradient vector가 0이 되는 지점을 찾으면 될 것이다.</p>
<p>하지만, $F(x, y)$가 복잡하고, constaint 식 역시 복잡하며, 심지어 contraint가 여러개일 경우, 이렇게 closed form으로 구하는게 불가능해진다. 이를 좀 더 보편적으로 해결하기 위한 방법이 Lagrangian multiplication을 이용하는 것이다.</p>
<h2 id="Lagrangian-Multiplication-1"><a href="#Lagrangian-Multiplication-1" class="headerlink" title="Lagrangian Multiplication"></a>Lagrangian Multiplication</h2><p>어떤 objective function $F(x, y)$가 있고, constraint 함수인 $g(x, y)$가 있을 때, $g(x, y)$를 만족하면서 $F(x, y)$를 최대화하는 지점 $(x’, y’)$에서는, $F$의 gradient vector $\nabla F(x’, y’)$와 $g$의 gradient vector $\nabla g(x’, y’)$의 방향은 같거나 180도 방향이다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200206200220651.png" alt="image-20200206200220651"></p>
<p>그리고, 그 외 지점에서는 이게 성립되지 않는다. 따라서, 다음을 만족하는 $(x’, y’)$은 maximum 또는 minimum point라고 볼 수 있다.<br>$$<br>\nabla F(x’, y’) = \lambda \nabla g(x’, y’)<br>$$<br>$\lambda$는 상수이며, 두 gradient vector가 반드시 같은 크기일 필요는 없다는 의미로 해석될 수 있다. 하지만, 방향은 반드시 평행하다.</p>
<p>이 수식을 이용해서 constraint optimization을 해결하는 방식을 lagrangian multiplication이라고 부르며, $\lambda$는 lagrangian constant라고 부른다.</p>
<p>예를들어, 다음을 만족하는 점을 찾는다고 가정한다.<br>$$<br>\underset{x, y}{ \text{min} } [F(x, y) = xy] ~ \text{ s.t } ~ x^2 + y^2 - 4 = 0<br>$$<br>즉, $g(x, y) = x^2 + y^2 - 4 = 0$이다.</p>
<p>이 수식은 대입법을 이용해서 closed form으로 바로 풀 수 있지만, lagrangian muliplication방법으로 풀어볼 수도 있다.<br>$$<br>\nabla F(x, y) = \begin{bmatrix}<br>\frac{\partial F(x, y)}{\partial x} \newline<br>\frac{\partial F(x, y)}{\partial y}<br>\end{bmatrix}<br>= \lambda \cdot \begin{bmatrix}<br>\frac{\partial g(x, y)}{\partial x} \newline<br>\frac{\partial g(x, y)}{\partial y}<br>\end{bmatrix}<br>$$<br>이므로,<br>$$<br>\begin{bmatrix}<br>y \newline<br>x<br>\end{bmatrix}<br>= \begin{bmatrix}<br>2 \lambda x \newline<br>2 \lambda y<br>\end{bmatrix}<br>$$<br>일 것이다. 또한, 3변수 연립방정식을 풀기 위해 $g(x, y) = x^2 + y^2 - 4 = 0$도 같이 이용한다. 이 세 가지 수식을 이용한 연립방정식을 풀면, constraint를 만족하는 극점(극대, 극소)을 얻을 수 있다.</p>
<p>요약하면, 다음을 만족하는 점 $(x, y)$는 optimal point이다. 따라서, 다음 연립방정식을 풀면 된다.<br>$$<br>\begin{bmatrix}<br>\frac{\partial F(x, y)}{\partial x} \newline<br>\frac{\partial F(x, y)}{\partial y}<br>\end{bmatrix}<br>=<br>\begin{bmatrix}<br>\frac{\partial g(x, y)}{\partial x} \newline<br>\frac{\partial g(x, y)}{\partial y}<br>\end{bmatrix}<br>$$</p>
<p>$$<br>g(x, y) = 0<br>$$</p>
<h3 id="Constraint-to-non-Constraint-Problem"><a href="#Constraint-to-non-Constraint-Problem" class="headerlink" title="Constraint to non-Constraint Problem"></a>Constraint to non-Constraint Problem</h3><p>Lagrangian multiplication을 푸는 것은 다음 식을 만족하는 $\vec{x}$를 구하는 것이다.<br>$$<br>\nabla F(\vec{x}) = \lambda \cdot \nabla g(\vec{x})<br>$$<br>이 식을 조금 변형해보면,<br>$$<br>\nabla F(\vec{x}) = \nabla \lambda g(\vec{x})<br>$$<br>$$<br>\nabla F(\vec{x}) - \nabla \lambda g(\vec{x}) = 0<br>$$</p>
<p>$$<br>\nabla (F(\vec{x}) - \lambda g(\vec{x})) = 0<br>$$</p>
<p>$Q(\vec{x}, \lambda) = F(\vec{x}) - \lambda g(\vec{x})$라고 정의해보면,<br>$$<br>\bigtriangledown Q(\vec{x}, \lambda) = 0<br>$$<br>으로 정리할 수 있다. 이것은, $Q(\vec{x},\lambda)$를 non-constaint optimization을 한 식이 된다.</p>
<p>즉, $F(\vec{x})$를 어떤 constraint $g(\vec{x})$에 맞게 optimization을 한다는 것은, $F(\vec{x}) - \lambda g(\vec{x})$를 non-constraint 환경에서 optimization하는 것과 같다.</p>
<p>Neural network regularization도 해당 constraint ($l_1 norm, l_2 norm$) 에 맞게 $loss$함수를 최적화하는 것이라고 해석할 수도 있지 않을까. 다만, 차이점은, lagrangian 에선, $\lambda$도 파라미터이고, $\vec{x}$뿐 아니라 $\lambda$에 대해서도 최적화를 수행한다. Neural network regularization에서는 $\vec{x}$에 대해서만 최적화를 하며, $\lambda$는 하이퍼파라미터로 한다. 제약조건을 완전히 지키지는 않고, 약간의 제제만 가하는 것이라고 볼 수 있겠다.</p>
<h3 id="Multi-constraint-Optimization"><a href="#Multi-constraint-Optimization" class="headerlink" title="Multi-constraint Optimization"></a>Multi-constraint Optimization</h3><p>만약, $F(\vec{x})$를 최적화하는데, constraint가 $g_1(\vec{x}), g_2(\vec{x}), \cdots, g_k(\vec{x})$ 등 $k$개가 있다고 해 보자. 이때, $F(\vec{x})$의 극점이면서, 위 constraint들을 만족시키는 $$\vec{x}$$를 구하는 것은 다음의 식을 푸는 것과 같다.<br>$$<br>\nabla (F(\vec{x}) - \lambda_1 g_1(\vec{x}) - \lambda_2 g_2 (\vec{x}) - \cdots - \lambda_k g_k (\vec{x})) = 0<br>$$<br>또는 $Q(\vec{x}, \lambda) = F(\vec{x}) - \lambda_1 g_2(\vec{x}) - \cdots - \lambda_k g_k(\vec{x})$의 극점을 구하는 것과 같다.<br>$$<br>\nabla Q(\vec{x}, \lambda) = 0 ~ \text{w.r.t} ~ \vec{x}, \lambda<br>$$</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/wiki/studynotes/reinforcement-learning/01_Introduction/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">다음 글</strong>
            <div class="article-nav-title">
                
                    01. Introduction
                
            </div>
        </a>
    
    
        <a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">이전 글</strong>
            <div class="article-nav-title">Principal Component Analysis</div>
        </a>
    
</nav>





    
    

    <script src="https://utteranc.es/client.js"
        repo="taeuk-gang/taeuk-gang.github.io"
        issue-term="title"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>



<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Lee Jaeyoung &copy; 2020 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <!-- <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a> -->
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>