<!DOCTYPE html>
<html lang="ko">

<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png">
    

    <title>
        
          Lagrangian Multiplication - Jaeyoung&#39;s Blog
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    <link rel="stylesheet" href="/css/book.css">
    <script src="/js/book.js"></script>

    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

</head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>JAEYOUNG&#39;S BLOG</span>
  </a>
</div>
    <div class="book-menu">
  <ul>
<li><a href="/"><strong>Home</strong></a></li>
</ul>
<h1 id="Study-Notes"><strong>Study Notes</strong></h1>
<h2 id="Machine-Learning">Machine Learning</h2>
<ul>
<li><a href="/wiki/studynotes/machine-learning/Machine-Learning">Machine Learning</a></li>
<li><a href="/wiki/studynotes/machine-learning/KL-Divergence">KL Divergence</a></li>
<li><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis">Principal Component Analysis</a></li>
<li><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st">Restrict Boltzmann Machine 1</a></li>
<li><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd">Restrict Boltzmann Machine 2</a></li>
</ul>
<h1 id="Extra"><strong>Extra</strong></h1>
<ul>
<li><a href="/Blog-Init">Blog Init</a></li>
</ul>

</div>

<script src="/js/book-menu.js"></script>
  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1>Lagrangian Multiplication</h1>
<p>Constraint optimization.</p>
<p>어떤 objective function을 파라미터에 대해 최대화하거나 loss function을 최소화하려고 하는데, 파라미터가 가질 수 있는 값에 제약조건이 있는 경우, Lagrangian multiplication을 사용할 수 있다.</p>
<h2 id="Background">Background</h2>
<p>Lagrangian multiplication은 gradient vector의 방향 특성을 이용한, constraint optimization을 푸는 방법론중 하나이다. Lagrangian을 알기 위해서는 gradient vector의 특성을 파악해야 한다.</p>
<h3 id="Gradient-Vectors">Gradient Vectors</h3>
<p>원 함수 $F(x, y) = x^2 + y^2 - 4 = 0$을 생각해보자. 이 함수의 differential $d F(x, y)$는 다음과 같다.<br>
$$<br>
d F(x, y) = \frac{\partial F(x, y)}{\partial x} \Delta x + \frac{\partial F(x, y)}{\partial y} \Delta y<br>
$$<br>
왜냐하면, $\frac{\partial F(x, y)}{\partial x} \Delta x$만을 봤을 때, $\frac{\partial F(x, y)}{\partial x}$는 $y$가 고정되어 있고, $x$만 변화시켰을때의 $F(x, y)$의 변화량이다. 즉, $x$방향의 기울기이다. 즉, $\frac{\partial F(x, y)}{\partial x}$는 $\Delta x$ 양 만큼의 미세한 변화를 $x$축에 가했을 때, $F(x, y)$의 변화량이다. 이건 $\frac{\partial F(x, y)}{\partial y}$도 마찬가지로 해석이 가능하다. 따라서, $d F(x, y)$는 $x, y$방향으로 각각 $\Delta x, \Delta y$만큼 변화를 가했을 때의 $F(x, y)$의 변화량이라고 볼 수 있다.</p>
<p>위 식은 다음처럼 표현할 수 있다.<br>
$$<br>
d F(x, y) = \begin{bmatrix}<br>
\frac{\partial F(x, y)}{\partial x} \newline<br>
\frac{\partial F(x, y)}{\partial y}<br>
\end{bmatrix} \cdot \begin{bmatrix}<br>
\Delta x \newline<br>
\Delta y<br>
\end{bmatrix} =<br>
\nabla F(x, y) \cdot \Delta v<br>
$$<br>
이때, $$\cdot$$은 내적이고, $\nabla F(x, y)$는 gradient vector, $\Delta v$는 $x, y$가 변화하는 방향 벡터이다.</p>
<p>Gradient vector $\nabla F(x, y)$는 $F(x, y)$의 surface(tangent plane)에 수직인 특징이 있다.</p>
<h3 id="Gradient-Descent">Gradient Descent</h3>
<p>어떤 함수 $F(x, y)$를 최소화하는 $x, y$를 찾고 싶을 때는, 최소가되는 그 지점에서의 gradient vector가 $\nabla F(x, y)=0$를 만족한다는 성질을 이용하면 된다. 이렇게 계산된다면, closed form으로 minimum점을 계산할 수 있다. 하지만, parameter(여기선 $x, y$)의 수가 많거나 식이 복잡해지면 그게 쉽지가 않고, 적절한 시간 안에 계산불가능할 수 있다.</p>
<p>Gradient descent는 어떤 함수 $F(x, y)$가 있을 때, 이 함수의 최솟값을 iterative한 방식으로 계산하는 방법 중 하나이다. 특히, 최솟값을 찾기 위해 gradient vector를 이용해서 함수 $F(x, y)$의 그래프를 따라 하강하게 된다.</p>
<p>다음의 과정을 통해 gradient descent가 동작한다.</p>
<ol>
<li>
<p>일단, $x_0, y_0$을 설정(초기값)</p>
</li>
<li>
<p>$(x_0, y_0)$에서의 gradient vector $\nabla F(x_0, y_0)$을 계산한다.</p>
</li>
<li>
<p>여기서 $F(x, y)$의 변화량이 가장 작은(가장 큰 음수) 방향으로 $x, y$를 이동시켜야 하는데, 즉, $\Delta v = \begin{bmatrix} \Delta x \newline \Delta y \end{bmatrix}$를 구해야 한다.</p>
</li>
<li>
<p>Gradient vector에서 나온 식에서, $dF(x, y)$를 최소화하는, 내적을 구해야 하는데, gradient vector는 이미 계산했고, $\Delta v$의 step size가 정해졌을 때, $\Delta v$의 방향을 gradient vector와 180도 반대방향으로 가게 한다면, $dF(x, y)$가 절댓값이 가장 큰 음수가 될 것이다. 즉,<br>
$$<br>
\Delta v = -\nabla F(x, y)<br>
$$<br>
하지만, step size를 1로 두면 너무 크다. 따라서, 작은 수 $\eta$를 곱해준다.<br>
$$<br>
\Delta v = - \eta \nabla F(x, y)<br>
$$</p>
</li>
</ol>
<h3 id="Contraint-Optmization">Contraint Optmization</h3>
<p>다음과 같이, 어떤 함수 $F(x, y)$를 최소화 또는 최대화하는데, 파라미터 $x, y$의 범위에 조건이 걸린 경우를 말한다.<br>
$$<br>
\underset{x, y}{ \text{min} } [F(x, y) = x^2 + y^2] ~ \text{ s.t } ~ x + y = 1<br>
$$<br>
즉, $x + y = 1$을 만족하는 $x, y$중에서 $x^2+ y^2$를 최소화하는 $x, y$를 찾아야 한다는 것.</p>
<p>이 경우는 매우 간단하게 contraint 식을 $F(x, y)$에 대입해주면 된다.<br>
$$<br>
F(x, y) = (1 - y)^2 + y^2<br>
$$<br>
따라서, 이를 미분하고 gradient vector가 0이 되는 지점을 찾으면 될 것이다.</p>
<p>하지만, $F(x, y)$가 복잡하고, constaint 식 역시 복잡하며, 심지어 contraint가 여러개일 경우, 이렇게 closed form으로 구하는게 불가능해진다. 이를 좀 더 보편적으로 해결하기 위한 방법이 Lagrangian multiplication을 이용하는 것이다.</p>
<h2 id="Lagrangian-Multiplication">Lagrangian Multiplication</h2>
<p>어떤 objective function $F(x, y)$가 있고, constraint 함수인 $g(x, y)$가 있을 때, $g(x, y)$를 만족하면서 $F(x, y)$를 최대화하는 지점 $(x’, y’)$에서는, $F$의 gradient vector $\nabla F(x’, y’)$와 $g$의 gradient vector $\nabla g(x’, y’)$의 방향은 같거나 180도 방향이다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200206200220651.png" alt="image-20200206200220651"></p>
<p>그리고, 그 외 지점에서는 이게 성립되지 않는다. 따라서, 다음을 만족하는 $(x’, y’)$은 maximum 또는 minimum point라고 볼 수 있다.<br>
$$<br>
\nabla F(x’, y’) = \lambda \nabla g(x’, y’)<br>
$$<br>
$\lambda$는 상수이며, 두 gradient vector가 반드시 같은 크기일 필요는 없다는 의미로 해석될 수 있다. 하지만, 방향은 반드시 평행하다.</p>
<p>이 수식을 이용해서 constraint optimization을 해결하는 방식을 lagrangian multiplication이라고 부르며, $\lambda$는 lagrangian constant라고 부른다.</p>
<p>예를들어, 다음을 만족하는 점을 찾는다고 가정한다.<br>
$$<br>
\underset{x, y}{ \text{min} } [F(x, y) = xy] ~ \text{ s.t } ~ x^2 + y^2 - 4 = 0<br>
$$<br>
즉, $g(x, y) = x^2 + y^2 - 4 = 0$이다.</p>
<p>이 수식은 대입법을 이용해서 closed form으로 바로 풀 수 있지만, lagrangian muliplication방법으로 풀어볼 수도 있다.<br>
$$<br>
\nabla F(x, y) = \begin{bmatrix}<br>
\frac{\partial F(x, y)}{\partial x} \newline<br>
\frac{\partial F(x, y)}{\partial y}<br>
\end{bmatrix}<br>
= \lambda \cdot \begin{bmatrix}<br>
\frac{\partial g(x, y)}{\partial x} \newline<br>
\frac{\partial g(x, y)}{\partial y}<br>
\end{bmatrix}<br>
$$<br>
이므로,<br>
$$<br>
\begin{bmatrix}<br>
y \newline<br>
x<br>
\end{bmatrix}<br>
= \begin{bmatrix}<br>
2 \lambda x \newline<br>
2 \lambda y<br>
\end{bmatrix}<br>
$$<br>
일 것이다. 또한, 3변수 연립방정식을 풀기 위해 $g(x, y) = x^2 + y^2 - 4 = 0$도 같이 이용한다. 이 세 가지 수식을 이용한 연립방정식을 풀면, constraint를 만족하는 극점(극대, 극소)을 얻을 수 있다.</p>
<h1>요약하면, 다음을 만족하는 점 $(x, y)$는 optimal point이다. 따라서, 다음 연립방정식을 풀면 된다.<br>
$$<br>
\begin{bmatrix}<br>
\frac{\partial F(x, y)}{\partial x} \newline<br>
\frac{\partial F(x, y)}{\partial y}<br>
\end{bmatrix}</h1>
<p>\begin{bmatrix}<br>
\frac{\partial g(x, y)}{\partial x} \newline<br>
\frac{\partial g(x, y)}{\partial y}<br>
\end{bmatrix}<br>
$$</p>
<p>$$<br>
g(x, y) = 0<br>
$$</p>
<h3 id="Constraint-to-non-Constraint-Problem">Constraint to non-Constraint Problem</h3>
<p>Lagrangian multiplication을 푸는 것은 다음 식을 만족하는 $\vec{x}$를 구하는 것이다.<br>
$$<br>
\nabla F(\vec{x}) = \lambda \cdot \nabla g(\vec{x})<br>
$$<br>
이 식을 조금 변형해보면,<br>
$$<br>
\nabla F(\vec{x}) = \nabla \lambda g(\vec{x})<br>
$$<br>
$$<br>
\nabla F(\vec{x}) - \nabla \lambda g(\vec{x}) = 0<br>
$$</p>
<p>$$<br>
\nabla (F(\vec{x}) - \lambda g(\vec{x})) = 0<br>
$$</p>
<p>$Q(\vec{x}, \lambda) = F(\vec{x}) - \lambda g(\vec{x})$라고 정의해보면,<br>
$$<br>
\bigtriangledown Q(\vec{x}, \lambda) = 0<br>
$$<br>
으로 정리할 수 있다. 이것은, $Q(\vec{x},\lambda)$를 non-constaint optimization을 한 식이 된다.</p>
<p>즉, $F(\vec{x})$를 어떤 constraint $g(\vec{x})$에 맞게 optimization을 한다는 것은, $F(\vec{x}) - \lambda g(\vec{x})$를 non-constraint 환경에서 optimization하는 것과 같다.</p>
<p>Neural network regularization도 해당 constraint ($l_1 norm, l_2 norm$) 에 맞게 $loss$함수를 최적화하는 것이라고 해석할 수도 있지 않을까. 다만, 차이점은, lagrangian 에선, $\lambda$도 파라미터이고, $\vec{x}$뿐 아니라 $\lambda$에 대해서도 최적화를 수행한다. Neural network regularization에서는 $\vec{x}$에 대해서만 최적화를 하며, $\lambda$는 하이퍼파라미터로 한다. 제약조건을 완전히 지키지는 않고, 약간의 제제만 가하는 것이라고 볼 수 있겠다.</p>
<h3 id="Multi-constraint-Optimization">Multi-constraint Optimization</h3>
<p>만약, $F(\vec{x})$를 최적화하는데, constraint가 $g_1(\vec{x}), g_2(\vec{x}), \cdots, g_k(\vec{x})$ 등 $k$개가 있다고 해 보자. 이때, $F(\vec{x})$의 극점이면서, 위 constraint들을 만족시키는 $\vec{x}$를 구하는 것은 다음의 식을 푸는 것과 같다.<br>
$$<br>
\nabla (F(\vec{x}) - \lambda_1 g_1(\vec{x}) - \lambda_2 g_2 (\vec{x}) - \cdots - \lambda_k g_k (\vec{x})) = 0<br>
$$<br>
또는 $Q(\vec{x}, \lambda) = F(\vec{x}) - \lambda_1 g_2(\vec{x}) - \cdots - \lambda_k g_k(\vec{x})$의 극점을 구하는 것과 같다.<br>
$$<br>
\nabla Q(\vec{x}, \lambda) = 0 ~ \text{w.r.t} ~ \vec{x}, \lambda<br>
$$</p>

</div>


  <div class="book-comments">
    




  </div>


<script src="/js/book-post.js"></script>
        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="L"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Lee Jaeyoung</div>
      <div>2020-03-02</div>
    </div>
  </div>

  
    <div class="divider"></div>

    <div class="link">
      <a class="category-link" href="/categories/Study-Notes/">Study Notes</a> <a class="category-link" href="/categories/Study-Notes/Machine-Learning/">Machine Learning</a>

      <a class="tag-link" href="/tags/MachineLearning/">#MachineLearning</a> <a class="tag-link" href="/tags/StudyNotes/">#StudyNotes</a>
    </div>
    
  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>

<script src="/js/book-toc.js"></script>
</div>
      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>

<script src="/js/book.js"></script>