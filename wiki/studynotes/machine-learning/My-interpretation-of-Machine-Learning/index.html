<!DOCTYPE html>
<html lang="ko">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>Machine Learning | Jaeyoung&#39;s Blog</title>
    
    
        <meta name="keywords" content="StudyNotes,MachineLearning">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="My Interpretation of Machine Learning통계학을 분류하는 방법으로는 여러 가지가 있지만, 다음과 같이 통계학을 분류할 수도 있다.  Frequentist Statistics Bayesian Statistics  각 통계학에서 machine learning의 방법론은 약간씩 차이가 있는 듯 하다. 하지만, 기본적으로 각 통계학의 궁">
<meta name="keywords" content="StudyNotes,MachineLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/index.html">
<meta property="og:site_name" content="Jaeyoung&#39;s Blog">
<meta property="og:description" content="My Interpretation of Machine Learning통계학을 분류하는 방법으로는 여러 가지가 있지만, 다음과 같이 통계학을 분류할 수도 있다.  Frequentist Statistics Bayesian Statistics  각 통계학에서 machine learning의 방법론은 약간씩 차이가 있는 듯 하다. 하지만, 기본적으로 각 통계학의 궁">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303084924874.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303084905099.png">
<meta property="og:updated_time" content="2020-03-06T02:34:38.895Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning">
<meta name="twitter:description" content="My Interpretation of Machine Learning통계학을 분류하는 방법으로는 여러 가지가 있지만, 다음과 같이 통계학을 분류할 수도 있다.  Frequentist Statistics Bayesian Statistics  각 통계학에서 machine learning의 방법론은 약간씩 차이가 있는 듯 하다. 하지만, 기본적으로 각 통계학의 궁">
<meta name="twitter:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303084924874.png">
    

    
        <link rel="alternate" href="/atom.xml" title="Jaeyoung&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">
    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Jaeyoung&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Main</a>
                
                    <a class="main-nav-link" href="/archives">TimeLine</a>
                
                    <a class="main-nav-link" href="/categories">Category</a>
                
                    <a class="main-nav-link" href="/tags">Tag</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '포스트',
            PAGES: 'Pages',
            CATEGORIES: '카테고리',
            TAGS: '태그',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Main</a></td>
                
                    <td><a class="main-nav-link" href="/archives">TimeLine</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Category</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tag</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>카테고리</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Log
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/Blog-Init/">Init Blog</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Study Notes
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Bayesian Statistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/01_Probability/">01. Probability</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/02_Distribution/">02. Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/">03. Frequentist Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/">04. Bayesian Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/">05. Credible Intervals</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/">06. Prior Predictive Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/07_Priors/">07. Priors</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/">08. Bayesian Modeling</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/">09. Monte Carlo Estimation</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/">10. Markov Chain Monte Carlo</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/11_Linear_Regression/">11. Linear Regression</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/">12. Prior Sensitivity Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/">13. Hierarchical Models</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/">14. Predictive Simulations</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/">Appendix 1. Maximize a Posterior</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/">Appendix 2. Empirical Bayes</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Machine Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file active"><a href="/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/">Machine Learning</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis/">Principal Component Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Lagrangian-Multiplication/">Lagrangian Multiplication</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Distillation-Methods/">Distillation Methods</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st/">Restrict Boltzmann Machines 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd/">Restrict Boltzmann Machines 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/">Hidden Markov Models 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/">Hidden Markov Models 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/KL Divergence/">KL Divergence</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Reinforcement Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/01_Introduction/">01. Introduction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/">02. K-arm Bandits Problems</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/">03. Markov Decision Process</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">04. Policies and Value Functions</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">05. Policy Evaluation & Control</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/">06. Sample-based Reinforcement Learning</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-studynotes/machine-learning/My-interpretation-of-Machine-Learning" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/Study-Notes/">Study Notes</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Study-Notes/Machine-Learning/">Machine Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/MachineLearning/">MachineLearning</a>, <a class="tag-link" href="/tags/StudyNotes/">StudyNotes</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/">
            <time datetime="2020-02-29T23:51:55.000Z" itemprop="datePublished">2020-03-01</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/raw/writing/source/_posts/studynotes/machine-learning/My-interpretation-of-Machine-Learning.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/edit/writing/source/_posts/studynotes/machine-learning/My-interpretation-of-Machine-Learning.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/commits/writing/source/_posts/studynotes/machine-learning/My-interpretation-of-Machine-Learning.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Machine Learning
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">카탈로그</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#My-Interpretation-of-Machine-Learning"><span class="toc-number">1.</span> <span class="toc-text">My Interpretation of Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Machine-Learning-in-Frequentist-Statistics"><span class="toc-number">1.0.1.</span> <span class="toc-text">Machine Learning in Frequentist Statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Maximum-Likelihood-Estimation"><span class="toc-number">1.0.1.1.</span> <span class="toc-text">Maximum Likelihood Estimation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#How-to-Maximize-Likelihood"><span class="toc-number">1.0.1.2.</span> <span class="toc-text">How to Maximize Likelihood</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Negative-Log-Likelihood-NLL-amp-Gradient-Descent"><span class="toc-number">1.0.1.3.</span> <span class="toc-text">Negative Log Likelihood(NLL) &amp; Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Machine-Learning-in-Frequentist-Statistics-1"><span class="toc-number">1.0.1.4.</span> <span class="toc-text">Machine Learning in Frequentist Statistics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Generalization-Issue"><span class="toc-number">1.0.1.5.</span> <span class="toc-text">Generalization Issue</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Summary"><span class="toc-number">1.0.1.6.</span> <span class="toc-text">Summary</span></a></li></ol></li></ol></li></ol></li></ol>
                </div>
            
        
        
            <h1 id="My-Interpretation-of-Machine-Learning"><a href="#My-Interpretation-of-Machine-Learning" class="headerlink" title="My Interpretation of Machine Learning"></a>My Interpretation of Machine Learning</h1><p>통계학을 분류하는 방법으로는 여러 가지가 있지만, 다음과 같이 통계학을 분류할 수도 있다.</p>
<ul>
<li><strong>Frequentist Statistics</strong></li>
<li><strong>Bayesian Statistics</strong></li>
</ul>
<p>각 통계학에서 machine learning의 방법론은 약간씩 차이가 있는 듯 하다. 하지만, 기본적으로 각 통계학의 궁극적인 목표 중 하나는 우리가 모르는, sample space distribution을 최대한 추정하는 것이다.</p>
<p>대충, 추정하는 방법은 먼저, 우리가 수집한 데이터셋 $D$의 likelihood를 최대화 하는 분포가 진짜 sample space를 가장 유사하게 추정하는 방법이라고 가정한다.</p>
<p>다시 한번 반복하자면, 통계학의 목표중 하나는 우리가 모르는, sample space의 분포를 추정하는 것이다.</p>
<h3 id="Machine-Learning-in-Frequentist-Statistics"><a href="#Machine-Learning-in-Frequentist-Statistics" class="headerlink" title="Machine Learning in Frequentist Statistics"></a>Machine Learning in Frequentist Statistics</h3><p>Frequentist statistics에서는 많은 데이터 샘플을 뽑는 시행을 한 후, 데이터를 이용해서 모분포를 추정하는 것이라는 목표가 있다. 이것을 하기 위해 데이터셋을 바탕으로 확률 분포가 대충 어떻게 생겼을지 모델링하게 되며, 이때, 확률 분포를 모델링하는데 쓰이는 것이 바로 <strong>머신러닝</strong>이다.</p>
<p>데이터셋 $D$가 있다고 하자. 이 데이터셋에 있는 각 샘플들 $d_i$들은 어쨌든 모 분포(sample space distribution)에서 나올 확률이 어느 정도 높으니까 샘플링되어 우리 손에 들어왔을 것이다. Frequentist statistics에서는 바로 이것에 주목한다. 우리가 모은 데이터셋은 모 분포를 반영해서, 확률이 높은 애들이 많이 뽑히고 낮은 애들이 적게 뽑힐 것이다. 따라서, 우리가 모은 데이터셋이 뽑혀왔을 확률, 즉, likelihood를 최대화 하는 분포를 찾는다면, 이 분포가 모분포와 매우 유사할 것이라고 가정한다.</p>
<p>그 전에, 조건이 있다.</p>
<ol>
<li>각 데이터 샘플 $d_i$는 반드시 모든 샘플과 동일한 분포에서 샘플링되어야 한다. 즉, 서울에서 온도측정하고 북극가서 온도 측정하면 안 된다. -&gt; identical</li>
<li>각 데이터 샘플 $d_i$는 모두 독립적인 시행으로 인한 샘플링이어야 한다. 즉, 첫 번째 동전던지기가 세 번째 동전던지기에 영향을 주지 않는다. 이와 같이, 이 조건을 자동으로 만족시키는 샘플링도 있다. -&gt; independent</li>
</ol>
<p>위 두 가지 조건을 합쳐서, “각 데이터 샘플 $d_i$는 <strong>iid</strong>(Identical independent distribution) 하에서 샘플링 되어야 한다”고 말한다.</p>
<p>iid를 만족시킴으로써, 우리는 하나의 모분포를 추정하기만 하면 된다. 즉, 한 분포의 파라미터 $\theta$를 추정하기만 하면 된다.</p>
<p>iid를 만족시키는 시행으로 얻어진 데이터셋의 joint distribution, 즉, 데이터셋을 얻었을 확률을 다음과 같이 계산할 수 있다.</p>
<p>$$<br>P(D|\theta) = P(d_1, d_2,…,d_n|\theta) = P(d_1|\theta)P(d_2|\theta)\cdots P(d_n|\theta)​<br>$$</p>
<h4 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h4><p>앞서 말했듯이, frequentist statistics에서는 모분포(sample space distribution)을 추정하기 위해서, likelihood를 모델링하고 likelihood를 최대화 하는 확률분포를 계산한다. 이 확률분포가 모분포의 추정이 된다. Likelihood라고 함은, 우리의 데이터 셋이 샘플링되어왔을 확률이라고 보면 된다.</p>
<p>$$<br>\text{Likelihood Dist.} = P(d_1|\theta)P(d_2|\theta)\cdots P(d_n|\theta) = P(D|\theta)<br>$$<br>여기서 $\theta$는 모분포의 파라미터이다.</p>
<p>이 likelihood를 최대화하는 분포를 찾는다면, 즉, 모분포의 파라미터 $\theta$의 추정값 $\hat{\theta}$를 찾는다면, 이 분포가 모분포와 유사할 것이다 라는 것이다.</p>
<p>$$<br>\hat{\theta} = argmax_{\theta} P(D|\theta)​<br>$$<br>이와 같이 likelihood를 최대화 시키는 분포 파라미터 $\hat{\theta}$를 찾고, 그것을 파라미터로 하는 분포는 모분포와 가깝다는 것이 frequentist statistics에서 모분포를 추정하는 대표적인 방법이다. 이것을 MLE(Maximum likelihood estimation)라고 부른다.</p>
<p>하지만, 어떻게 최대화 시킬까? 무언가를 최대화 최소화시키는데 가장 먼저 떠오르는건 미분을 통해 극점을 찾는 것이다. 하지만, likelihood가 주어지지 않아서 미분또한 할 수 없다. 따라서 우리는 먼저 likelihood를 모델링해야한다. 이것은 잠시 후에 설명한다.</p>
<h4 id="How-to-Maximize-Likelihood"><a href="#How-to-Maximize-Likelihood" class="headerlink" title="How to Maximize Likelihood"></a>How to Maximize Likelihood</h4><p>Likelihood를 최대화하는 분포를 구한다면, 그것이 모분포와 비슷해질 것이라는 것은 알겠다. 그렇다면, 어떻게 최대화시키는지 알아야 할 것이다. </p>
<p>그에 앞서서 동전을 100번 던지는 시행을 예로 들자. 우리는 동전을 100번 던졌을 때, 앞면이 몇번 나올까에 대한 확률 분포를 추정하고 싶다. 이때, <strong>각 시행은 동전 1번 던지는게 아니라 100번 던지는게 1번의 시행이다.</strong> 우리는 시행 1회에 대한 확률을 먼저 모델링해야 한다. 이것은 binomial distribution으로 모델링할 수 있을 것이다.</p>
<p>$$<br>\text{i-th experiment} = P(d_i|\theta) = \begin{pmatrix} 100 \ n_i \end{pmatrix} \theta^{n_i} (1 - \theta)^{100-n_i}​<br>$$<br>$i$번째의 시행에서는 100번 동전을 던지고 $n_i$회의 앞면이 나왔다. 그리고 동전이 앞면이 나올 확률은 $\theta$이다.</p>
<p>모든 시행은 iid조건을 만족한다면, identical한 distribution에서 샘플링된 데이터이므로 모든 시행에서 $\theta$는 같다. 이 시행을 1000회 해서 1000개의 데이터를 모았다고 가정한다. 그럼 likelihood는 이들을 곱한 것이다.</p>
<p>$$<br>P(D|\theta) = P(d_1|\theta)P(d_2|\theta)\cdots P(d_1000|\theta)<br>$$<br>이때, likelihood는 파라미터 $\theta$에 대한 함수가 된다. Likelihood는 이처럼 분포 파라미터의 함수가 된다. 그런데, 동전던지기는 우리가 미리 잘 알고있다시피 베르누이 시행이고, 이들을 100번 던졌을때 앞면이 몇번 나올까에 대한 것은 binomial distribution을 따른다. 따라서 <strong>시행 1회를 binomial distribution으로 모델링할 수 있었지만, 일반적으로 데이터 샘플링을 모델링할때는 무슨 distribution으로  모델링을 해야 할지 알 수 없다.</strong>  이때 등장하는게 바로 Machine Learning이다. Machine Learning은 이 “시행”을 모델링하는데 사용한다. 더 나아가 likelihood를 모델링하는데 사용한다!</p>
<p>Likelihood를 모델링했다면, 이 likelihood는 우리가 추정하고자 하는 parameter인 $\theta$에 대해 미분이 가능해진다(parameter인 $\theta$를 구한다는 것은 likelihood를 추정하는 것이다. $\theta$는 likelihood를 나타내는 파라미터이기 때문이다.). 미분이 가능하다면, $\theta$에 따른 likelihood의 극점을 찾을 수 있다는 것이다. 많은 분들이 아시다시피 다음 조건을 만족할때, 극점이라고 부른다.</p>
<p>$$<br>\frac{dP(D|\theta)}{d\theta} = 0​<br>$$<br>하지만, 이 식은 극점을 가르처주지만, 그 점이 극대인지, 극소인지 가르쳐주지는 않는다. 이 것을 해결하기 위한 것이 <strong>gradient(기울기)</strong>이다.</p>
<h4 id="Negative-Log-Likelihood-NLL-amp-Gradient-Descent"><a href="#Negative-Log-Likelihood-NLL-amp-Gradient-Descent" class="headerlink" title="Negative Log Likelihood(NLL) &amp; Gradient Descent"></a>Negative Log Likelihood(NLL) &amp; Gradient Descent</h4><p>흔히, 우리는 gradient descent를 machine learning 알고리즘의 최적화 방법론으로 알고 있다. 그런데 이것이 왜 machine learning 알고리즘을 최적화 할 수 있는 것인지 알아보려고 한다.</p>
<p>likelihood를 모델링했고, 극대점을 찾아야 한다는 것도 알았다. 그런데, 단순히 미분값이 0인 $\hat{\theta}$를 찾는 것만으로는 극대인지, 극소인지 알 수 없다. 이때 사용하는 것이 gradient인데, 방법은 다음과 같다.</p>
<ol>
<li><p>일단 $\theta$를 임의로 초기화한다.</p>
</li>
<li><p>현재 $\theta$값에 대해서 likelihood를 미분해본다.</p>
</li>
<li><p>미분해서 나온 값<strong>(gradient라고 부른다)</strong>의 부호가 (+)이라는 의미는 $\theta$를 증가시키면 likelihood가 증가한다는 의미이다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303084924874.png" alt="image-20200303084924874"></p>
</li>
<li><p>반대로, gradient가 (-)부호라는 것은 $\theta$를 감소시켜야 likelihood가 증가한다는 의미이다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303084905099.png" alt="image-20200303084905099"></p>
</li>
</ol>
<p>따라서 likelihood의 완전한 maximum 지점을 찾을 수는 없더라도 현재 위치에서 어느 방향으로 가야 likelihood를 증가시키는지 알 수 있다. gradient를 이용해서 likelihood 산을 오른다는 느낌으로, gradient ascent 라는 용어가 있을 수 있겠다. 하지만, 문제가 있다. Likelihood는 확률을 데이터 샘플 수만큼 곱한 것이다. 즉, 매우매우 0에 가까운 값으로, 일반적으로 데이터 샘플 수는 1만개, 10만개가 넘어가는 경우도 많다. 이들을 다 곱하면 컴퓨터에게는 그냥 0이다. 따라서 미분을 하기도 전에 이미 likelihood는 표현조차 불가능하다.</p>
<p>이것을 해결하기 위해 likelihood에 로그를 씌워서 log likelihood를 만든다. 확률값은 0과 1 사이값이므로, log를 씌우면 상당히 절댓값이 큰 (-)값이 나올 가능성이 높다. 즉, 너무너무 작아서 컴퓨터로 표시되지 않는 문제를 해결할 수 있다. 또한, log를 씌운다고 해서 씌우기 전에 대소관계가 씌운 후에 바뀐다거나 하지 않는다. (log함수는 monotonically 증가하는 함수이다.) 다른 의미로, log likelihood를 최대화하는 $\hat{\theta}$와 likelihood를 최대화하는 $\hat{\theta}$가 같다.</p>
<p>좋은 예시로, $y=-(x-1)^2+1$를 최대화 하는 x값이나, $\text{log}[y] = \text{log}[-(x-1)^2+1]$를 최대화하는 x값은 똑같이 1이다.</p>
<p>그런데, likelihood는 확률이므로, 0과 1사이 값이다. 따라서 log를 씌우면 무조건 0보다 작거나 같다. 이 모양이 조금 이상하니까, -1를 곱해서 negative log likelihood를 도입한다. 즉 다음과 같다.</p>
<p>$$<br>\text{NLL} = -\text{log}[P(D|\theta)]​<br>$$<br>그런데, 정보 이론을 공부해보신 분이라면 어디서 많이 본 모양일 것이다. 정보 이론에서 entroy와 cross entropy라는 개념이 있다. 다음과 같다.</p>
<p>$$<br>\text{Entropy} = E_p[-\text{log} ~ p], \</p>
<p>\text{Cross Entropy} = E_p[-\text{log} ~ q]​<br>$$<br><strong>정보 이론에서 엔트로피란, 불확실성의 높고 낮음을 나타낸다.</strong> (다른 의미로 정보량이 적고 많음을 의미한다) 즉, 확률 분포 p가 uniform distribution과 같이 뭐가 샘플링될지 전혀 알 수 없을수록, 엔트로피는 증가한다. 반대로, 어느 지점에서 확률이 매우 높은(분산이 매우 작은 normal distribution을 떠올리자) 분포는 우리가 어느 정도 뭐가 나올지 알고, 여러개 샘플링해보면 데이터의 다양성이 떨어진다. 따라서 엔트로피가 감소한다.</p>
<p><strong>Cross entropy란, 어느 분포 p에 대해서 다른 분포 q의 기댓값이다.</strong> 무슨 의미냐면, p분포와 q분포가 많이 다르게 생기면 생길수록 cross entropy가 증가한다. 반면, p분포와 q분포가 비슷하게 생길수록 cross entropy가 감소한다. 이는 두 분포간의 거리를 계산한다는 KL-divergence의 개념과도 거의 유사하며, 사실상 다 이어져 있는 개념이다. KL-divergence는 실제로 entropy와 cross entropy의 합이다.</p>
<p>왜 이 개념을 말했냐면, negative log likelihood에서, 모분포를 $\mathbb{P}$라고 했을 때, 우리 손에 들어온 모든 데이터 샘플이 나왔을 확률을 모두 같다고 가정해보자. 왜 모두 같다고 해도 되냐면, 데이터 샘플에는 중복되어 샘플링 된 샘플이 많을 것이다. 동전 던지기를 10번 해서 앞면이 7번 나왔다면, 그 10번 시행에 모두 같은 비중치를 뒀지만, 다 더해보면 앞면의 비중치는 0.7, 뒷면의 비중치는 0.3으로, 많이 샘플링된 얘들은 높은 비중치를 가지게 된다. 따라서, 모든 데이터 샘플들에 비중치를 같다고 둬도, 중복된 샘플들 덕분에 실제로 비중치는 데이터 확률 분포를 반영하게 되는 것이다.</p>
<p>그렇게 되면 각 비중치 $\alpha$를 통해 다음과 같이 표현될 수 있다.</p>
<p>$$<br>\text{NLL} = \Sigma_i-\alpha\text{log}P(d_i|\theta) = [\text{예시}]: \Sigma_i[-0.1 \cdot \text{log} \theta] = -0.7 \cdot \text{log} \theta - 0.3 \cdot \text{log}(1-\theta)<br>$$<br>각 비중치 위 모양은 cross entropy와 정확히 일치한다. 참고로 $\alpha$를 곱해준다고 해서 likelihood를 최대화시키는 $\hat{\theta}$값은 변하지 않는다. $-(x-1)^2$를 최대화 시키는 x나, $-0.1*(x-1)^2$을 최대화 시키는 x는 모두 1이다. 같은 이유이다.</p>
<p>따라서, negative log likelihood는 cross entropy라고도 부른다.</p>
<p>종합해보면 다음 문장들은 모두 같은 의미이다.</p>
<ul>
<li>Likelihood를 최대화 시키는 파라미터를 구한다.</li>
<li>Log likelihood를 최대화 시키는 파라미터를 구한다.</li>
<li>Negative log likelihood를 최소화 시키는 파라미터를 구한다.</li>
<li>Cross entropy를 최소화 시키는 파라미터를 구한다.</li>
</ul>
<p><strong>그리고, likelihood를 최대화 시키기 위해서 gradient ascent를 해야 했지만, negative로 만듦으로써, 이번엔 negative likelihood를 하강해야 하므로, gradient descent를 해야 하는 것이다.</strong></p>
<p>하지만, 보다시피, gradient descent 방법으로는 theta를 증가시킬지, 감소시킬지는 가르쳐주지만, 어느정도 감소시켜야 할지, 증가시켜야 할지는 알려주지 않는다. 따라서 theta를 조금씩 증감하면서 gradient가 0이 되는 극점을 찾는 것이 gradient descent optimization이고, 극점을 찾는 과정을 training/learning(학습)이라고 부른다.</p>
<h4 id="Machine-Learning-in-Frequentist-Statistics-1"><a href="#Machine-Learning-in-Frequentist-Statistics-1" class="headerlink" title="Machine Learning in Frequentist Statistics"></a>Machine Learning in Frequentist Statistics</h4><p>앞서 말했듯이, 데이터 샘플을 뽑는 1회 시행은 우리가 알고 있는 normal distribution이나, bernoulli distribution같은 것이 아닐 가능성이 있다. 따라서 우리는 유연하게 “시행”을 모델링해야 할 필요가 있다.</p>
<p>Machine learning이라고 하면, weights $w$로 parameterize되며, 데이터 샘플 하나 $x$를 입력으로 받으면, 그 데이터 샘플이 어느 부류인지에 대한 확률 $P(x|w)$을 계산한다. 즉, machine learning이라는 이름의 방법론 속에 숨어있는 확률 분포로부터 입력으로 넣은 데이터 샘플이 뽑힐 확률을 계산하는 것이다.</p>
<p>이들을 모든 데이터 샘플들에 반복해서 모두 곱하면,</p>
<p>$$<br>P(x_1|w)P(x_2|w)\cdots P(x_n|w) = P(X|w)​<br>$$<br>즉,  likelihood를 모델링한 것이다. machine learning은 데이터 샘플 1개가 샘플링되는 모 확률 분포라고 가정하고, 그 모 확률분포를 모델링한 것에 지나지 않는다.</p>
<h4 id="Generalization-Issue"><a href="#Generalization-Issue" class="headerlink" title="Generalization Issue"></a>Generalization Issue</h4><p>머신 러닝으로 데이터 샘플들의 확률 분포를 모델링했다(말 그대로 모델링한 것이지, 진짜 모 확률 분포가 아니다. 추정일 뿐이다.). 그런데, 우리는 수집된 데이터 셋만을 이용해서 모 확률분포를 추정했는데, 우리가 모은 샘플들이 모 분포에서 나올 수 있는 모든 샘플들을 포함할까? 절대 아니다. 그럼, 샘플링되지 않은 놈들이 있을 수 있는데, 이들에 대해서도 잘 작동하는지는 어떻게 보장하나?</p>
<p>지금 우리는 frequentist statistics에서의 machine learning을 말하고 있다. 답은 <strong>frequentist statistics</strong>에 있다. Frequentist statistics에서는 가능한 많은 데이터 샘플을 뽑고 분포를 추정하게 된다. 여기서, <strong>“가능한 많은 데이터 샘플”</strong>이 핵심이다. 가능한 많은 데이터 샘플들을 뽑게 되면, 어쨌든 샘플링될 확률이 높은 데이터샘플은 많이 뽑힐 것이고, 적은 확률로 샘플링된 녀석들도 적은 개수나마 샘플링될 것이다. 즉, 데이터셋을 바탕으로 확률 분포를 모델링할때, 실제로 높은 확률을 가지는 샘플은 수가 많아서 모델링된 분포에도 높은 확률을 가질 것이고, 실제로 낮은 확률을 가지는 샘플은 수가 적어서 모델링된 분포에도 낮은 확률을 가질 것이다. 즉, 데이터 샘플수가 충분히 많다면, 모 확률분포와 매우매우 유사해진다. 그래서 머신러닝에서 <strong>“데이터셋을 많이 모아라~”</strong> 하는 것이다.</p>
<p>또한, 데이터 샘플들의 수가 많아질수록 매우 다양한 샘플들이 샘플링되어 있을 것이며, 이들 만으로도 충분히 모 확률분포에서 나올 수 있는 샘플들을 커버할 수 있다는 것이다. 따라서, 데이터 샘플 수가 적당히 많다면, 이들을 이용하면 모 확률분포와 매우 유사하게 모델링이 가능하고, 그럼, 미처 샘플링되지 못한 샘플들에 대해서도 잘 작동할 것이라는 이론이 있다.</p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>정리해보면, frequentist statistics에서의 machine learning이란, 일단은 <strong>함수</strong>이다. 그런데, 확률값을 반환하는 <strong>확률 함수</strong>이다. 좀 더 정확히 말해보면 데이터 샘플들의 진짜 모 확률분포를 모델링한, <strong>확률분포</strong>이다.</p>
<p>Machine learning은 parameter(weight)를 이용해서 이 확률 분포를 모델링하고, likelihood를 모델링한다. Likelihood를 최대화하는 parameter를 계산(정확히는 “추정”)한다. 이때, gradient descent를 이용한다. parameter를 계산하는 과정을 학습이라고 한다. 학습이 끝나면, machine learning은 모분포를 잘 추정한다고 가정하며, 일반화도 잘 이루어 졌을 것이라고 가정한다.</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/wiki/Blog-Init/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">다음 글</strong>
            <div class="article-nav-title">
                
                    Init Blog
                
            </div>
        </a>
    
    
</nav>





    
    

    <script src="https://utteranc.es/client.js"
        repo="taeuk-gang/taeuk-gang.github.io"
        issue-term="title"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>



<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Lee Jaeyoung &copy; 2020 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <!-- <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a> -->
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>