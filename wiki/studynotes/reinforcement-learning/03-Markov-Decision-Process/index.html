<!DOCTYPE html>
<html lang="ko">

<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png">
    

    <title>
        
          03. Markov Decision Process - Jaeyoung&#39;s Blog
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    <link rel="stylesheet" href="/css/book.css">
    <script src="/js/book.js"></script>

    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

</head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>JAEYOUNG&#39;S BLOG</span>
  </a>
</div>
    <div class="book-menu">
  <ul>
<li><a href="/"><strong>Home</strong></a></li>
</ul>
<h1 id="Study-Notes"><strong>Study Notes</strong></h1>
<h2 id="Machine-Learning">Machine Learning</h2>
<ul>
<li><a href="/wiki/studynotes/machine-learning/Machine-Learning">Machine Learning</a></li>
<li><a href="/wiki/studynotes/machine-learning/KL-Divergence">KL Divergence</a></li>
<li><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis">Principal Component Analysis</a></li>
<li><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st">Restrict Boltzmann Machine 1</a></li>
<li><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd">Restrict Boltzmann Machine 2</a></li>
<li><a href="/wiki/studynotes/machine-learning/Lagrangian-Multiplier">Lagrangian Multiplier</a></li>
<li><a href="/wiki/studynotes/machine-learning/Hidden-Markov-Models-1">Hidden Markov Models 1</a></li>
<li><a href="/wiki/studynotes/machine-learning/Hidden-Markov-Models-2">Hidden Markov Models 2</a></li>
</ul>
<h1 id="Bayesian-Statistics"><strong>Bayesian Statistics</strong></h1>
<ul>
<li><a href="/wiki/studynotes/machine-learning/01_Probability">01. Probability</a></li>
<li><a href="/wiki/studynotes/machine-learning/02_Distribution">02. Distribution</a></li>
<li><a href="/wiki/studynotes/machine-learning/03_Frequentist_Inference">03. Frequentist Inference</a></li>
<li><a href="/wiki/studynotes/machine-learning/04_Bayesian_Inference">04. Bayesian Inference</a></li>
<li><a href="/wiki/studynotes/machine-learning/05_Credible_Intervals">05. Credible Intervals</a></li>
</ul>
<h1 id="Reinforcement-Learning"><strong>Reinforcement Learning</strong></h1>
<h1 id="Extra"><strong>Extra</strong></h1>
<ul>
<li><a href="/wiki/Blog-Init">Blog Init</a></li>
</ul>

</div>

<script src="/js/book-menu.js"></script>
  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1>Markov Decision Process</h1>
<p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p>
<p>마르코드 결정 과정.</p>
<p>이름에서 유추할수 있다시피, Markov assumption에 기반한 decision process로, $t+1$에서의 상태 $s_{t+1}$는 오직 현재 $t$에서의 상태인 $s_{t}$에 있을 때, agent의 decision인 $a_{t}$에 의해서만 결정된다는 것이다.</p>
<h3 id="Markov-Decisoin-Process-vs-K-arm-Bandit-Problems">Markov Decisoin Process vs K-arm Bandit Problems</h3>
<p>Markov decision process에서는 K-arm bandit에서 가정했던 여러가지 조건을 해제한 application에 적용이 가능하다.</p>
<ul>
<li>K-arm bandit problem에서는 항상 하나의 state만 존재했지만, Markov decision process는 액션을 취함에 따라 state가 변하는 application에도 적용이 가능하다.</li>
<li>K-arm bandit problem에는 언제나 선택할 수 있는 action 리스트가 고정되어 있었다. 하지만, Markov decision process가 적용되는 application은 그럴 필요가 없다. 각 state에 서로 다른 action list가 있을 수 있다.</li>
<li>K-arm bandit problem에는 state와 선택 가능한 액션 리스트가 하나임과 동시에 매 time마다 optimal action은 항상 고정되어 있었다. 하지만, 이번에 이야기할 reinforcement learning environment는 state마다 optimal action이 다를 수 있다.</li>
</ul>
<h3 id="Finite-Markov-Decision-Process">Finite Markov Decision Process</h3>
<p>Agent와 상호작용하는 environment에는 여러 state가 있을 수 있는데, 이 state의 개수가 finite 하며, 각 state에서 존재하는 action 개수도 finite한 경우에 적용되는 Markov decision process를 finite Markov decision process라고 한다. 물론 finite 하지 않는 경우가 매우 많다.</p>
<p>현재 상태를 $s$, 이 상태를 기준으로 내린 decision $a$, 그리고, 그 결정에 의해 변한 상태를 $s’$, 그로인해 받는 reward를 $r$라고 했을 때, Markov decision process의 <strong>state transition probability</strong>는 다음과 같다.<br>
$$<br>
p(s’,r|s,a)<br>
$$</p>
<h3 id="Episodic-Tasks-vs-Continuous-Tasks">Episodic Tasks vs Continuous Tasks</h3>
<ul>
<li>
<p><strong>Episodic Tasks</strong></p>
<p>바둑, 스타크래프트와 같은 게임처럼, &quot;한번의 판(한 판), stage&quot;이 존재하는 problem을 가리킨다. 따라서, terminal state 라는 것이 존재하며, 하나의 stage를 시작해서 끝난 후 최종 reward까지 받을 때 까지를 하나의 episode라고 부른다. Agent는 여러 episode를 체험해보면서 학습하게 된다. 한 episode에서 이런 선택을 했다면 다음 episode에서 다른 선택을 하면서 다른 결말 및 reward를 획득하면서 학습하게 되는 task이다.</p>
<p>Episode는 이전의 모든 episode와 독립적이다. 즉, 이전 episode가 어떻게 끝났던 간에, 현재 episode는 이전 episode에 의해 영향을 받지 않는다. 매 게임이 독립이라는 이야기이다.</p>
</li>
<li>
<p><strong>Continuous Tasks</strong></p>
<p>일반적인 로봇이 수행하는 작업들이 보통 continuous task이다. 이 경우, terminal state가 없으며, 그냥 life를 살아가면서 마주치는 state에서 action을 수행하면서 학습을 진행하게 된다.</p>
</li>
</ul>
<h2 id="Goals-of-MDP">Goals of MDP</h2>
<p>MDP의 목적은 당장 action을 선택했을 때의 reward를 최대화 하는 것이 아닌, 현재 어떤 action을 선택한 후, 미래의 모든 reward들 합의 기댓값을 최대하하도록 하는 action을 선택하는 것이다. 즉, 다음과 같은 action $a_t^* $를 선택한다.<br>
$$<br>
a_t^* = \underset{a}{\text{argmax} } ~ \mathbb{E}[G_t] = \underset{a}{\text{argmax}} ~ \mathbb{E}[R_{t+1} + \cdots + R_T]<br>
$$<br>
이때, $T$는 final state에서의 time 이다. 즉, 한 episode의 끝일때의 time이다.</p>
<p>$G_t$는 random variable인데, $R_t$들이 random variable이고, random variable의 합이기 때문이다. 따라서, random variable $G_t$의 기댓값을 최대화하는 action $a$를 선택하도록 한다.</p>
<h3 id="Goals-of-MDP-for-Continuous-Tasks">Goals of MDP for Continuous Tasks</h3>
<p>위에서 소개한 action 선택법은 episodic task에만 적용이 가능하다. 미래의 모든 reward의 합의 기댓값이므로, terminal state가 존재해야 $\mathbb{E}[G_t]$가 finite($\infty$가 아님)하다. continuous task의 경우에는, $R_T$가 없고 무한히 더해지기 때문에, $\mathbb{E}[G_t] \approx \infty$가 된다. 따라서 <strong>discounting</strong>이라는 것을 통해 액션을 선택한다.<br>
$$<br>
a_t^* = \underset{a}{\text{argmax} } ~ G_t = \underset{a}{\text{argmax} } ~ [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots] = \underset{a}{\text{argmax} } ~ [R_{t+1} + \gamma G_{t+1} ]<br>
$$<br>
Discounting을 하는 이유는 $G_t$를 finite하게 만들기 위함이며, 다음과 같기 때문에 finite하다. 이때, $0 \leq \gamma &lt; 1$이어야 한다. $R_{max}$를 agent가 한 액션을 취했을때 얻을 수 있는 액션의 최대치라고 하자.<br>
$$<br>
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots<br>
$$<br>
$$<br>
G_t \leq R_{max} + \gamma R_{max} + \gamma^2 R_{max} + \cdots<br>
$$</p>
<p>$$<br>
G_t \leq R_{max}(1 + \gamma + \gamma^2 + \cdots)<br>
$$</p>
<p>$$<br>
G_t \leq R_{max} \cdot \frac{1}{1 - \gamma} ~ \text{iff } 0 \leq \gamma &lt; 1<br>
$$</p>
<p>따라서, $0 \leq \gamma &lt; 1$을 만족하면, $G_t$는 $R_{max} \cdot \frac{1}{1 - \gamma}$보다 작다. 그리고, finite하다($\infty$가 아니다).</p>
<p>굳이 episodic task라고 해서 discounting을 사용하지 말라는 법은 없다. discount rate $$\gamma$$를 통해 미래 reward 지향적일지, 즉각적인 reward 지향적일지 정할 수있기 때문에 discounting 방법은 episodic task에서도 많이 이용된다.</p>
<h2 id="Summary">Summary</h2>
<p>MDP란, 현재 상태만을 바탕으로 action을 취하고 reward를 얻는 환경에서의 reinforcement learning 방법 또는 decision process중 하나이다. 액션은 다음과 같이 취한다.<br>
$$<br>
a^* (t) = \underset{a(t)}{\text{argmax} } ~ \mathbb{E}[G_t] = \underset{a(t)}{\text{argmax} } ~ \mathbb{E}[R_{t+1} + \gamma \cdot G_{t+1} ]<br>
$$</p>

</div>


  <div class="book-comments">
    
<script src="https://utteranc.es/client.js"
        repo="wayexists02/wayexists02.github.io"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>





  </div>


<script src="/js/book-post.js"></script>
        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure class="avatar avatar-lg">
          <img src="/log.png" alt="...">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Lee Jaeyoung</div>
      <div>2020-03-03</div>
    </div>
  </div>

  
    <div class="divider"></div>

    <div class="link">
      <a class="category-link" href="/categories/Study-Notes/">Study Notes</a> <a class="category-link" href="/categories/Study-Notes/Reinforcement-Learning/">Reinforcement Learning</a>

      <a class="tag-link" href="/tags/ReinforcementLearning/">#ReinforcementLearning</a> <a class="tag-link" href="/tags/StudyNotes/">#StudyNotes</a>
    </div>
    
  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>

<script src="/js/book-toc.js"></script>
</div>
      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>

<script src="/js/book.js"></script>