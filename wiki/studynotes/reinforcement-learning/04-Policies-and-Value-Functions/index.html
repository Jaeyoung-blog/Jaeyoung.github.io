<!DOCTYPE html>
<html lang="ko">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>04. Policies and Value Functions | Jaeyoung&#39;s Blog</title>
    
    
        <meta name="keywords" content="StudyNotes,ReinforcementLearning">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Policies and Value Functions참고: Coursera Reinforcement Learning (Alberta Univ.) RL에서의 학습이란, action을 결정하는 policy와 value function을 추정하는 과정이라고 볼 수 있을 만큼, 두 과정의 RL에의 영향은 절대적이다. Policies우리말로, 정책이라고 하며, 말 그">
<meta name="keywords" content="StudyNotes,ReinforcementLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="04. Policies and Value Functions">
<meta property="og:url" content="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/index.html">
<meta property="og:site_name" content="Jaeyoung&#39;s Blog">
<meta property="og:description" content="Policies and Value Functions참고: Coursera Reinforcement Learning (Alberta Univ.) RL에서의 학습이란, action을 결정하는 policy와 value function을 추정하는 과정이라고 볼 수 있을 만큼, 두 과정의 RL에의 영향은 절대적이다. Policies우리말로, 정책이라고 하며, 말 그">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://jaeyoung-blog.github.io/GoogleDrive/Notes/note-images/04-Policies-and-Value-Functions/image-20200119160231033.png">
<meta property="og:image" content="https://jaeyoung-blog.github.io/GoogleDrive/Notes/note-images/04-Policies-and-Value-Functions/image-20200119163026023.png">
<meta property="og:updated_time" content="2020-03-05T03:35:46.791Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="04. Policies and Value Functions">
<meta name="twitter:description" content="Policies and Value Functions참고: Coursera Reinforcement Learning (Alberta Univ.) RL에서의 학습이란, action을 결정하는 policy와 value function을 추정하는 과정이라고 볼 수 있을 만큼, 두 과정의 RL에의 영향은 절대적이다. Policies우리말로, 정책이라고 하며, 말 그">
<meta name="twitter:image" content="https://jaeyoung-blog.github.io/GoogleDrive/Notes/note-images/04-Policies-and-Value-Functions/image-20200119160231033.png">
    

    
        <link rel="alternate" href="/atom.xml" title="Jaeyoung&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">
    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Jaeyoung&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Main</a>
                
                    <a class="main-nav-link" href="/archives">TimeLine</a>
                
                    <a class="main-nav-link" href="/categories">Category</a>
                
                    <a class="main-nav-link" href="/tags">Tag</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '포스트',
            PAGES: 'Pages',
            CATEGORIES: '카테고리',
            TAGS: '태그',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Main</a></td>
                
                    <td><a class="main-nav-link" href="/archives">TimeLine</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Category</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tag</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>카테고리</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Log
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/Blog-Init/">Init Blog</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Study Notes
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Bayesian Statistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/01_Probability/">01. Probability</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/02_Distribution/">02. Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/">03. Frequentist Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/">04. Bayesian Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/">05. Credible Intervals</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/">06. Prior Predictive Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/07_Priors/">07. Priors</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/">08. Bayesian Modeling</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/">09. Monte Carlo Estimation</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/">10. Markov Chain Monte Carlo</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/11_Linear_Regression/">11. Linear Regression</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/">12. Prior Sensitivity Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/">13. Hierarchical Models</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/">14. Predictive Simulations</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/">Appendix 1. Maximize a Posterior</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/">Appendix 2. Empirical Bayes</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Machine Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/">Machine Learning</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis/">Principal Component Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Distillation-Methods/">Distillation Methods</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/">Hidden Markov Models 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/">Hidden Markov Models 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/KL Divergence/">KL Divergence</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Reinforcement Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/01_Introduction/">01. Introduction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/">02. K-arm Bandits Problems</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/">03. Markov Decision Process</a></li>  <li class="file active"><a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">04. Policies and Value Functions</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">05. Policy Evaluation & Control</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/">06. Sample-based Reinforcement Learning</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-studynotes/reinforcement-learning/04-Policies-and-Value-Functions" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/Study-Notes/">Study Notes</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Study-Notes/Reinforcement-Learning/">Reinforcement Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/ReinforcementLearning/">ReinforcementLearning</a>, <a class="tag-link" href="/tags/StudyNotes/">StudyNotes</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">
            <time datetime="2020-03-03T01:00:03.000Z" itemprop="datePublished">2020-03-03</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/raw/writing/source/_posts/studynotes/reinforcement-learning/04-Policies-and-Value-Functions.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/edit/writing/source/_posts/studynotes/reinforcement-learning/04-Policies-and-Value-Functions.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/commits/writing/source/_posts/studynotes/reinforcement-learning/04-Policies-and-Value-Functions.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            04. Policies and Value Functions
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">카탈로그</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Policies-and-Value-Functions"><span class="toc-number">1.</span> <span class="toc-text">Policies and Value Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Policies"><span class="toc-number">1.0.1.</span> <span class="toc-text">Policies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Valid-amp-Invalid-Policies"><span class="toc-number">1.0.2.</span> <span class="toc-text">Valid &amp; Invalid Policies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-Functions"><span class="toc-number">1.0.3.</span> <span class="toc-text">Value Functions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bellman-Equation"><span class="toc-number">1.1.</span> <span class="toc-text">Bellman Equation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bellman-Equation-vs-Value-Function"><span class="toc-number">1.1.1.</span> <span class="toc-text">Bellman Equation vs Value Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#State-value-Bellman-Equation"><span class="toc-number">1.1.2.</span> <span class="toc-text">State-value Bellman Equation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Action-value-Bellman-Equation"><span class="toc-number">1.1.3.</span> <span class="toc-text">Action-value Bellman Equation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Compute-Value-using-Bellman-Equation"><span class="toc-number">1.1.4.</span> <span class="toc-text">Compute Value using Bellman Equation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimality"><span class="toc-number">1.2.</span> <span class="toc-text">Optimality</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimal-Policies"><span class="toc-number">1.2.1.</span> <span class="toc-text">Optimal Policies</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimal-Values"><span class="toc-number">1.2.2.</span> <span class="toc-text">Optimal Values</span></a></li></ol></li></ol></li></ol>
                </div>
            
        
        
            <h1 id="Policies-and-Value-Functions"><a href="#Policies-and-Value-Functions" class="headerlink" title="Policies and Value Functions"></a>Policies and Value Functions</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p>
<p>RL에서의 학습이란, action을 결정하는 policy와 value function을 추정하는 과정이라고 볼 수 있을 만큼, 두 과정의 RL에의 영향은 절대적이다.</p>
<h3 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h3><p>우리말로, <strong>정책</strong>이라고 하며, 말 그대로, 주어진 현재 상황에서 agent가 어떤 action을 선택할지 결정해주는 정책을 의미한다.</p>
<p><strong>Policy는 각 state에서 가능한 action들의 probability distribution이다.</strong></p>
<p>Value function과 그 의미가 매우 유사해보인다. 하지만, Value function은 reward의 기댓값을 계산해주는 것일 뿐, action을 결정하는 것은 policy에게 달려 있다.</p>
<ul>
<li><p>Deterministic Policies<br>$$<br>\pi(s)= a<br>$$<br>위 식처럼, deterministic policy란 state를 입력으로 받아서 action 1개를 반환하는 함수이다. 즉, state를 action으로 매핑하는 함수라고 볼 수 있다.</p>
</li>
<li><p>Stochastic Policies<br>$$<br>\pi(a|s) = \text{ {Probability distribution matrix} }<br>$$<br>Stochastic policy는 한 state를 주게 되면, 그 state에서의 action probability distribution을 반환하는 함수이다. 따라서, 한 state를 주면, 여러 action이 나오게 된다.</p>
<p>이때, action probability distribution matrix는 다음을 만족해야 한다.<br>$$<br>\sum_a \pi(a|s) = 1<br>$$<br>$$<br>\pi(a|s) \geq 0<br>$$</p>
<p>즉, 행렬의 각 row가 적법한 probability distribution이어야 한다.</p>
</li>
</ul>
<p>action probability distribution은 각 state마다 다르다.</p>
<h3 id="Valid-amp-Invalid-Policies"><a href="#Valid-amp-Invalid-Policies" class="headerlink" title="Valid &amp; Invalid Policies"></a>Valid &amp; Invalid Policies</h3><p>MDP에서 policies는 반드시 현재 타임에서의 상태에 존재하는 정보를 이용해서 action을 결정해야 한다. 즉, policies의 인자는 반드시 time $t$에서는 $s_t$만이 되어야 하며, 따라서 다음과 같다.<br>$$<br>\pi(s_t) \rightarrow a_t<br>$$<br>만약, policy가 이전 time의 state나 action을 이용해서 action을 결정한다면, 적법한 policy라고 부르지 않는다.</p>
<h3 id="Value-Functions"><a href="#Value-Functions" class="headerlink" title="Value Functions"></a>Value Functions</h3><p>다음 두 가지로 나뉜다.</p>
<ul>
<li><p>State Value Function $v_{\pi}(s_t)$</p>
<p>한 상태 $s_t$에서, 앞으로 어떤 policy $\pi$를 따른다고 했을 때, <strong>앞으로 얻을 수 있는</strong> reward 기댓값을 의미한다.</p>
</li>
</ul>
<p>$$<br>v_{\pi}(s_t) = \sum_{a_{t} } \pi(a_t|s_t) \sum_{s_{t+1},r_{t+1} } p(s_{t+1},r_{t+1}|s_{t},a_t)[r_{t+1} + \gamma \cdot v_{\pi}(s_{t+1})]<br>$$</p>
<ul>
<li><p>Action Value Function $q_{\pi}(s_t, a_t)$</p>
<p>한 상태 $s_t$에서, 어떤 액션 $a_t$를 취하고 난 후, 앞으로 어떤 policy $\pi$를 따른다고 했을 때, <strong>앞으로 얻을 수 있는</strong> reward의 기댓값을 의미한다.</p>
</li>
</ul>
<p>$$<br>  q_{\pi} (s_t, a_t) = \sum_{s_{t+1}, r_{t+1}} p(s_{t+1}, r_{t+1}|s_t, a_t) [r_{t+1} + \gamma \cdot \sum_{a_{t+1} } \pi(a_{t+1}|s_{t+1})q_{\pi}(s_{t+1}, a_{t+1})]<br>$$</p>
<p>둘 다 현재 어떤 상황에서 앞으로 얻을 수 있는 reward의 기댓값을 의미한다.</p>
<h2 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h2><p><strong>현재 시간 $t$에서의 value와 다음 시간 $t+1$에서의 value와의 관계식</strong>을 의미한다. State-value Bellman equation과 action-value Bellman equation이 존재하며, reinforcement learning 알고리즘 구현에 있어서 가장 중요한 알고리즘 중 하나이다.</p>
<h3 id="Bellman-Equation-vs-Value-Function"><a href="#Bellman-Equation-vs-Value-Function" class="headerlink" title="Bellman Equation vs Value Function"></a>Bellman Equation vs Value Function</h3><p>두 용어의 개념에 대한 차이는 거의 없다. Bellman equation도 value function이다. 다만,  $t$에서의 value function을 $t+1$에서의 value function에 대한 식으로 나타냈다 뿐. Recursive하게 표현한 value function을 Bellman equation이라고 부를 뿐이다.</p>
<h3 id="State-value-Bellman-Equation"><a href="#State-value-Bellman-Equation" class="headerlink" title="State-value Bellman Equation"></a>State-value Bellman Equation</h3><p>State-value function에 대한 Bellman equation으로, state-value function입장에서, $t$에서의 state value와 $t+1$에서의 state value와의 관계식이다.<br>$$<br>v_{\pi} (s_t) = \sum_a \pi(a|s_t) \sum_{s_{t+1},r} p(s_{t+1}, r|s_{t}, a)[r + \gamma \cdot v_{\pi}(s_{t+1})]<br>$$</p>
<h3 id="Action-value-Bellman-Equation"><a href="#Action-value-Bellman-Equation" class="headerlink" title="Action-value Bellman Equation"></a>Action-value Bellman Equation</h3><p>Action-value function에 대한 Bellman equation으로, action-value funciton입장에서, $t$에서의 actionvalue와 $t+1$에서의 action value와의 관계식이다.<br>$$<br>q_{\pi} (s_{t}, a_{t}) = \sum_{s_{t+1}, r} p(s_{t+1}, r|s_t, a_t)[r + \gamma \cdot \sum_{a_{t+1} } \pi(a_{t+1}|s_{t+1}) \cdot q_{\pi}(s_{t+1}, a_{t+1})]<br>$$</p>
<h3 id="Compute-Value-using-Bellman-Equation"><a href="#Compute-Value-using-Bellman-Equation" class="headerlink" title="Compute Value using Bellman Equation"></a>Compute Value using Bellman Equation</h3><p>Bellman equation의 가장 큰 장점은, value function을 매우 효율적으로 계산할 수 있게 해 준다는 것이다. Value function은 정의에서 보다시피, 미래의 reward의 기댓값이다. 즉, 현재 시간 $t$이후의 모든 시간에서의 reward 기댓값인데, 이 정의로는 value를 계산할 수 없다. Bellman equation은 이 무한 수열 계산문제를 단순한 linear equation으로 바꿔준다.</p>
<p>다음 board를 생각해 보자.</p>
<p><img src="../../../../../../../GoogleDrive/Notes/note-images/04-Policies-and-Value-Functions/image-20200119160231033.png" alt="image-20200119160231033"></p>
<p>보드에는 $A,B,C,D$라는 4개의 공간이 있으며, 말 하나를 이 공간 내에서 움직이려 한다. 즉, 각 공간이 곧 state이며, 총 4개의 state가 있는 environment이다.</p>
<p>Action은 상,하,좌,우 4개의 움직임이 존재한다. Policy는 총 4개의 움직임에 대해 uniform distribution이다. 말이 $B$$로 들어오거나 B$에 머무는 움직임에 대해서만 reward +5를 부여하고 나머지는 0을 부여한다. Discount factor는 0.7로 하자.</p>
<p>State $A$에서의 value는 무한 수열식이지만, Bellman equation을 이용한다면, 다음 state의 value를 이용해서 계산이 가능하다.<br>$$<br>V(A) = \sum_{a} \pi(a|A) \sum_{s’,r} p(s’,r|a,A)[r + \gamma \cdot V(s’)]<br>$$<br>그런데, action이 정해지면, state는 확정(deterministic)이므로, 위 Bellman equation을 다음처럼 변경할 수 있다.<br>$$<br>V(A) = \sum_a \pi(a|A) [0 + 0.7 \cdot V(s’)]<br>$$<br>$$</p>
<p>V(A) = \frac{1}{4} \cdot 0.7 \cdot V(C) + \frac{1}{2} \cdot 0.7 \cdot V(A) + \frac{1}{4} \cdot (5 + 0.7 \cdot V(B)))<br>$$</p>
<p>$V(B), V(C), V(D)$도 유사하게 $V(A), V(B), V(C), V(D)$에 대한 식으로 표현이 가능하며, 일차 연립방정식으롤 표현이 가능하다. 즉, 무한 수열을 푸는 문제가 일차 연립 방정식을 푸는 문제로 바뀐 것이다.</p>
<p>하지만, 현실에서는 approximation방법을 많이 이용한다. State개수가 많아서 그런가?</p>
<h2 id="Optimality"><a href="#Optimality" class="headerlink" title="Optimality"></a>Optimality</h2><p>Reinforcement learning의 목적은 단순히 value function과 policy를 계산하는게 아니라, optimal policy와 optimal value function, 즉, reward를 최대화하는 policy와 value function을 찾는 것이다.</p>
<h3 id="Optimal-Policies"><a href="#Optimal-Policies" class="headerlink" title="Optimal Policies"></a>Optimal Policies</h3><p>Optimal policy란, 모든 state에서 가장 높은 value를 반환하게 하는 policy를 말한다. 즉, 다음 그림처럼 어떤 여러개의 policies들보다 항상 큰 value를 반환하게 하는 policy는 항상 존재한다.</p>
<p><img src="../../../../../../../GoogleDrive/Notes/note-images/04-Policies-and-Value-Functions/image-20200119163026023.png" alt="image-20200119163026023"></p>
<p>즉, $\pi_1, \pi_2$보다 항상 크거나 같은 value를 반환하는 policy는 항상 존재한다는 건데, 방법은 간단하다. $\pi_1 \leq \pi_2$인 state에서는 $\pi_2$의 policy를 따르고, $\pi_1 &gt; \pi_2$인 state에서는 policy $\pi_1$을 따르도록 하는 새로운 policy $\pi_3$를 만들면 된다.</p>
<p>이와 같은 방법으로, 언제나 모든 policy보다 크거나 같은 value를 반환하는 policy를 만들 수 있으며, 이런 policy는  unique할 수도 있고 여러개가 될 수도 있다(모든 state에 걸쳐서 똑같은 value를 반환하는 policy가 여러개일 수 있음).</p>
<p>어쨌든, 이런 과정을 거쳐서 가장 높은 value를 모든 state에 걸쳐서 반환하는 policy를 optimal policy라고 부른다. 방금 말했듯이, optimal policy는 반드시 존재하며, 여러개일 수 있다.</p>
<p>또 하나 생각할 점은, 위 그림에서 $\pi_3$은 분명히, $\pi_1, \pi_2$중 하나를 선택한 policy에 불과하므로, $\pi_1, \pi_2$둘 중 value가 높은 policy의 value와 같아야 할 것이데, 어느 지점에서는 $\pi_1, \pi_2$ 모두의 value보다 높다. 이것은, future value까지 반영해서 생기는 현상으로, 미래 state에서도 최선 policy인 $\pi_3$을 따르므로, value는 재료가 된 policy들보다 커질수도 있다.</p>
<h3 id="Optimal-Values"><a href="#Optimal-Values" class="headerlink" title="Optimal Values"></a>Optimal Values</h3><p>보통 optimal policy는 unknown으로, 바로 계산할 수 없다. 애초에 reinforcement learning의 목적은 optimal policy를 찾는 것이다. Optimal policy를 계산할때는 opimal value function를 이용하게 된다.</p>
<p>Optimal value function이란, 현재 state에서 가능한 모든 액션과 그에 다른 다음 value를 보고, 다음 value가 가장 높은 action을 deterministic하게 선택했을 때의 value function을 의미한다.<br>$$<br>v_* (s) = \underset{a}{\text{max} } ~ \sum_{s’,r} p(s’,r|s,a)[r + \gamma \cdot v_<em>(s’)]<br>$$<br>보다시피, action의 분포(policy)가 사라지고, 그냥 다음 state인 $s’$의 value $v_</em> (s’)$가 가장 높은 action을 무조건(deterministically) 취하게 한다. 또한, 이 value $v_<em>(s’)$만으로 $v_</em> (s)$를 계산하도록 한다.</p>
<p>앞서, value function은 두 가지가 있고, 두 가지 value function 모두 Bellman equation 형태로 바꿀 수 있었다. Optimal value function도 마찬가지이며, optimal한 value function을 Bellman equation형태로 바꾼 것을 Bellman optimality equation이라고 부른다.</p>
<ul>
<li><strong>Bellman optimality equation for state value function</strong></li>
</ul>
<p>$$<br>  v_* (s) = \underset{a}{\text{max} } ~ \sum_{s’,r} p(s’,r|s,a) [r + \gamma \cdot v_* (s’)]<br>$$</p>
<ul>
<li><strong>Bellman optimality equation for action value function</strong></li>
</ul>
<p>$$<br>  q_* (s,a) = \underset{a}{\text{max} } \sum_{s’,r} p(s’,r|s,a)[r + \gamma \cdot \underset{a’}{\text{max} } ~ q_* (s’,a’)]<br>$$</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">다음 글</strong>
            <div class="article-nav-title">
                
                    05. Policy Evaluation &amp; Control
                
            </div>
        </a>
    
    
        <a href="/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">이전 글</strong>
            <div class="article-nav-title">03. Markov Decision Process</div>
        </a>
    
</nav>





    
    

    <script src="https://utteranc.es/client.js"
        repo="taeuk-gang/taeuk-gang.github.io"
        issue-term="title"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>



<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Lee Jaeyoung &copy; 2020 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <!-- <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a> -->
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>