<!DOCTYPE html>
<html lang="ko">

<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, user-scalable=no">

    
      <link rel="icon" href="/favicon.png">
    

    <title>
        
          06. Sample-based Reinforcement Learning - Jaeyoung&#39;s Blog
        
    </title>

    <!-- Spectre.css framework -->
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-exp.min.css">
    <link rel="stylesheet" href="https://unpkg.com/spectre.css/dist/spectre-icons.min.css">

    <!-- theme css & js -->
    <link rel="stylesheet" href="/css/book.css">
    <script src="/js/book.js"></script>

    <!-- tocbot -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.4.2/tocbot.css">
    
    <!-- katex -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">

    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/zooming/2.1.1/zooming.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    const zooming = new Zooming()
    zooming.listen('.book-content img')
})
</script>

</head>

<body>

<div class="book-container">
  <div class="book-sidebar">
    <div class="book-brand">
  <a href="/">
    <img src="/favicon.png">
    <span>JAEYOUNG&#39;S BLOG</span>
  </a>
</div>
    <div class="book-menu">
  <ul>
<li><a href="/"><strong>Home</strong></a></li>
</ul>
<h1 id="Study-Notes"><strong>Study Notes</strong></h1>
<h2 id="Machine-Learning">Machine Learning</h2>
<ul>
<li><a href="/wiki/studynotes/machine-learning/My-Interpretation-of-Machine-Learning">Machine Learning</a></li>
<li><a href="/wiki/studynotes/machine-learning/KL-Divergence">KL Divergence</a></li>
<li><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis">Principal Component Analysis</a></li>
<li><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machine-1">Restrict Boltzmann Machine 1</a></li>
<li><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machine-2">Restrict Boltzmann Machine 2</a></li>
</ul>

</div>

<script src="/js/book-menu.js"></script>
  </div>

  <div class="sidebar-toggle" onclick="sidebar_toggle()" onmouseover="add_inner()" onmouseleave="remove_inner()">
  <div class="sidebar-toggle-inner"></div>
</div>

<script>
function add_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.add('show')  
}

function remove_inner() {
  let inner = document.querySelector('.sidebar-toggle-inner')
  inner.classList.remove('show')
}

function sidebar_toggle() {
    let sidebar_toggle = document.querySelector('.sidebar-toggle')
    let sidebar = document.querySelector('.book-sidebar')
    let content = document.querySelector('.off-canvas-content')
    if (sidebar_toggle.classList.contains('extend')) { // show
        sidebar_toggle.classList.remove('extend')
        sidebar.classList.remove('hide')
        content.classList.remove('extend')
    }
    else { // hide
        sidebar_toggle.classList.add('extend')
        sidebar.classList.add('hide')
        content.classList.add('extend')
    }
}
</script>

  <div class="off-canvas-content">
    <div class="columns">
      <div class="column col-10 col-lg-12">
        <div class="book-navbar">
          <!-- For Responsive Layout -->

<header class="navbar">
  <section class="navbar-section">
    <a onclick="open_sidebar()">
      <i class="icon icon-menu"></i>
    </a>
  </section>
</header>

        </div>
        <div class="book-content">
          <div class="book-post">
  <h1>Sample-based Reinforcement Learning</h1>
<p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p>
<h3 id="Motivations">Motivations</h3>
<ul>
<li>지금까지 K-arm bandit problem으로 reinforcement learning의 기초를 보았고, 그를 이용해서 exploration-exploitation dilemma를 보았다.</li>
<li>또한, Exploration, exploitation이 충분히 이루어져서 environment dynamic에 해당하는, transition probability $p(s’,r|s,a)$가 이미 모델링되었다고 가정했을 때, MDP 환경에 한정해서 value function과 policy를 어떻게 정의하고 계산하는지 보았다.</li>
<li>Bellman equation을 이용해서 dynamic programming 방식으로 policy iteration, value iteration 등, optimal value function과 optimal policy를 계산하는 방법을 보았다.</li>
</ul>
<p>하지만, 여기서, 현실은 environment는 너무나도 복합적인 것이라, 모델링하기가 쉽지 않다. 그러나, 지금까지 해 왔던 방법들은 environment가 필수적으로 모델링되어 있어야 한다.</p>
<p>이렇게, environment를 모델링하는 데 있어서 어려움이 있다는 문제를 해결하는 방법 중 하나가 <strong>Sample-based Reinforcement Learning</strong>이다.</p>
<h3 id="Introduction">Introduction</h3>
<p>이름에서 알 수 있다시피, 이 방법의 주요 특징은, environment dynamic에 해당하는 transition probability function $p(s’,r|s,a)$를 모델링하는 대신, sample-based로 추정해보겠다는 의미이다. 특히 <strong>Monte Carlo Estimation</strong>이 이용된다.</p>
<h2 id="Monte-Carlo-Estimation">Monte Carlo Estimation</h2>
<p>Monte-Carlo Estimation이란, 파라미터의 기댓값을 계산하고자 할 때, 그 파라미터를 모델링하는 과정을 거치지 않고, 파라미터에 대한 샘플을 많이 얻어서 그 샘플들의 평균값을 기댓값으로 추정하는 방법을 의미한다.</p>
<p>샘플 개수가 많을수록, Central Limit Theorem에 의해, 샘플들의 평균은 실제 평균과 매우 가깝다고 확신할 수 있다.</p>
<p>여기서 직감할 수 있다시피, 정확한 기댓값을 추정하기 위해서는 상당히 많은 샘플이 필요하다.</p>
<h3 id="Implementation-Overview">Implementation Overview</h3>
<p>Episodic task로 예를 들려고 한다.</p>
<ol>
<li>
<p>일단 하나의 episode를 완주한다. 즉, 게임이 끝날 때 까지 일단 플레이를 한다. 지나온(또는 처해있었던) state, 취했던 action들, 받았던 reward들의 History는 기록해 둔다.</p>
</li>
<li>
<p>위 history를 $S_0, A_0, S_1, R_1, A_1, S_2, …, S_{T-1}, R_{T-1}, A_{t-1}, S_T, R_T$라는 sequence로 표현했을 떄, $T$에서 backward방향으로 reward 기댓값, 즉, value를 계산한다.</p>
<p>(Final state는 정의에 따라 value가 0이다)<br>
$$<br>
G_T = 0<br>
$$</p>
<p>$$<br>
G_{T-1} = R_{T} + \gamma G_T<br>
$$</p>
<p>$$<br>
G_{T-2} = R_{T-1} + \gamma G_{T-1}<br>
$$</p>
<p>$$<br>
\cdots<br>
$$</p>
</li>
<li>
<p>$G_t$를 $t$일때의 state였던 놈, $S_t$에 대한 value의샘플이라고 간주한다. 하나의 episode에 그 state를 지난 횟수만큼 샘플이 생긴다</p>
</li>
<li>
<p>여러 episode를 플레이해본다.</p>
</li>
<li>
<p>모아진 value sample을 이용해서 state-value function을 추정한다(평균내기).</p>
</li>
</ol>
<p>하지만, 하나의 episode가 모든 state를 골고루 방문하지는 않으므로, 여러 episode를 시행해도, state마다 샘플 수는 다르다. 따라서 어떤 state는 state-value의 정확한 추정이 어려울 수 있다.</p>
<h3 id="Exploring-Starts">Exploring Starts</h3>
<p>Action value function도 state value function을 추정하는 방법과 똑같이 추정할 수 있다. Action value function을 굳이 쓰는 이유는 한 state에서 어떤 액션을 선택할지에 대한, 즉 policy를 찾는데 도움을 줄 수 있기 때문이다.</p>
<p>하지만, 만약, deterministic policy를 따르고 있으면, 각 state에서 특정 action만 수행한다. 따라서, 하나의 액션만 exploitation하게 되는데, 이러면, policy를 비교, 조정할 수 없다. 이에 대한 해결책 중 하나로, episode의 시작은 무조건 random state에서 random action을 취하도록 시작하는 것이다. 첫 번째 액션을 취한 이후로는 policy를 따르게 된다.</p>
<p>이 문제는 결국, exploration에 관한 문제이다.</p>
<h3 id="Monte-Carlo-Prediction">Monte Carlo Prediction</h3>
<p>Reinforcement learning의 context에서, Monte Carlo prediction이란, 주어진 episode를 플레이한 history $S_0, A_0, R_1, S_1, A_1, …, R_T, S_T$를 이용해서 value function을 추정하는 것을 의미한다.</p>
<h2 id="Monte-Carlo-for-Policy-Control">Monte Carlo for Policy Control</h2>
<p>Monte Carlo를 통해 generalized policy iteration을 구현할 수 있다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200125175225811.png" alt="image-20200125175225811"></p>
<ul>
<li>Policy Evaluation: 1번의 Episode 수행 및 Monte Carlo Estimation</li>
<li>Policy Improvement: 계산된 value function을 통한 policy의 greedify</li>
</ul>
<p>이전과 같이 policy evaluation과 improvement를 반복하게 되며, evaluation 단계에서 Monte Carlo prediction을 적용하여 value function을 추정하게 된다. Improvement 단계에서는 추정된 value function을 바탕으로 greedy한 policy를 생성해 낸다.</p>
<p>한 번의 반복(evaluation-improvement) 동안, 단 1번의 episode를 플레이하기 때문에 evaluation 단계에서 value function을 완전히 추정하지 않는다. Evaluation을 완성하려면 수많은 episode를 플레이하고 value function을 제대로 추정하고 improvement로 넘어가야 할 것이다. 하지만, 엄청 오래 걸릴 것이다.</p>
<p>Pseudo-code는 다음과 같다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">monte_carlo_gpi</span><span class="params">(states, actions, gamma)</span>:</span></span><br><span class="line">    pi = initialize_policies(states)</span><br><span class="line">    action_values = initialize_action_values(states, actions)</span><br><span class="line">    returns = initialize_rewards(states, actions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        s0, a0 = exploring_starts(states, actions)</span><br><span class="line">        estimated_action_values, history, T = play_one_episode(pi)</span><br><span class="line">        G = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>, <span class="number">0</span>, by=<span class="number">-1</span>):</span><br><span class="line">            G = history[t+<span class="number">1</span>][<span class="string">"reward"</span>] + gamma * G</span><br><span class="line">            </span><br><span class="line">            state = history[t+<span class="number">1</span>][<span class="string">"state"</span>]</span><br><span class="line">            action = history[t+<span class="number">1</span>][<span class="string">"action"</span>]</span><br><span class="line">            </span><br><span class="line">            returns[state][action].append(G)</span><br><span class="line">            </span><br><span class="line">            action_values[state][action] = mean(returns[state][action])</span><br><span class="line">            pi[state] = argmax(action_values[state][action])</span><br></pre></td></tr></table></figure>
<h3 id="Epsilon-soft-Policy">Epsilon-soft Policy</h3>
<p>Exploring starts 방식은 deterministic policy 환경에서 출발점에서나마 랜덤으로 state와 action을 선택하게 함으로써, 모든 state들이 그래도 한번씩은 다 방문되도록 하게끔 하는 것이다. 하지만, 다음 문제점이 있다.</p>
<ul>
<li>State 개수가 너무 많을 경우, 첫 시작을 임의로 시작한다고 한들, 모든 state를 다 방문하기엔 역부족이다. 계산 불가능할 정도로 많은 시도횟수를 요구할 것이다.</li>
</ul>
<p>하지만, exploring 방법은 여전히 필요하며, exploitation만 할 수는 없다. exploring starts의 대안으로 나온 것이 바로 $\epsilon$-soft 방식이다. 이것은 $\epsilon$-greedy을 포함하는 상위 개념으로, optimal action에는 좀 높은 확률을 두고, 나머지 액션은 확률 0이 아니라 작은 확률을 설정해 두는 것이다.</p>
<p>구현 방법은 value evaluation에서 계산된 value function으로부터 가장 좋은 action을 뽑아내고, 그 액션에 좀 높은 확률을 주고, 나머지 액션은 작은 확률을 준다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200126105656485.png" alt="image-20200126105656485"></p>
<p>$$\epsilon$$-soft는 stochastic policy이다. 즉, optimal policy보다는 value 기댓값이 적다. 하지만, 적절히 greedy한 액션도 취해가면서 확률적으로 많은 state를 방문할 수 있게 해 준다.</p>

</div>


  <div class="book-comments">
    




  </div>


<script src="/js/book-post.js"></script>
        </div>
      </div>
      <div class="column col-2 hide-lg">
        <div class="book-post-info">
  
    <div class="book-post-meta">

  <div class="author">

    <!-- Author image -->
    <div class="author-img">
      
        <figure
          class="avatar avatar-lg"
          data-initial="L"
          style="background-color: #3b4351;">
        </figure>
      
    </div>

    <!-- Author title -->
    <div class="author-title">
      <div>Lee Jaeyoung</div>
      <div>2020-03-03</div>
    </div>
  </div>

  
    <div class="divider"></div>

    <div class="link">
      <a class="category-link" href="/categories/Study-Notes/">Study Notes</a> <a class="category-link" href="/categories/Study-Notes/Reinforcement-Learning/">Reinforcement Learning</a>

      <a class="tag-link" href="/tags/ReinforcementLearning/">#ReinforcementLearning</a> <a class="tag-link" href="/tags/StudyNotes/">#StudyNotes</a>
    </div>
    
  

  <div class="divider"></div>
</div>
  

  <div class="book-tocbot">
</div>
<div class="book-tocbot-menu">
  <a class="book-toc-expand" onclick="expand_toc()">Expand all</a>
  <a onclick="go_top()">Back to top</a>
  <a onclick="go_bottom()">Go to bottom</a>
</div>

<script src="/js/book-toc.js"></script>
</div>
      </div>
    </div>
  </div>
  
  <a class="off-canvas-overlay" onclick="hide_canvas()"></a>
</div>

</body>
</html>

<script src="/js/book.js"></script>