<!DOCTYPE html>
<html lang="ko">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>10. Prediction and Control with Function Approximation | Jaeyoung&#39;s Blog</title>
    
    
        <meta name="keywords" content="StudyNotes,ReinforcementLearning">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Prediction and Control with Function Approximation참고: Coursera Reinforcement Learning (Alberta Univ.) 지금까지, state와 action이 discrete하며, (state, action) pair의 value가 deterministic한, tabular method를 보았다.">
<meta name="keywords" content="StudyNotes,ReinforcementLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="10. Prediction and Control with Function Approximation">
<meta property="og:url" content="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation/index.html">
<meta property="og:site_name" content="Jaeyoung&#39;s Blog">
<meta property="og:description" content="Prediction and Control with Function Approximation참고: Coursera Reinforcement Learning (Alberta Univ.) 지금까지, state와 action이 discrete하며, (state, action) pair의 value가 deterministic한, tabular method를 보았다.">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200224100627139.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200225104841408.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200225135449599.png">
<meta property="og:updated_time" content="2020-03-18T00:39:57.400Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="10. Prediction and Control with Function Approximation">
<meta name="twitter:description" content="Prediction and Control with Function Approximation참고: Coursera Reinforcement Learning (Alberta Univ.) 지금까지, state와 action이 discrete하며, (state, action) pair의 value가 deterministic한, tabular method를 보았다.">
<meta name="twitter:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200224100627139.png">
    

    
        <link rel="alternate" href="/atom.xml" title="Jaeyoung&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">
    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Jaeyoung&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Main</a>
                
                    <a class="main-nav-link" href="/archives">TimeLine</a>
                
                    <a class="main-nav-link" href="/categories">Category</a>
                
                    <a class="main-nav-link" href="/tags">Tag</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '포스트',
            PAGES: 'Pages',
            CATEGORIES: '카테고리',
            TAGS: '태그',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Main</a></td>
                
                    <td><a class="main-nav-link" href="/archives">TimeLine</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Category</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tag</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>카테고리</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Log
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/Blog-Init/">Init Blog</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Study Notes
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Bayesian Statistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/01_Probability/">01. Probability</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/02_Distribution/">02. Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/">03. Frequentist Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/">04. Bayesian Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/">05. Credible Intervals</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/">06. Prior Predictive Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/07_Priors/">07. Priors</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/">08. Bayesian Modeling</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/">09. Monte Carlo Estimation</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/">10. Markov Chain Monte Carlo</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/11_Linear_Regression/">11. Linear Regression</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/">12. Prior Sensitivity Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/">13. Hierarchical Models</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/">14. Predictive Simulations</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/">Appendix 1. Maximize a Posterior</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/">Appendix 2. Empirical Bayes</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Machine Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/">Machine Learning</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis/">Principal Component Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Lagrangian-Multiplication/">Lagrangian Multiplication</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Distillation-Methods/">Distillation Methods</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st/">Restrict Boltzmann Machines 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd/">Restrict Boltzmann Machines 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/">Hidden Markov Models 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/">Hidden Markov Models 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/KL Divergence/">KL Divergence</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Reinforcement Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/01_Introduction/">01. Introduction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/">02. K-arm Bandits Problems</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/">03. Markov Decision Process</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">04. Policies and Value Functions</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">05. Policy Evaluation & Control</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/">06. Sample-based Reinforcement Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/07-Off-Policy-Learning/">07. Off-policy Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/08-Temporal-Difference-Learning/">08. Temporal Difference Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/09-Models-and-Planning/">09. Models and Planning</a></li>  <li class="file active"><a href="/wiki/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation/">10. Prediction and Control with Function Approximation</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/11-Feature-Construction/">11. Feature Construction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/12-Controls-with-Approximation/">12. Controls with Approximation</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/Study-Notes/">Study Notes</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Study-Notes/Reinforcement-Learning/">Reinforcement Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/ReinforcementLearning/">ReinforcementLearning</a>, <a class="tag-link" href="/tags/StudyNotes/">StudyNotes</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation/">
            <time datetime="2020-03-18T00:00:08.000Z" itemprop="datePublished">2020-03-18</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/raw/writing/source/_posts/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/edit/writing/source/_posts/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/commits/writing/source/_posts/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            10. Prediction and Control with Function Approximation
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">카탈로그</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Prediction-and-Control-with-Function-Approximation"><span class="toc-number">1.</span> <span class="toc-text">Prediction and Control with Function Approximation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tabular-Method-is-a-Linear-Function"><span class="toc-number">1.1.</span> <span class="toc-text">Tabular Method is a Linear Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generalization-and-Discrimination"><span class="toc-number">1.2.</span> <span class="toc-text">Generalization and Discrimination</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Value-Estimation-as-Supervised-Learning"><span class="toc-number">1.3.</span> <span class="toc-text">Value Estimation as Supervised Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Objective-for-On-policy-Prediction"><span class="toc-number">1.4.</span> <span class="toc-text">The Objective for On-policy Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Value-Error-Objective"><span class="toc-number">1.4.1.</span> <span class="toc-text">The Value Error Objective</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Monte-Carlo-for-Policy-Evaluation"><span class="toc-number">1.4.2.</span> <span class="toc-text">Gradient Monte Carlo for Policy Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#State-Aggregation"><span class="toc-number">1.4.3.</span> <span class="toc-text">State Aggregation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Objective-for-TD"><span class="toc-number">1.5.</span> <span class="toc-text">The Objective for TD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TD-vs-Monte-Carlo"><span class="toc-number">1.6.</span> <span class="toc-text">TD vs Monte Carlo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-TD"><span class="toc-number">1.7.</span> <span class="toc-text">Linear TD</span></a></li></ol></li></ol>
                </div>
            
        
        
            <h1 id="Prediction-and-Control-with-Function-Approximation"><a href="#Prediction-and-Control-with-Function-Approximation" class="headerlink" title="Prediction and Control with Function Approximation"></a>Prediction and Control with Function Approximation</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p>
<p>지금까지, state와 action이 discrete하며, (state, action) pair의 value가 deterministic한, tabular method를 보았다. 하지만, 이건 실생활에서 매우 한정적일 수 밖에 없다.</p>
<p>Value function을 table로 표현하지 말고, function으로 추정하자는게 지금부터 다룰 내용이다.<br>$$<br>V(s) = f_W(s)<br>$$</p>
<p>Function은 어떤 파라미터 $W$로 parameterized되어 있으며, function은 linear 형태의 function이나, 인공신경망과 같이 non-linear한 형태도 가능하다.</p>
<h2 id="Tabular-Method-is-a-Linear-Function"><a href="#Tabular-Method-is-a-Linear-Function" class="headerlink" title="Tabular Method is a Linear Function"></a>Tabular Method is a Linear Function</h2><p>Tabular method는 linear function approximation의 한 방법이다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200224100627139.png" alt="image-20200224100627139"></p>
<p>state의 개수만큼 feature가 있고, feature는 각 state를 나타내는 indicator가 된다. 그러면, 각 state의 value는 그 state의 weight가 된다.<br>$$<br>V(s_i) = \begin{pmatrix}<br>0 \newline<br>\cdots \newline<br>0 \newline<br>1 \newline<br>0 \newline<br>\cdots \newline<br>0<br>\end{pmatrix} \cdot<br>\begin{pmatrix}<br>w_1 \newline<br>\cdots \newline<br>w_{i-1} \newline<br>w_i \newline<br>w_{i+1} \newline<br>\cdots \newline<br>w_{16}<br>\end{pmatrix} = w_i<br>$$<br>즉, tabular method는 위와 같이 indicator feature를 이용해서 linear function으로 표현이 가능하다. 따라서, tabular method는 linear function approximation의 한 instance이다.</p>
<h2 id="Generalization-and-Discrimination"><a href="#Generalization-and-Discrimination" class="headerlink" title="Generalization and Discrimination"></a>Generalization and Discrimination</h2><p>Generalization과 discrminiation은 reinforcement learning에서 상당히 중요하다.</p>
<ul>
<li><p>Generalization</p>
<p>Generalizaiton은 어떤 state를 학습(value를 추정)했다면, 비슷한 다른 state까지 영향을 미쳐서 학습되는 것을 의미한다.</p>
</li>
<li><p>Discrimination</p>
<p>Discrimination은 서로 다른 state끼리는 학습 또는 추정시, 영향을 미치지 않아야 함을 의미한다. 즉, 어떤 state끼리는 독립적으로 value가 추정되어야 한다는 것이다.</p>
</li>
</ul>
<p>Tabular method는 generalization을 전혀 하지 못하고, discrimination을 100% 수행하는 방법이다. 반면, 모든 state를 똑같은 value를 두도록 설정하면, discrimination을 전혀 수행하지 못하고 generalization을 100% 수행하게 된다. RL에서는 generalization과 discrimination을 동시에 높여야 하며, trade-off관계라서, 적절히 조정하는게 필요하다.</p>
<h2 id="Value-Estimation-as-Supervised-Learning"><a href="#Value-Estimation-as-Supervised-Learning" class="headerlink" title="Value Estimation as Supervised Learning"></a>Value Estimation as Supervised Learning</h2><p>True reward 리턴값이 있다면, reward를 target label로 삼아서 $f_W(s)$를 학습할 수 있지 않을까.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200225104841408.png" alt="image-20200225104841408"></p>
<p>하지만, reinforcement learning에서는 각 데이터 샘플(하나의 state-action pair)이 서로 correlate되어 iid에 위반되므로, 모든 supervised learning technique이 다 잘 적용될 수 있는 것은 아니다. 또한, TD의 경우, target 자체가 estimation값(다음 state의 value는 또 다른 estimation 값임)이므로, target이 학습이 지속되면서 변한다. 이는, supervised learning이랑 상당히 다른 환경이다.</p>
<h2 id="The-Objective-for-On-policy-Prediction"><a href="#The-Objective-for-On-policy-Prediction" class="headerlink" title="The Objective for On-policy Prediction"></a>The Objective for On-policy Prediction</h2><p>Function approximation을 하기 위해서는, target value와 얼만큼 가까운지 판단할 수 있는 objective function이 필요하다. </p>
<h3 id="The-Value-Error-Objective"><a href="#The-Value-Error-Objective" class="headerlink" title="The Value Error Objective"></a>The Value Error Objective</h3><p>다음은 Mean squared error를 이용한 objective function이다.<br>$$<br>\text{VE} = \sum_s \mu(s)[V_{\pi}(s) - \hat{v}(s, W)]^2<br>$$<br>이때, 각 state마다 서로 다른 가중치 $\mu(s)$를 주어서, 상대적으로 중요한 state에게는 높은 가중치를, 덜 중요한 state에게는 낮은 가중치를 주도록 한다. Policy에 의해 자주 방문하는 state에 대해서는 높은 가중치를 줄 수도 있겠다.</p>
<h3 id="Gradient-Monte-Carlo-for-Policy-Evaluation"><a href="#Gradient-Monte-Carlo-for-Policy-Evaluation" class="headerlink" title="Gradient Monte Carlo for Policy Evaluation"></a>Gradient Monte Carlo for Policy Evaluation</h3><p>Gradient descent를 Monte Carlo RL에 맞게 수정한 것. Stochastic gradient descent.</p>
<p>Value error식은 때때로 state개수가 너무 많아서 계산이 불가능하다. 대신, gradient를 approximation한다. 원래 Gradient descent식은 다음과 같다. 이때, $x(s)$는 state $s$의 feature vector이다.<br>$$<br>W \leftarrow W + \alpha \sum_s \mu(s)[V_{\pi}(s) - \hat{v}(s,W)] \nabla \hat{v}(s, W)<br>$$<br>그런데, 이 식을 쓰지 말고, 다음처럼 gradient를 approximate해서 쓰자는 것이 된다.<br>$$<br>W \leftarrow W + \alpha [V_{\pi}(S_t) - \hat{v}(S_t, W)] \nabla \hat{v}(S_t, W)<br>$$<br>왜냐하면 다음이 성립하기 떄문. 즉, 위 gradient는 원래 gradient의 추정치라고 볼 수 있다.<br>$$<br>E[V_{\pi}(S_t) - \hat{v}(S_t,W)] = \sum_s \mu(s)[V_{\pi}(s) - \hat{v}(s,W)]<br>$$<br>이는, 샘플 하나 (state-action pair 1개)씩 보면서 한 번 업데이트하는 stochastic gradient descent 방식이다.</p>
<p>하지만, 이 경우는 target value function인 $V_{\pi}(s)$를 알아야 한다. 얘네도 $G_t$를 이용해 approximation한다.<br>$$<br>W \leftarrow W + \alpha [G_t - \hat{v}(S_t,W)] \nabla \hat{v}(S_t,W)<br>$$<br>역시 다음을 추정한 것이다.<br>$$<br>E[V(S_t)] = E[G_t|S_t]<br>$$<br>다음은 pseudo code.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200225135449599.png" alt="image-20200225135449599"></p>
<h3 id="State-Aggregation"><a href="#State-Aggregation" class="headerlink" title="State Aggregation"></a>State Aggregation</h3><p>State개수가 너무 많아서 모든 state를 따로 업데이트하기 힘들 때, 비슷한 state끼리는 묶어서 하나의 state로 보는 것을 말한다. 따라서, 하나의 state가 업데이트되면 같은 그룹의 다른 state도 같은 값으로 업데이트된다.</p>
<h2 id="The-Objective-for-TD"><a href="#The-Objective-for-TD" class="headerlink" title="The Objective for TD"></a>The Objective for TD</h2><p>On-policy learning인 Gradient Monte Carlo의 objective로 squared error를 사용했었다. 하지만, TD는 $G_t$가 다음의 특성을 가진다.<br>$$<br>v(S_t, W) = R_{t+1} + \gamma v(S_{t+1}, W)<br>$$<br>($G_t = \hat{v}(S_t, A_t)$이다. ) 즉, 다음의 gradient 수식에서,<br>$$<br>(G_t - \hat{v}(S_{t}, W)) \nabla \hat{v}(S_{t}, W)<br>$$<br>$G_t$부분은 실제 value의 추정값이므로, TD learning에서는 $v(S_t,W)$와 대체해야 한다. 그런데, 이놈은 $v(S_{t+1}, W)$를 참조하고 있으며, 이 $v(S_{t+1}, W)$는 true value의 estimation이기 보단 현재의 value estimation이다. 따라서, biased된 추정값이며, $v(S_t, W)$를 미분해도 이 식이 $W$를 가지고 있으므로, Gradient Monte Carlo의 gradient 수식과 같게 나오지 않는다. 하지만, 그냥 $v(S_t, W)$는 상수처럼 간주해버리고 쓰게 되는데(즉, $W$에 대한 함수가 아니라고 간주), 이를 semi-gradient 방법이라고 부른다.</p>
<p>최종적으로 $W$의 업데이트 식은 다음과 같이 쓴다.<br>$$<br>W \leftarrow W + \alpha(R_{t+1} + \gamma v(S_{t+1}, W) - \hat{v}(S_t,W)) \nabla \hat{v}(S_t, W)<br>$$</p>
<h2 id="TD-vs-Monte-Carlo"><a href="#TD-vs-Monte-Carlo" class="headerlink" title="TD vs Monte Carlo"></a>TD vs Monte Carlo</h2><p>Function approximation에서, TD와 Monte Carlo 방식의 차이점은 다음과 같다.</p>
<p><strong>TD</strong></p>
<ul>
<li>장점<ul>
<li>에피소드가 끝나기 전에 바로바로 학습하므로 빠른 학습 속도(loss가 빠르게 줄어듬)</li>
</ul>
</li>
<li>단점<ul>
<li>Estimation이 최종 reward를 반영하지 않은, 현재의 value estimation을 true estimation으로 삼기 때문에 biased된 학습. 즉, 부정확할 수 있다. Local minimum의 근처까지밖에 못갈 수 있다.</li>
</ul>
</li>
</ul>
<p><strong>Monte Carlo</strong></p>
<ul>
<li>장점<ul>
<li>에피소드의 최종 reward를 반영한 true value의 estimation을 사용하기에 TD보단 unbiased된 학습. 즉, local minimum을 다소 정확하게 찾는다.</li>
</ul>
</li>
<li>단점<ul>
<li>느리다. step size를 작게 줘야 한다.</li>
</ul>
</li>
</ul>
<h2 id="Linear-TD"><a href="#Linear-TD" class="headerlink" title="Linear TD"></a>Linear TD</h2><p>Value function을 linear하게 모델링한 것을말하며, 간단하고 쉽지만, 잘 정제된 feature가 있다면, 매우 강력한 성능을 발휘한다.</p>
<p>Tabular TD(0)는 linear TD의 한 종류인데, 다음처럼 feature가 생겼다고 가정한다면, 완벽히 linear TD이다.<br>$$<br>w = \begin{pmatrix}<br>w_0 \newline<br>w_1 \newline<br>w_2 \newline<br>w_3 \newline<br>\cdots \newline<br>w_d<br>\end{pmatrix},<br>x(s_i) = \begin{pmatrix}<br>0 \newline<br>0 \newline<br>1 \newline<br>0 \newline<br>\cdots \newline<br>0<br>\end{pmatrix},<br>\hat{v}(s,w) = w \cdot x<br>$$<br>feature는 어떤 state인지 나타내는 indicator이고, weight가 각각 상응하는 state들의 value가 되는 셈. Feature $x$를 어떤 aggregation인지를 나타낸다고 하면, aggregation tabular TD(0)역시 linear TD의 모양이 되므로, aggregation tabular TD(0)역시, linear TD의 한 종류이다.</p>
<p>만약, squared error를 사용하는 linear TD라면, 다음 식으로 $w$가 업데이트된다.<br>$$<br>w \leftarrow w + \alpha (R_{t+1} + \gamma \hat{v}(S_{t+1}, w) - \hat{v}(S_t, w)) X(S_t)<br>$$</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/wiki/studynotes/reinforcement-learning/11-Feature-Construction/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">다음 글</strong>
            <div class="article-nav-title">
                
                    11. Feature Construction
                
            </div>
        </a>
    
    
        <a href="/wiki/studynotes/reinforcement-learning/09-Models-and-Planning/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">이전 글</strong>
            <div class="article-nav-title">09. Models and Planning</div>
        </a>
    
</nav>





    
    

    <script src="https://utteranc.es/client.js"
        repo="taeuk-gang/taeuk-gang.github.io"
        issue-term="title"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>



<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Lee Jaeyoung &copy; 2020 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <!-- <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a> -->
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>