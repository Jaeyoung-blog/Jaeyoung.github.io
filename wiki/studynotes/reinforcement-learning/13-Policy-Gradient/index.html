<!DOCTYPE html>
<html lang="ko">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>13. Policy Gradient | Jaeyoung&#39;s Blog</title>
    
    
        <meta name="keywords" content="StudyNotes,ReinforcementLearning">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Policy GradientPolicy gradient 방법은, policy를 parameterized function으로 모델링해서, state, action feature vector로부터 바로 policy를 추론하는 방법이다. 즉, state, action feature로부터 action value function을 추론하고 그로부터 policy를 계">
<meta name="keywords" content="StudyNotes,ReinforcementLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="13. Policy Gradient">
<meta property="og:url" content="https://wayexists02.github.io/wiki/studynotes/reinforcement-learning/13-Policy-Gradient/index.html">
<meta property="og:site_name" content="Jaeyoung&#39;s Blog">
<meta property="og:description" content="Policy GradientPolicy gradient 방법은, policy를 parameterized function으로 모델링해서, state, action feature vector로부터 바로 policy를 추론하는 방법이다. 즉, state, action feature로부터 action value function을 추론하고 그로부터 policy를 계">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200309091753523.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200309092901785.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200309092952785.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200310110319075.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200311104559800.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200311114613408.png">
<meta property="og:updated_time" content="2020-03-23T00:13:37.659Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="13. Policy Gradient">
<meta name="twitter:description" content="Policy GradientPolicy gradient 방법은, policy를 parameterized function으로 모델링해서, state, action feature vector로부터 바로 policy를 추론하는 방법이다. 즉, state, action feature로부터 action value function을 추론하고 그로부터 policy를 계">
<meta name="twitter:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200309091753523.png">
    

    
        <link rel="alternate" href="/atom.xml" title="Jaeyoung&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">
    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Jaeyoung&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Main</a>
                
                    <a class="main-nav-link" href="/archives">TimeLine</a>
                
                    <a class="main-nav-link" href="/categories">Category</a>
                
                    <a class="main-nav-link" href="/tags">Tag</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '포스트',
            PAGES: 'Pages',
            CATEGORIES: '카테고리',
            TAGS: '태그',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Main</a></td>
                
                    <td><a class="main-nav-link" href="/archives">TimeLine</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Category</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tag</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>카테고리</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Log
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/Blog-Init/">Init Blog</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Study Notes
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Bayesian Statistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/01_Probability/">01. Probability</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/02_Distribution/">02. Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/">03. Frequentist Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/">04. Bayesian Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/">05. Credible Intervals</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/">06. Prior Predictive Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/07_Priors/">07. Priors</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/">08. Bayesian Modeling</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/">09. Monte Carlo Estimation</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/">10. Markov Chain Monte Carlo</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/11_Linear_Regression/">11. Linear Regression</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/">12. Prior Sensitivity Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/">13. Hierarchical Models</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/">14. Predictive Simulations</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/">Appendix 1. Maximize a Posterior</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/">Appendix 2. Empirical Bayes</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Machine Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/">Machine Learning</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis/">Principal Component Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Lagrangian-Multiplication/">Lagrangian Multiplication</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Distillation-Methods/">Distillation Methods</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st/">Restrict Boltzmann Machines 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd/">Restrict Boltzmann Machines 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/">Hidden Markov Models 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/">Hidden Markov Models 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/KL Divergence/">KL Divergence</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Reinforcement Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/01_Introduction/">01. Introduction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/">02. K-arm Bandits Problems</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/">03. Markov Decision Process</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">04. Policies and Value Functions</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">05. Policy Evaluation & Control</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/">06. Sample-based Reinforcement Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/07-Off-Policy-Learning/">07. Off-policy Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/08-Temporal-Difference-Learning/">08. Temporal Difference Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/09-Models-and-Planning/">09. Models and Planning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation/">10. Prediction and Control with Function Approximation</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/11-Feature-Construction/">11. Feature Construction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/12-Controls-with-Approximation/">12. Controls with Approximation</a></li>  <li class="file active"><a href="/wiki/studynotes/reinforcement-learning/13-Policy-Gradient/">13. Policy Gradient</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/Appendix-1-RL-Cheatsheet/">Appendix 01. Which Algorithm Should be selected</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                     <li class="file"><a href="/wiki/index/">Home</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-studynotes/reinforcement-learning/13-Policy-Gradient" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/Study-Notes/">Study Notes</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Study-Notes/Reinforcement-Learning/">Reinforcement Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/ReinforcementLearning/">ReinforcementLearning</a>, <a class="tag-link" href="/tags/StudyNotes/">StudyNotes</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/studynotes/reinforcement-learning/13-Policy-Gradient/">
            <time datetime="2020-03-23T00:12:08.000Z" itemprop="datePublished">2020-03-23</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/raw/writing/source/_posts/studynotes/reinforcement-learning/13-Policy-Gradient.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/edit/writing/source/_posts/studynotes/reinforcement-learning/13-Policy-Gradient.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/commits/writing/source/_posts/studynotes/reinforcement-learning/13-Policy-Gradient.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            13. Policy Gradient
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">카탈로그</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Policy-Gradient"><span class="toc-number">1.</span> <span class="toc-text">Policy Gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax-Policies"><span class="toc-number">1.1.</span> <span class="toc-text">Softmax Policies</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Action-Preference-vs-Action-Value"><span class="toc-number">1.1.1.</span> <span class="toc-text">Action Preference vs Action Value</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Policies"><span class="toc-number">1.1.2.</span> <span class="toc-text">Stochastic Policies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Selecting-Function-Approximation"><span class="toc-number">1.2.</span> <span class="toc-text">Selecting Function Approximation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Gradient-for-Continuous-Tasks"><span class="toc-number">1.3.</span> <span class="toc-text">Policy Gradient for Continuous Tasks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Policy-Gradient-Theorem"><span class="toc-number">1.3.1.</span> <span class="toc-text">The Policy Gradient Theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Estimation-of-Policy-Gradient"><span class="toc-number">1.3.2.</span> <span class="toc-text">Estimation of Policy Gradient</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic-Algorithm"><span class="toc-number">1.4.</span> <span class="toc-text">Actor-Critic Algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic-for-Continuous-Actions"><span class="toc-number">1.5.</span> <span class="toc-text">Actor-Critic for Continuous Actions</span></a></li></ol></li></ol>
                </div>
            
        
        
            <h1 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h1><p>Policy gradient 방법은, policy를 parameterized function으로 모델링해서, state, action feature vector로부터 바로 policy를 추론하는 방법이다.</p>
<p>즉, state, action feature로부터 action value function을 추론하고 그로부터 policy를 계산하는 이전 방법에서, action value function을 거치지 않고 바로 policy를 추론하는 것이다. </p>
<p>차이점은, 이전 방법에서는 action value function을 parameterize했지만, policy gradient에서는 policy를 parameterize한다. 이때, policy funciton의 parameter는 $\theta$로 표기한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200309091753523.png" alt="image-20200309091753523"></p>
<h2 id="Softmax-Policies"><a href="#Softmax-Policies" class="headerlink" title="Softmax Policies"></a>Softmax Policies</h2><p>Policy는 반드시 확률분포여야 한다. 즉, 다음이 성립해야 한다.<br>$$<br>\sum_a \pi(a|s, \theta)=1 ~ \text{ for all } s<br>$$<br>즉, 각 state에서, action들의 확률들의 합은 1이어야 한다. 또한, 각 확률은 [0, 1] 범위에 있어야 한다.</p>
<p>Action value function에서는 linear function을 이용했지만, policy function은 위와 같은 이유로 linear function의 결과값을 그대로 이용할 수 없다. 그 대신, linear function의 결과값에 softmax 함수를 적용해 준다.<br>$$<br>\pi(a|s, \theta) = \text{softmax}(h(s, a, \theta)) = \frac{e^{h(s, a, \theta)}}{\sum_{a’} e^{h(s, a’, \theta)}}<br>$$<br>이때, $h(s, a, \theta)$는 state $s$와 action $a$를 입력으로 받고 action preference를 출력하는 linear 함수(마지막 레이어가 linear이면 linear하다고 하자)이다.</p>
<p>이제, action preference는 linear해도 된다. action preference가 심지어 음수가 나와도 exponential에 의해 양수임이 보장되며, 분모의 normalization으로 인해 [0, 1]사이 값으로 유지됨이 보장되며, 합이 1임이 보장된다.</p>
<h3 id="Action-Preference-vs-Action-Value"><a href="#Action-Preference-vs-Action-Value" class="headerlink" title="Action Preference vs Action Value"></a>Action Preference vs Action Value</h3><p>Action preference는 action value와는 다르다. Action value는 미래 expected return의 합으로 이루어져 있으나, action preference는 그러한 것을 고려하지 않는다.</p>
<p>또한, action value는 가장 높은 값을 가지는 action만이 높은 확률을 갖고($1 - \epsilon$), 나머지 action은 $\epsilon/N_a$값을 가진다($N_a$는 액션 개수). 즉, 높은 action value 값을 가지는 action이외의 action은 모두 작은 확률로 같은 확률을 가진다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200309092901785.png" alt="image-20200309092901785"></p>
<p>하지만, action preference는 그 값이 크고 작음의 순서가 유지가 된다. Softmax 함수는 preference가 클수록 큰 확률을 가지게 하며, 작을수록 작은 확률을 가지게 만들어준다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200309092952785.png" alt="image-20200309092952785"></p>
<h3 id="Stochastic-Policies"><a href="#Stochastic-Policies" class="headerlink" title="Stochastic Policies"></a>Stochastic Policies</h3><p>때로는 deterministic policy를 가지면 풀기가 불가능한 문제도 있다고 한다. Epsilon greedy방법(action value approximation에서의)은 deterministic policy를 결과로 내놓기에, 이때 매우 성능이 좋지 않다. Policy gradient는 반면, softmax함수로 인해 stochastic policy를 출력한다.</p>
<p>(Action value에 따라서 policy를 정할때도 epsilon greedy가 아닌 softmax를 씌우면 어떻게 될까)</p>
<h2 id="Selecting-Function-Approximation"><a href="#Selecting-Function-Approximation" class="headerlink" title="Selecting Function Approximation"></a>Selecting Function Approximation</h2><p>때로는 action value를 parameterized function으로 모델링하는 것 보다 policy를 parameterized function으로 모델링하는게, 훨씬 간단할 때도 있고 반대의 경우도 존재한다. 따라서, 적절히 덜 복잡한 방향을 선택하면 될 듯 하다.</p>
<h2 id="Policy-Gradient-for-Continuous-Tasks"><a href="#Policy-Gradient-for-Continuous-Tasks" class="headerlink" title="Policy Gradient for Continuous Tasks"></a>Policy Gradient for Continuous Tasks</h2><p>Episodic task에서는 다음을 최대화하는 액션을 선택하게 하는 policy를 찾는 것이다.<br>$$<br>G_t = \sum_{t=0}^T R_t<br>$$<br>Continuous task에서는 다음을 최대화하는 액션을 선택하게 하는 policy를 찾는 것이다.<br>$$<br>G_t = \sum_{t=0}^T \gamma^t R_t ~ (0 \leq \gamma &lt; 1)<br>$$<br>(Continuous이기 때문에 discounting이 필요. 안그러면 $\infty$로 간다.)</p>
<p>Continuous task에서는 다음을 최대화하는 액션을 선택하게 할 수도 있다.<br>$$<br>G_t = \sum_{t=1}^T (R_t - r(\pi))<br>$$<br>이때, $R_t - r(\pi)$는 발산하지 않고 수렴하기에 discounting이 필요하지 않다.</p>
<p>어떤 policy가 최대 reward를 획득하게 한다는 것은 모든 state의 평균 reward를 의미하는 $r(\pi)$를 최대화하는 것과 같다. 그 policy가 궁극적으로 총 reward가 최대가 되도록 학습한다면, $r(\pi)$도 최대이기 때문.</p>
<p>$r(\pi)$는 다음과 같았다.<br>$$<br>r(\pi) = \sum_s \mu_{\pi}(s) \sum_a \pi(a|s, \theta) \sum_{s’,r} p(s’, r|s, a)r<br>$$</p>
<p>이 $r(\pi)$를 최대화하기 위해서, policy $\pi$에 대해 미분해보면,<br>$$<br>\nabla r(\pi) = \nabla \sum_s \mu_{\pi}(s) \sum_a \pi(a|s, \theta) \sum_{s’, r} p(s’, r|s, a)r<br>$$<br>인데, $\mu_{\pi}(s)$는 각 state의 방문 횟수로, policy에 의해 영향을 받는다. 하지만, $\mu_{\pi}(s)$는 stationary distribution으로, 추정하기 쉽지 않다.</p>
<h3 id="The-Policy-Gradient-Theorem"><a href="#The-Policy-Gradient-Theorem" class="headerlink" title="The Policy Gradient Theorem"></a>The Policy Gradient Theorem</h3><p>The Policy Gradient Theorem이라는 이름으로, $\nabla r(\pi)$는 다음으로 추정할 수 있다고 증명되어 있다.<br>$$<br>\nabla r(\pi) = \sum_s \mu_{\pi}(s) \sum_a \nabla \pi(a|s,\theta) \cdot q_{\pi}(s,a)<br>$$<br>$\nabla \pi(a|s,\theta)$는 액션 $a$의 확률값이 높아지는 방향의 gradient이다. 이것을 상응하는 action value와 weighted sum하게 된다. 그러면 전체 average reward가 상승하는 방향일 것이라는 것이고, 이 과정을 모든 state에서 계산하고 모두 더해준다.</p>
<p>예를들어, 어떤 state에서 액션이 상, 하, 좌, 우 네가지가 있고, 상, 좌 방향으로는 action value가 낮다고 하자. 하지만, 하, 우 방향으로는 높다고 할 때, 상대적으로, 하, 우 방향의 action value가 높으므로, 하, 우 방향이 gradient와 action value의 곱이 상대적으로 비중을 차지하는 비율이 증가할 것이다. 그리고, policy는 이 gradient 방향으로 업데이트하게 되면, policy는 하, 우 의 확률 비중을 약간 늘릴 수 있을 것이다. 이런 과정을 전체 state에 반복하면서 전체 average gradient를 높일 수 있다는 것이다.</p>
<h3 id="Estimation-of-Policy-Gradient"><a href="#Estimation-of-Policy-Gradient" class="headerlink" title="Estimation of Policy Gradient"></a>Estimation of Policy Gradient</h3><p>$\nabla r(\pi)$를 계산하는 대신 추정하고자 하는데, $\nabla r(\pi)$는 다음과 같았다.<br>$$<br>\nabla r(\pi) = \sum_s \mu_{\pi}(s) \sum_a \nabla \pi(a|s,\theta) q(s,a)<br>$$<br>그런데, 이것은 다음처럼 $\mu$에 대한 기댓값으로 표현이 가능하다.<br>$$<br>\nabla r(\pi) = E_{\mu}[\sum_a \nabla \pi(a|s,\theta) q(s, a)]<br>$$<br>참고로, $E[X]$는 stochastic sample인 $x_i \sim X$ 여러개로 추정이 가능하다. 따라서, $\nabla r(\pi)$는 다음처럼 추정할 수 있다.<br>$$<br>\nabla r(\pi) = \sum_a \nabla \pi(a|S_t,\theta) q(S_t, a)<br>$$<br>이때, $S_t$는 agent가 environment와 상호작용하면서 얻은 샘플 또는 경험이다. 하지만, action에 대한 summation이 남아 있다. 이것은 계산이 가능하지만, 이것 또한 stochastic sample을 이용해서 추정이 가능하다.</p>
<p>다음처럼 $\pi$에 대한 기댓값이 되도록 식을 수정한다.<br>$$<br>\nabla r(\pi) = \sum_a \pi(a|S_t, \theta) \frac{1}{\pi(a|S_t,\theta)} \nabla \pi(a|S_t, \theta) q(S_t, a)<br>$$</p>
<p>$$<br>\nabla r(\pi) = E_{\pi} [\frac{\nabla \pi(a|S_t, \theta)}{\pi(a|S_t,\theta)}q(S_t, a)]<br>$$</p>
<p>이것은 다음으로 추정이 가능할 것이다(stochastic sample).<br>$$<br>\nabla r(\pi) = \frac{\nabla \pi(A_t|S_t,\theta)}{\pi(A_t|S_t,\theta)} q(S_t, A_t)<br>$$<br>최종적으로, policy gradient ascent는 다음과 같다.<br>$$<br>\theta_{t+1} \leftarrow \theta_t + \alpha \cdot \nabla r(\pi)<br>$$</p>
<p>$$<br>\theta_{t+1} \leftarrow \theta_t + \alpha \cdot \frac{\nabla \pi(A_t|S_t,\theta)}{\pi(A_t|S_t,\theta)} q(S_t, A_t)<br>$$</p>
<p>($\nabla r(\pi)$는 $r(\pi)$가 증가하는 방향이므로 +를 한다.)</p>
<p>이것은 또 다음처럼 간단하게 쓸 수 있다.<br>$$<br>\theta_{t+1} \leftarrow \theta_t + \alpha \cdot \nabla ~ \text{ln}[\pi(A_t|S_t, \theta) ]q(S_t, A_t)<br>$$<br>$\pi$는 parameterized function이라 계산이 가능하고, $q(S_t,A_t)$는 TD같은 것으로 추정할 수 있다(Value function도 여전히 추정해야 한다!).<br>$$<br>\theta_{t+1} \leftarrow \theta_t + \alpha \cdot \nabla \text{ln}[\pi(A_t|S_t,\theta)](R_{t+1} - r(\pi) + \hat{v}(S_{t+1}))<br>$$<br>Action value는 액션 $A_t$을 취했을 때 얻은 reward $R_{t+1}$와 그 후의 value, 즉 state value $v(S_{t+1})$을 합한 것과 같기 때문에 위 처럼 된다. 이 경우, action value function은 단순히 reward의 합이 아닌, differential reward의 합을 추정한 놈이므로 $q(S_t, A_t) = R_{t+1} - r(\pi) + v(S_{t+1},W)$이 된다.</p>
<h2 id="Actor-Critic-Algorithm"><a href="#Actor-Critic-Algorithm" class="headerlink" title="Actor-Critic Algorithm"></a>Actor-Critic Algorithm</h2><p>Actor가 critic의 피드백을 받고 본인의 policy를 수정하면서 발전해가는 형식이라고 한다.</p>
<ul>
<li><p>Actor</p>
<p>Parameterized policy function을 말하며, 어떤 행동을 하는 객체라고 해서 이렇게 이름을 붙인 듯.</p>
</li>
<li><p>Critic</p>
<p>Parameterized state value function을 말하며, actor가 다음에 action을 취할지, 즉, policy를 어떻게 업데이트할지에 대해, action value $q(S_t, A_t)$값으로 피드백을 주고 actor의 다음 행동에 영향을 미치게 한다.</p>
</li>
</ul>
<p>Agent가 environment와 상호작용하면서 얻은 샘플(또는 경험)들을 이용해서 actor와 critic을 동시에 업데이트시키게 된다.</p>
<p>그 전에, 업데이트의 편의를 위해 policy의 파라미터인 $\theta$의 업데이트식에 action value baseline을 추가한다.<br>$$<br>\theta_{t+1} \leftarrow \theta_t + \alpha \cdot \nabla \text{ln}[\pi(A_t|S_t,\theta)](R_{t+1} - r(\pi) + \hat{v}(S_{t+1},W)- \hat{v}(S_t,W))<br>$$<br>이때, $\hat{v}(S_t,W)$가 action value의 baseline이며, 이것을 추가하는 것은 $\theta$의 업데이트 방향에 영향을 전혀 미치지 않는다. 왜냐하면, 위 식을 다시 기댓값 식으로 바꿔보면,<br>$$<br>\nabla r(\pi) = \nabla \text{ln}[\pi(A_t|S_t, \theta)](R_{t+1} - r(\pi) + \hat{v}(S_{t+1},W))<br>$$</p>
<p>$$<br>\nabla r(\pi) = E_{\pi} [\nabla \text{ln}[\pi(a|S_t, \theta)](R_{t+1} - r(\pi) + \hat{v}(S_{t+1},W))]<br>$$</p>
<p>그런데 여기서, 다음을 더해주는데, 아래 기댓값은 0이기 때문에 $\nabla r(\pi)$에 영향을 미치지 않는다.<br>$$<br>E_{\pi}[-\nabla \text{ln} [\pi(a|S_t,\theta)]\hat{v}(S_t,W)]<br>$$<br>따라서, 다음과 같이 된다.<br>$$<br>\nabla r(\pi) = \nabla \text{ln}[\pi(A_t|S_t, \theta)](R_{t+1} - r(\pi) + \hat{v}(S_{t+1},W) - \hat{v}(S_t,W))<br>$$<br>이 식이 Actor-Critic 알고리즘에서 사용될 policy gradient 식인데, baseline의 기댓값이 0인 이유는 다음과 같다.<br>$$<br>E_{\pi}[-\nabla \text{ln} [\pi(a|S_t,\theta)] \hat{v}(S_t, W)]<br>$$</p>
<p>$$<br>= -\sum_a \pi(a|S_t,\theta)\nabla \text{ln} [\pi(a|S_t,\theta)] \hat{v}(S_t, W)<br>$$</p>
<p>$$<br>= -\sum_a \nabla \pi(a|S_t,\theta)\nabla \hat{v}(S_t, W)<br>$$</p>
<p>이때, $S_t$는 이미 주어져 있다($S_t$가 주어진 후, action/policy에 대한 기댓값이니까). 따라서, $\hat{v}$은 action에 의해 영향을 받지 않는 값이며, 밖으로 나올 수 있다.<br>$$<br>= -\hat{v}(S_t, W) \sum_a \nabla \pi(a|S_t,\theta)<br>$$<br>그리고, gradient의 합은 합의 gradient이므로 다음과 같다.<br>$$<br>= -\hat{v}(S_t, W) \nabla \sum_a \pi(a|S_t,\theta)<br>$$<br>근데, 이때, $\pi$는 확률분포이고, 그들의 합은 1이다. 따라서,<br>$$<br>= -\hat{v}(S_t,W) \nabla 1 = 0<br>$$<br>어쨌든 최종적으로, policy gradient식은 다음과 같다.<br>$$<br>\nabla r(\pi) = \nabla \text{ln}[\pi(A_t|S_t, \theta)](R_{t+1} - r(\pi) + \hat{v}(S_{t+1},W) - \hat{v}(S_t,W))<br>$$</p>
<p>$$<br>\nabla r(\pi) = \nabla \text{ln}[\pi(A_t|S_t, \theta)] \delta_t<br>$$</p>
<p>이때, $\delta$는 TD error이다(TD error로 만들어주기 위해 baseline을 넣은 것이다).</p>
<p>최종적으로, Actor-Critic 알고리즘의 pseudo code는 다음과 같다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200310110319075.png" alt="image-20200310110319075"></p>
<p>(Learning rate는 actor, critic, 그리고 average reward 평균비율 전부 따로 줄 수 있다)</p>
<p>State value function의 feature는 state로만 구성되지만, policy의 feature는 state와 action 모두로 구성된다. Policy의 feature는 state feature를 action 개수만큼 stack한 feature라고 가정해보자.</p>
<p>다음 그림에서 state feature는 $x_0(s), x_1(s), x_2(s), x_3(s)$ 4개로 구성되어 있고, state-action pair feature 개수는 state feature 4개를 3번 복사한, 총 12개로 구성되는 것이다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200311104559800.png" alt="image-20200311104559800"></p>
<p>만약, state value function의 구현으로 linear parameterized function을 사용한다면, 그의 파라미터 $w$의 업데이트 규칙은 다음처럼 될 것이다.<br>$$<br>w \leftarrow w + \alpha \cdot \delta \cdot x(s)<br>$$<br>또한, policy function을 softmax로 구현했다면, 그의 파라미터 $\theta$의 업데이트는 다음처럼 될 것이다.<br>$$<br>\theta \leftarrow \theta + \alpha \cdot \delta \cdot (x_h(s,a) - \sum_b \pi(b|s,\theta) x_h(s,b))<br>$$</p>
<p>이때, $x(s)$는 4개로 구성된 state feature이고, $x_h(s,a)$는 12개로 구성된 state-action pair feature인데, action $a$에 해당하는 4개의 feature만 가저온 것이다.</p>
<p>($\nabla \text{ln} \pi(a|s,\theta) = x_h(s,a) - \sum_b \pi(b|s,\theta) x_h(s,b)$이기 때문; $\nabla h(s,a,\theta) = x_h(s,a)$)</p>
<h2 id="Actor-Critic-for-Continuous-Actions"><a href="#Actor-Critic-for-Continuous-Actions" class="headerlink" title="Actor-Critic for Continuous Actions"></a>Actor-Critic for Continuous Actions</h2><p>Action 개수만큼 state feature를 stacking하는 것도 가능한 action 집합이 discrete할때만 유효한 것이다. 만약, action 집합이 continuous하다면, critic을 softmax로 모델링 할 수 없다.</p>
<p>이런 경우에는, critic을 각 state에 따른 action을 distribution으로 모델링하는 것이다. 예를 들어, state에 따른 action 분포를 gaussian distribution으로 모델링한다고 가정한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200311114613408.png" alt="image-20200311114613408"></p>
<p>이때, critic은 이 gaussian distribution의 파라미터 $\mu, \sigma$를 모델링하게 된다. 즉, $\theta = { \theta_{\mu}, \theta_{\sigma} }$가 된다.</p>
<p>때로는 discrete한 액션 집합이더라도, 그 수가 많고 촘촘하다면, continuous하게 취급하는 것도 도움이 된다. Distribution으로 모델링할 때의 장점은 distribution이기 때문에, 하나의 action에 대한 업데이트도 범위의 액션에 영향을 미치기 때문에 action generalization이 실현된다.</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/wiki/studynotes/reinforcement-learning/Appendix-1-RL-Cheatsheet/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">다음 글</strong>
            <div class="article-nav-title">
                
                    Appendix 01. Which Algorithm Should be selected
                
            </div>
        </a>
    
    
        <a href="/wiki/studynotes/reinforcement-learning/12-Controls-with-Approximation/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">이전 글</strong>
            <div class="article-nav-title">12. Controls with Approximation</div>
        </a>
    
</nav>





    
    

    <script src="https://utteranc.es/client.js"
        repo="taeuk-gang/taeuk-gang.github.io"
        issue-term="title"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>



<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Lee Jaeyoung &copy; 2020 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <!-- <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a> -->
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>