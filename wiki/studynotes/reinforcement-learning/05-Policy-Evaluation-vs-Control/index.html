<!DOCTYPE html>
<html lang="ko">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>05. Policy Evaluation &amp; Control | Jaeyoung&#39;s Blog</title>
    
    
        <meta name="keywords" content="StudyNotes,ReinforcementLearning">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Policy Evaluation &amp;amp; Control참고: Coursera Reinforcement Learning (Alberta Univ.) 현재 가지고있는 policy가 좋은지 평가하고(evaluation) 더 좋은 policy로 향상시키는 작업(control)을 의미한다. 현재 가지고 있는 policy $\pi$와 dynamics of envir">
<meta name="keywords" content="StudyNotes,ReinforcementLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="05. Policy Evaluation &amp; Control">
<meta property="og:url" content="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/index.html">
<meta property="og:site_name" content="Jaeyoung&#39;s Blog">
<meta property="og:description" content="Policy Evaluation &amp;amp; Control참고: Coursera Reinforcement Learning (Alberta Univ.) 현재 가지고있는 policy가 좋은지 평가하고(evaluation) 더 좋은 policy로 향상시키는 작업(control)을 의미한다. 현재 가지고 있는 policy $\pi$와 dynamics of envir">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200121183345364.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200121183703146.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200122120728463.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200122121014540.png">
<meta property="og:updated_time" content="2020-03-05T03:45:14.440Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="05. Policy Evaluation &amp; Control">
<meta name="twitter:description" content="Policy Evaluation &amp;amp; Control참고: Coursera Reinforcement Learning (Alberta Univ.) 현재 가지고있는 policy가 좋은지 평가하고(evaluation) 더 좋은 policy로 향상시키는 작업(control)을 의미한다. 현재 가지고 있는 policy $\pi$와 dynamics of envir">
<meta name="twitter:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200121183345364.png">
    

    
        <link rel="alternate" href="/atom.xml" title="Jaeyoung&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">
    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Jaeyoung&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Main</a>
                
                    <a class="main-nav-link" href="/archives">TimeLine</a>
                
                    <a class="main-nav-link" href="/categories">Category</a>
                
                    <a class="main-nav-link" href="/tags">Tag</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '포스트',
            PAGES: 'Pages',
            CATEGORIES: '카테고리',
            TAGS: '태그',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Main</a></td>
                
                    <td><a class="main-nav-link" href="/archives">TimeLine</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Category</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tag</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>카테고리</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Log
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/Blog-Init/">Init Blog</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Study Notes
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Bayesian Statistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/01_Probability/">01. Probability</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/02_Distribution/">02. Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/">03. Frequentist Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/">04. Bayesian Inference</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/">05. Credible Intervals</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/">06. Prior Predictive Distribution</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/07_Priors/">07. Priors</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/">08. Bayesian Modeling</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/">09. Monte Carlo Estimation</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/">10. Markov Chain Monte Carlo</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/11_Linear_Regression/">11. Linear Regression</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/">12. Prior Sensitivity Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/">13. Hierarchical Models</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/">14. Predictive Simulations</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/">Appendix 1. Maximize a Posterior</a></li>  <li class="file"><a href="/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/">Appendix 2. Empirical Bayes</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Machine Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/">Machine Learning</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Principal-Component-Analysis/">Principal Component Analysis</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Lagrangian-Multiplication/">Lagrangian Multiplication</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Distillation-Methods/">Distillation Methods</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st/">Restrict Boltzmann Machines 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd/">Restrict Boltzmann Machines 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/">Hidden Markov Models 1</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/">Hidden Markov Models 2</a></li>  <li class="file"><a href="/wiki/studynotes/machine-learning/KL Divergence/">KL Divergence</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Reinforcement Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/01_Introduction/">01. Introduction</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/">02. K-arm Bandits Problems</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/">03. Markov Decision Process</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">04. Policies and Value Functions</a></li>  <li class="file active"><a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">05. Policy Evaluation & Control</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/">06. Sample-based Reinforcement Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/07-Off-Policy-Learning/">07. Off-policy Learning</a></li>  <li class="file"><a href="/wiki/studynotes/reinforcement-learning/08-Temporal-Difference-Learning/">08. Temporal Difference Learning</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                     </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/Study-Notes/">Study Notes</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Study-Notes/Reinforcement-Learning/">Reinforcement Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/ReinforcementLearning/">ReinforcementLearning</a>, <a class="tag-link" href="/tags/StudyNotes/">StudyNotes</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">
            <time datetime="2020-03-03T01:00:04.000Z" itemprop="datePublished">2020-03-03</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/raw/writing/source/_posts/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/edit/writing/source/_posts/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/taeuk-gang/taeuk-gang.github.io/commits/writing/source/_posts/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            05. Policy Evaluation &amp; Control
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">카탈로그</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Policy-Evaluation-amp-Control"><span class="toc-number">1.</span> <span class="toc-text">Policy Evaluation &amp; Control</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Evaluation"><span class="toc-number">1.1.</span> <span class="toc-text">Policy Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Iterative-Policy-Evaluation"><span class="toc-number">1.1.1.</span> <span class="toc-text">Iterative Policy Evaluation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Control"><span class="toc-number">1.2.</span> <span class="toc-text">Policy Control</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy-Improvement-Theorem"><span class="toc-number">1.2.1.</span> <span class="toc-text">Policy Improvement Theorem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Iteration-Dynamic-Programming"><span class="toc-number">1.3.</span> <span class="toc-text">Policy Iteration - Dynamic Programming</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generalized-Policy-Iteration"><span class="toc-number">1.4.</span> <span class="toc-text">Generalized Policy Iteration</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-Iteration"><span class="toc-number">1.4.1.</span> <span class="toc-text">Value Iteration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Asynchronous-Dynamic-Programming"><span class="toc-number">1.4.2.</span> <span class="toc-text">Asynchronous Dynamic Programming</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Monte-Carlo-Methods"><span class="toc-number">1.4.3.</span> <span class="toc-text">Monte Carlo Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Brute-Force-Estimation"><span class="toc-number">1.4.4.</span> <span class="toc-text">Brute-Force Estimation</span></a></li></ol></li></ol></li></ol>
                </div>
            
        
        
            <h1 id="Policy-Evaluation-amp-Control"><a href="#Policy-Evaluation-amp-Control" class="headerlink" title="Policy Evaluation &amp; Control"></a>Policy Evaluation &amp; Control</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p>
<p>현재 가지고있는 policy가 좋은지 평가하고(evaluation) 더 좋은 policy로 향상시키는 작업(control)을 의미한다. 현재 가지고 있는 policy $\pi$와 dynamics of environment를 표현하는 $p(s’,r|s,a)$분포가 있으면, dynamic programming을 통해 value function을 계산해낼 수 있고, 그 value function을 이용해서 policy를 평가(evaluation)할 수 있다. 또한, dynamic programming을 통해 더 나은 policy를 찾을 수 있다(control).</p>
<p>Optimal policy를 찾기 위해서 policy evaluation과 control을 이용하게 된다.</p>
<h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>어떤 policy가 좋은지 평가하는 방법은 value function를 보는 것이 있겠다. Policy evaluation이란, 주어진 policy에 대해 value function을 구하는 것을 말한다.</p>
<p>임의의 policy를 하나 설정하고, 각 액션에 대해 immediate reward를 설정한 후에는 value function을 계산할 수 있을 것이다.</p>
<p><strong>어찌됬든 요약하면, policy evaluation은 그 policy를 이용한 value function을 계산하는 것을 말한다.</strong></p>
<p>주어진 policy에 대해 value function을 정확히 계산하기보단, approximation 방법을 이용한다.</p>
<h3 id="Iterative-Policy-Evaluation"><a href="#Iterative-Policy-Evaluation" class="headerlink" title="Iterative Policy Evaluation"></a>Iterative Policy Evaluation</h3><p>주어진 policy를 이용하여 value function을 approximation하는 한 가지 방법으로, dynamic programming을 통한 iterative 방법이다. 처음에 모든 state의 value를 0으로(또는 임의의 아무 숫자) 초기화시킨 후, state-Bellman equation을 통해 모든 state의 value를 수렴할때까지 업데이트하게 된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterative_policy_evaluation</span><span class="params">(policy, p, states, threshold=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">    values_curr = np.zeros(states.shape)</span><br><span class="line">    max_diff = <span class="number">1e6</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        </span><br><span class="line">        values_curr = compute_state_bellman_equation(policy, p, values_curr)</span><br><span class="line">        max_diff = np.max(np.sqrt((values_next - values_curr)**<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> max_diff &lt; threshold:</span><br><span class="line">            <span class="keyword">return</span> values_curr</span><br></pre></td></tr></table></figure>

<p>일단 방법은 다음과 같다.</p>
<ol>
<li>두 개의 value matrix $V, V’$를 만든다. $V$는 현재 value function을 저장할 matrix, $V’$는 value function을 계산한 결과를 저장할 matrix이다.</li>
<li>State bellman equation $v_{\pi}(s)$을 통해 $V$만을 이용해서 $V’$를 계산한다.</li>
<li>$\epsilon = max|V - V’|^2$를 계산한다. 즉, 가장 value가 크게 변한 state를 찾는다.</li>
<li>어떤 작은 수 $\theta$에 대해 $\epsilon \leq \theta$이라면, value function이 충분히 수렴되었다고 간주하고 $V’$를 반환한다.</li>
<li>$\epsilon &gt; \theta$이라면, $V \leftarrow V’$로 대입하고 2번으로 돌아간다.</li>
</ol>
<h2 id="Policy-Control"><a href="#Policy-Control" class="headerlink" title="Policy Control"></a>Policy Control</h2><p>Policy control이란, 주어진 policy와 그것으로부터 만들어낸 value function을 가지고, optimal policy를 찾는 과정을 말한다.</p>
<h3 id="Policy-Improvement-Theorem"><a href="#Policy-Improvement-Theorem" class="headerlink" title="Policy Improvement Theorem"></a>Policy Improvement Theorem</h3><p>Action value를 비교하는데, 현재 상태에서 액션을 원래 policy $\pi$에 따라 선택한 후, policy $\pi$를 따르는 action value를 $q_{\pi}(s, \pi(s))$라고 하자. 또, 같은 상태에서 액션을 다른 policy $\pi’$에 따라 선택한 후, 원래 policy $\pi$를 따르는 action value를 $q_{\pi}(s, \pi’(s))$라고 하자.</p>
<p>$q_{\pi} (s, \pi’(s)) \geq q_{\pi} (s, \pi(s))$를 만족하면, 적어도 $\pi’$는 $\pi$보다는 좋다라는 이론이다. 만약, 두 action value가 같다면, 이미 optimal일 확률이 높다.</p>
<p>위 이론에 따라, 현재 policy보다 좀 더 좋은 policy를 찾는 방법은, 주어진 value function에 따라 확률적으로 action을 선택하던 현재 policy를 greedy한 deterministic한 policy로 바꾸는 것이다.<br>$$<br>\pi’ = \underset{a}{\text{argmax} } \sum_{s’,r} p(s’,r|s,a)[r + \gamma \cdot v_{\pi}(s’)] ~ \text{(for all state } s\text{)}<br>$$</p>
<h2 id="Policy-Iteration-Dynamic-Programming"><a href="#Policy-Iteration-Dynamic-Programming" class="headerlink" title="Policy Iteration - Dynamic Programming"></a>Policy Iteration - Dynamic Programming</h2><p><strong>Optimal policy를 찾는 알고리즘</strong>으로, 다음과 같은 과정으로 이루어진다. 일단 어떤 state에서 어떤 액션을 취하면 어떤 immediate reward를 받는지는 이미 알고 있다고 가정한다. Immediate reward의 분포 $p(r|s)$은 exploration &amp; exploitation 으로 추정해야 하거나 개발자가 이미 정해놓거나?</p>
<ol>
<li>Initialize $\pi_0$. 즉, 최초의 policy를 만들고 이를 현재의 policy $\pi$로 삼는다. 최초의 policy는 아무거나로 한다. 모든 액션을 uniform distribution에 따라 선택하는 policy로 해도 된다.</li>
<li>Evaluate $\pi$. 즉, 주어진 policy에 대해 value function을 계산한다.</li>
<li>Control $\pi$. 즉, 계산된 value function을 바탕으로 모든 상태에서 greedy한 action을 선택하는 새로운 policy $\pi’$를 만든다.</li>
<li>$\pi’ = \pi$라면, $\pi$를 반환하고 끝낸다. 아니라면, $\pi$에  $\pi’$를 대입하고 2번으로 간다.</li>
</ol>
<p>이 과정을 통틀어서 policy iteration 방법이라고 부른다. 다음은 전체 pseudo code.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200121183345364.png" alt="image-20200121183345364"></p>
<p>일단, 가장 처음 policy를 제외하고 이후 control에 의해 만들어진 모든 policy는 greedy하고 deterministic한 policy이다. 하지만, 이것은 새로 evaluate로 생성된 value function에서 greedy하지 않게 된다.</p>
<p>즉, evaluate과정을 거처서 만들어낸, 현 policy를 따르는 value function에서 현재 policy가 greedy하지 않게 되고,</p>
<p>control하는 과정을 거치면 greedy한 policy를 만들 수 있지만, 이건 또 다시 policy evalutation을 통해 더 좋은 policy가 있다는 것이 밝혀진다.</p>
<p>이 과정을 통해, 더 이상 좋은 policy가 없을 때 까지 수렴하게 된다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200121183703146.png" alt="image-20200121183703146"></p>
<h2 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h2><p>Policy Iteration의 일반적인 형태.</p>
<p>앞서 나온 policy iteration은 policy evaluation과 control를 번갈아가면서 수행했다. 그리고, evaluation에서는 value function이 수렴할때까지 loop를 돌렸고, 수렴한 다음에야 policy control을 시행했다. Policy control 또한 완전한 greedy한 deterministic policy를 선택했다. 하지만, generalized policy iteration은 다음처럼 작동한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200122120728463.png" alt="image-20200122120728463"></p>
<p>Policy evaluation은 loop를 돌지 않고 한번만 회전하고 policy control또한 조금 완화된 greedy action을 선택하게끔 한다.</p>
<h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><p>Generalized policy iteration의 한 방법으로, policy evaluation과 control를 번갈아서 수행하지 않고 evaluation을 policy와 관계없이 value값에 대해서만 수행해서 수렴시키고 control를 최종적으로 수행한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200122121014540.png" alt="image-20200122121014540"></p>
<p>알고리즘은 위와 같은데, value funciton을 계산할 때, “어떤 state에서는 무조건 이 액션을 선택해”라고 말하는 policy를 넣지 않고, 그냥 value를 최대로 하는 액션을 선택하도록 한다. 그리고 그 최대 value로 업데이트한다. 이 과정을 반복하면 value function 혼자서 optimal에 수렴하게 되고 optimal value function을 이용해서 policy를 뽑아낸다.</p>
<h3 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h3><p>한번 value function을 업데이트할때, 모든 state를 순차적으로 다 돌지 말고, 필요한 state에 대한 value만, 순서관계없이 업데이트하자는 것이라고 한다.</p>
<p>또한, 모든 state를 다 업데이트하는것이 아니라 관계있는 state들만 업데이트한다.</p>
<h3 id="Monte-Carlo-Methods"><a href="#Monte-Carlo-Methods" class="headerlink" title="Monte Carlo Methods"></a>Monte Carlo Methods</h3><p>지금까지 dynamic programming을 통해 value function과 policy를 계산 및 추정했는데, dynamic programming을 통한 방법 외에도 여러가지 방법이 존재한다.</p>
<p>Monte carlo method는 하나의 state에 대해 각 액션을 많이 취해보고 Monte carlo estimation을 통해 value 추정값을 계산하자는 방법이다. 즉, 그 state에서 각각 액션을 많이 취해보고 얻은 reward들을 단순 평균내자는 이야기이다. 이 방법은 optimal policy를 매우 정확하게 찾을 것을 보장해준다.(단, action을 해서 reward를 한 trial이 많아야 한다.)</p>
<p>Monte carlo estimation의 단점은 모든 state에서 모든 액션을 많이 취해봐야 정확한 value function을 추정할 수 있는데, 그게 현실적으로 불가능하다.</p>
<h3 id="Brute-Force-Estimation"><a href="#Brute-Force-Estimation" class="headerlink" title="Brute-Force Estimation"></a>Brute-Force Estimation</h3><p>Brute-force 방법은 간단하다. 가능한 모든 deterministic policy 조합을 나열하고 그중에서 optimal policy를 찾는 것을 말한다. 이 방법 역시 optimal policy를 반드시 찾을 것을 보장해준다. 하지만, action수에 따라 가능한 policy 조합이 exponential하게 증가한다. 그래서 사실상 적용이 불가능하다.</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">다음 글</strong>
            <div class="article-nav-title">
                
                    06. Sample-based Reinforcement Learning
                
            </div>
        </a>
    
    
        <a href="/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">이전 글</strong>
            <div class="article-nav-title">04. Policies and Value Functions</div>
        </a>
    
</nav>





    
    

    <script src="https://utteranc.es/client.js"
        repo="taeuk-gang/taeuk-gang.github.io"
        issue-term="title"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>



<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Lee Jaeyoung &copy; 2020 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <!-- <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a> -->
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>