<!DOCTYPE html>
<html lang="ko">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    
    <title>12. Controls with Approximation | Jaeyoung&#39;s Blog</title>
    
    
        <meta name="keywords" content="StudyNotes,ReinforcementLearning">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Controls with Approximation Value function을 function approximation을 통해 모델링했다면, 이제, 주어진 value function으로 policy를 계산해 낼 차례이다. Representation of Actions Sarsa, expected Sarsa, Q-learning등을 하려면, state val">
<meta name="keywords" content="StudyNotes,ReinforcementLearning">
<meta property="og:type" content="article">
<meta property="og:title" content="12. Controls with Approximation">
<meta property="og:url" content="https://wayexists02.github.io/studynotes/reinforcement-learning/12-Controls-with-Approximation/index.html">
<meta property="og:site_name" content="Jaeyoung&#39;s Blog">
<meta property="og:description" content="Controls with Approximation Value function을 function approximation을 통해 모델링했다면, 이제, 주어진 value function으로 policy를 계산해 낼 차례이다. Representation of Actions Sarsa, expected Sarsa, Q-learning등을 하려면, state val">
<meta property="og:locale" content="ko">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307093934585.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307094147927.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307094258479.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307094518046.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307112210435.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307113711291.png">
<meta property="og:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307113828452.png">
<meta property="og:updated_time" content="2020-03-22T00:47:59.673Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="12. Controls with Approximation">
<meta name="twitter:description" content="Controls with Approximation Value function을 function approximation을 통해 모델링했다면, 이제, 주어진 value function으로 policy를 계산해 낼 차례이다. Representation of Actions Sarsa, expected Sarsa, Q-learning등을 하려면, state val">
<meta name="twitter:image" content="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307093934585.png">
    

    
        <link rel="alternate" href="/atom.xml" title="Jaeyoung&#39;s Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon/favicon.ico">
    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/open-sans/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">
    <script src="/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
</head>
</html>
<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">Jaeyoung&#39;s Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Main</a>
                
                    <a class="main-nav-link" href="/archives">TimeLine</a>
                
                    <a class="main-nav-link" href="/categories">Category</a>
                
                    <a class="main-nav-link" href="/tags">Tag</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '포스트',
            PAGES: 'Pages',
            CATEGORIES: '카테고리',
            TAGS: '태그',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Main</a></td>
                
                    <td><a class="main-nav-link" href="/archives">TimeLine</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Category</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tag</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="검색" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>카테고리</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Log
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/Blog-Init/">Init Blog</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Study Notes
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Bayesian Statistics
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/studynotes/bayesian-statistics/01_Probability/">01. Probability</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/02_Distribution/">02. Distribution</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/03-Frequentist-Inference/">03. Frequentist Inference</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/04-Bayesian-Inference/">04. Bayesian Inference</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/05-Credible-Intervals/">05. Credible Intervals</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/">06. Prior Predictive Distribution</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/07_Priors/">07. Priors</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/08_Bayesian_Modeling/">08. Bayesian Modeling</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/">09. Monte Carlo Estimation</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/">10. Markov Chain Monte Carlo</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/11_Linear_Regression/">11. Linear Regression</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/">12. Prior Sensitivity Analysis</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/13_Hierarchical_models/">13. Hierarchical Models</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/14_Predictive_Simulation/">14. Predictive Simulations</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/15-definition-of-mixture-models/">15. Definition of Mixture Models</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/APPENDIX-1-MAP/">Appendix 1. Maximize a Posterior</a></li>  <li class="file"><a href="/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/">Appendix 2. Empirical Bayes</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Machine Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/studynotes/machine-learning/Distillation-Methods/">Distillation Methods</a></li>  <li class="file"><a href="/studynotes/machine-learning/Hidden-Markov-Models-1/">Hidden Markov Models 1</a></li>  <li class="file"><a href="/studynotes/machine-learning/Hidden-Markov-Models-2/">Hidden Markov Models 2</a></li>  <li class="file"><a href="/studynotes/machine-learning/KL-Divergence/">KL Divergence</a></li>  <li class="file"><a href="/studynotes/machine-learning/Lagrangian-Multiplier/">Lagrangian Multiplication</a></li>  <li class="file"><a href="/studynotes/machine-learning/Machine-Learning/">Machine Learning</a></li>  <li class="file"><a href="/studynotes/machine-learning/Principal-Component-Analysis/">Principal Component Analysis</a></li>  <li class="file"><a href="/studynotes/machine-learning/Restrict-Boltzmann-Machines-1st/">Restrict Boltzmann Machines 1</a></li>  <li class="file"><a href="/studynotes/machine-learning/Restrict-Boltzmann-Machines-2nd/">Restrict Boltzmann Machines 2</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Reinforcement Learning
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/studynotes/reinforcement-learning/01_Introduction/">01. Introduction</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/">02. K-arm Bandits Problems</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/03-Markov-Decision-Process/">03. Markov Decision Process</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/">04. Policies and Value Functions</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/">05. Policy Evaluation & Control</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/">06. Sample-based Reinforcement Learning</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/07-Off-Policy-Learning/">07. Off-policy Learning</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/08-Temporal-Difference-Learning/">08. Temporal Difference Learning</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/09-Models-and-Planning/">09. Models and Planning</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/10-Prediction-and-Control-with-Function-Approximation/">10. Prediction and Control with Function Approximation</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/11-Feature-Construction/">11. Feature Construction</a></li>  <li class="file active"><a href="/studynotes/reinforcement-learning/12-Controls-with-Approximation/">12. Controls with Approximation</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/13-Policy-Gradient/">13. Policy Gradient</a></li>  <li class="file"><a href="/studynotes/reinforcement-learning/Appendix-1-RL-Cheatsheet/">Appendix 01. Which Algorithm Should be selected</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                     <li class="file"><a href="/menu/"></a></li>  <li class="file"><a href="/index/">Home</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-studynotes/reinforcement-learning/12-Controls-with-Approximation" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/Study-Notes/">Study Notes</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/Study-Notes/Reinforcement-Learning/">Reinforcement Learning</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link" href="/tags/ReinforcementLearning/">ReinforcementLearning</a>, <a class="tag-link" href="/tags/StudyNotes/">StudyNotes</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/studynotes/reinforcement-learning/12-Controls-with-Approximation/">
            <time datetime="2020-03-22T00:22:08.000Z" itemprop="datePublished">2020-03-22</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                            <div class="article-meta-button">
                                <a href='https://github.com/wayexists02/wayexists02.github.io/raw/writing/source/_posts/studynotes/reinforcement-learning/12-Controls-with-Approximation.md'> Source </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/wayexists02/wayexists02.github.io/edit/writing/source/_posts/studynotes/reinforcement-learning/12-Controls-with-Approximation.md'> Edit </a>
                            </div>
                            <div class="article-meta-button">
                                <a href='https://github.com/wayexists02/wayexists02.github.io/commits/writing/source/_posts/studynotes/reinforcement-learning/12-Controls-with-Approximation.md'> History </a>
                            </div>
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            12. Controls with Approximation
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">카탈로그</strong>
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#null"><span class="toc-number">1.</span> <span class="toc-text">Controls with Approximation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Representation-of-Actions"><span class="toc-number">1.1.</span> <span class="toc-text">Representation of Actions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Controls-with-Function-Approximation"><span class="toc-number">1.2.</span> <span class="toc-text">Controls with Function Approximation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sarsa-with-Function-Approximation"><span class="toc-number">1.2.1.</span> <span class="toc-text">Sarsa with Function Approximation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Expected-Sarsa-with-Function-Approximation"><span class="toc-number">1.2.2.</span> <span class="toc-text">Expected Sarsa with Function Approximation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-learning-with-Function-Approximation"><span class="toc-number">1.2.3.</span> <span class="toc-text">Q-learning with Function Approximation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Exploration-with-Function-Approximation"><span class="toc-number">1.3.</span> <span class="toc-text">Exploration with Function Approximation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#epsilon-greedy-with-Function-Approximation"><span class="toc-number">1.3.1.</span> <span class="toc-text">$\epsilon$-greedy with Function Approximation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Average-Rewards"><span class="toc-number">1.4.</span> <span class="toc-text">Average Rewards</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Differential-Sarsa"><span class="toc-number">1.4.1.</span> <span class="toc-text">Differential Sarsa</span></a></li></ol></li></ol></li></ol>
                </div>
            
        
        
            <h1>Controls with Approximation</h1>
<p>Value function을 function approximation을 통해 모델링했다면, 이제, 주어진 value function으로 policy를 계산해 낼 차례이다.</p>
<h2 id="Representation-of-Actions">Representation of Actions</h2>
<p>Sarsa, expected Sarsa, Q-learning등을 하려면, state value function보단, action value function이 더 유용한데, 앞에서 봤던 function approximation방법은 state value function을 추정하는 것이었다.</p>
<p>Action을 추가한 function approximation은 두 가지 방법이 있을 수 있다.</p>
<ol>
<li>
<p>Action별로 따로 function을 모델링한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307093934585.png" alt="image-20200307093934585"></p>
<p>즉, 어떤 action 1개당 1개의 function이 있는 셈이며, 위 그림처럼 stacking해서 하나의 linear 형태로 표현이 가능하다. 하지만, 이 경우, action끼리의 generalization이 일어나지 않는다. 즉, $a_0$의 function과 $a_1$의 function은 서로 다른 weight를 사용하기 때문에 서로 영향을 미치지 못한다.</p>
<p>신경망으로 치자면 다음처럼 될 것이다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307094147927.png" alt="image-20200307094147927"></p>
<p>마지막 hidden layer의 output이 state의 representation이며, 그것과 최종 output layer사이의 weight는 action끼리 서로 공유되지 않는다.</p>
</li>
<li>
<p>Action도 function의 입력으로 넣는다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307094258479.png" alt="image-20200307094258479"></p>
<p>신경망의 output은 하나로 통일해버리고, action은 input으로 넣는 것이다. 이렇게되면, state뿐 아니라 action 사이에서도 weight가 공유되므로, action generalization도 수행될 것이다.</p>
</li>
</ol>
<h2 id="Controls-with-Function-Approximation">Controls with Function Approximation</h2>
<h3 id="Sarsa-with-Function-Approximation">Sarsa with Function Approximation</h3>
<p>방법은 tabular Sarsa와 semi-gradient TD와 상당히 유사하다. Value function을 weight로 모델링한 후, weight와 policy를 initialization한다. 그리고, 다음 코드를 구현한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307094518046.png" alt="image-20200307094518046"></p>
<p>Semi-gradient 방법을 사용한다.</p>
<h3 id="Expected-Sarsa-with-Function-Approximation">Expected Sarsa with Function Approximation</h3>
<p>Sarsa와 Expected Sarsa는 Bellman equation 형태만 조금 다르므로, 그것만 수정해주면 된다. 다음은 Sarsa의 업데이트 방식이다.<br>
$$<br>
w \leftarrow w + \alpha(R_{t+1} + \gamma \cdot \hat{q}(s_{t+1}, a_{t+1}, w) - \hat{q}(s_t, a_t, w)) \nabla \hat{q}(s_t, a_t, w)<br>
$$<br>
다음은 expected Sarsa의 업데이트 방식이다.<br>
$$<br>
w \leftarrow w + \alpha(R_{t+1} + \gamma \cdot \sum_{a_{t+1}} \pi(a_{t+1}|s_{t+1}) \hat{q}(s_{t+1}, a_{t+1}, w) - \hat{q}(s_t, a_t, w)) \nabla \hat{q}(s_t, a_t, w)<br>
$$<br>
다만, action방향으로 expectation을 계산할 수 있어야 한다. (여기서는 action set이 finite하다고 가정)</p>
<h3 id="Q-learning-with-Function-Approximation">Q-learning with Function Approximation</h3>
<p>Q-learning은 expected Sarsa의 특수한 형태이다. 즉, expectation값을 구하는 대신 maximum action value를 취한다.<br>
$$<br>
w \leftarrow w + \alpha (R_{t+1} + \gamma \cdot \underset{a_{t+1}}{ \text{argmax} } ~ \hat{q}(s_{t+1}, a_{t+1}, w) - \hat{q}(s_t, a_t, w)) \nabla \hat{q}(s_t, a_t, w)<br>
$$</p>
<h2 id="Exploration-with-Function-Approximation">Exploration with Function Approximation</h2>
<p>Function approximation에서도 exploration-exploitation dilema가 발생한다. 따라서, 이를 완화시켜야 하는데, function approxmiation은 각 state 사이에 value generalization이 이뤄지기 때문에 tabular settings보다 exploration에서 제한적이다.</p>
<p>예를들어, optimistic initial value를 들어본다. Optimistic initial value setting에서는 처음에 value function을 매우 높은 값으로 초기화하고, 어떤 state를 방문할수록 방문한 state의 value function이 낮아지며, 자연스럽게 아직 방문하지 않은 state에 방문하게 된다. 학습이 진행될수록, 낮은 value를 가진 state의 value funciton은 낮은 값이 되어 더 이상 방문하지 않게 될 것이다.</p>
<p>하지만, function approximation setting에서는 이것이 유효하지 않은데, generalization이 이뤄지기 때문에 방문하지 않은 state의 value function도 같이 낮아진다는 것이다. 따라서, optimistic initial value의 의도와는 다르게 흘러가며, exploration이 제대로 이뤄지지 않는다.</p>
<h3 id="epsilon-greedy-with-Function-Approximation">$\epsilon$-greedy with Function Approximation</h3>
<p>하지만, $\epsilon$-greedy 방식은 어떤 function approximation 방법과도 융합될 수 있다.</p>
<ul>
<li>
<p>$1 - \epsilon$확률에 따라 exploitation<br>
$$<br>
a \leftarrow \underset{a}{ \text{argmax} } ~ \hat{q}(s, a, w)<br>
$$</p>
</li>
<li>
<p>$\epsilon$확률에 따라 exploration<br>
$$<br>
a \leftarrow \text{random}(a)<br>
$$</p>
</li>
</ul>
<p>이외에 function approximation setting에서의 exploration-exploitation 조화는 아직 연구중인 분야라고 한다.</p>
<h2 id="Average-Rewards">Average Rewards</h2>
<p>지금까지, 어떤 state에서 어떤 action을 취했을 때의 value는 discounting을 이용한 future reward의 합으로 정의했다. 하지만, 이것은 discount라는 hyperparameter가 존재하며, 이것을 정하는 것은 어떤 문제를 푸느냐에 따라 크게 달라질 수 있다. 때로는 discount rate가 알고리즘을 잘못된 방향으로 학습시킬 수 있다(큰 reward를 받는게 너무 먼 미래인 경우 discount가 너무 많이 된다). 이것은 continuous task일때도, 마찬가지로, 당장은 작은 reward, 먼 미래에 다소 큰 reward를 받는 액션 중 택하는 문제에서, discount는 큰 영향을 준다.</p>
<p>Continuous task를 위한 RL알고리즘들은 보통 discounting대신 average reward방식을 사용한다고 한다.</p>
<p>Average reward는 이것을 해결하기 위해서 나왔으며, 어떤 policy를 따를 때, 앞으로 받을 reward의 평균을 말한다.<br>
$$<br>
r(\pi) = \lim_{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^h E[R_t|S_t,A_{0:t-1} \sim \pi]<br>
$$<br>
Value의 평균이 아니라, reward의 평균이다. 이는 다음처럼 generalize할 수 있다.<br>
$$<br>
r(\pi) = \sum_s \mu_{\pi}(s) \sum_a \pi(a|s) \sum_{s’,r} p(s’,r|s,a)r<br>
$$<br>
$\mu_{\pi}(s)$는 $s$가 해당 policy $\pi$에 따른 방문 횟수 비율 분포이다.</p>
<p>예를들어, 다음 environment가 있다고 했을 때,</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307112210435.png" alt="image-20200307112210435"></p>
<p>Left로만 가라고 하는 deterministic policy $\pi_L$일때, state $s$에서의 average reward $r(\pi_L)$은 0.2이다. 왼쪽으로만 가라고 하고, 왼쪽으로 갔을 때, 총 5개의 state를 지나며, 총 +1의 reward를 얻을 수 있으니, 왼쪽으로 가는 action을 선택했을 때 평균 reward는 0.2이다. 오른쪽으로는 갈 수가 없으므로, 고려하지 않는다.</p>
<p>반면, 오른쪽으로만 가라고 하는 deterministic poliyc $\pi_R$의 경우, average reward $r(\pi_R)$은 0.4이다.</p>
<p>따라서, 두 개의 policy를 average reward로 비교했을 때, $\pi_R$이 더 좋다고 할 수 있다. <strong>즉, average reward를 통해 어떤 policy가 더 좋은지 판단할 수 있다.</strong></p>
<p>Policy 하나당 하나의 average reward를 계산할 수 있다.</p>
<p>하나의 policy에서 average reward를 계산했다면, 그 policy내에서 어느 action이 좋은지에 대해서도 판단할 수 있다. 즉, value function을 새롭게 계산할 수 있다는 것이다. 이때, value는 다음처럼 정의한다.<br>
$$<br>
G_t = (R_{t+1} - r(\pi)) + (R_{t+2} - r(\pi)) + \cdots<br>
$$<br>
이때, $R - r(\pi)$를 differential return이라고 부른다. 즉, value를 미래 reward의 총합이 아닌, differential reward의 총합으로 differential return을 정의한다.</p>
<p>따라서, action value Bellman equation은 다음처럼 변형된다.<br>
$$<br>
q(s, a) = \sum_{s’, r} p(s’, r|s, a) \sum_{a’} [r - r(\pi) + \sum_{a’}\pi(a’|s’) q(s’,a’)]<br>
$$</p>
<h3 id="Differential-Sarsa">Differential Sarsa</h3>
<p>Differential return을 이용해서 Sarsa를 변형한 것을 말한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307113711291.png" alt="image-20200307113711291"></p>
<p>그런데, average reward를 계산할 때, TD error로 계산하는게 더 효과적이라고 한다.</p>
<p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200307113828452.png" alt="image-20200307113828452"></p>
<p>(이렇게 되면 average reward가 아니라 average value로 differential return을 계산?)</p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/studynotes/reinforcement-learning/13-Policy-Gradient/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">다음 글</strong>
            <div class="article-nav-title">
                
                    13. Policy Gradient
                
            </div>
        </a>
    
    
        <a href="/studynotes/machine-learning/KL-Divergence/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">이전 글</strong>
            <div class="article-nav-title">KL Divergence</div>
        </a>
    
</nav>





    
    

    <script src="https://utteranc.es/client.js"
        repo="taeuk-gang/taeuk-gang.github.io"
        issue-term="title"
        label="comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>



<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Lee Jaeyoung &copy; 2020 
            <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a>
            <!-- <br> Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>. Theme - <a href="https://github.com/zthxxx/hexo-theme-Wikitten">wikitten</a> -->
            
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
            
        </div>
    </div>
</footer>

        

    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>