<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jaeyoung&#39;s Blog</title>
  
  <subtitle>Jaeyoung&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jaeyoung-blog.github.io/"/>
  <updated>2020-03-03T00:54:07.087Z</updated>
  <id>https://jaeyoung-blog.github.io/</id>
  
  <author>
    <name>Lee Jaeyoung</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>01. Introduction</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/01_Introduction/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/01_Introduction/</id>
    <published>2020-03-03T01:00:00.000Z</published>
    <updated>2020-03-03T00:54:07.087Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Reinforcement Learning이란, 무엇을 할지에 대해 학습하는 것이다. 다르게 말하면, 어떤 상황이 입력으로 들어가서 어떤 액션이 출력되는 함수를 학습하는 것이다.</p><p>Reinforcement learning에선, 두 가지 중요한 특징이 있는데, 다음과 같다.</p><ul><li>Trails &amp; erros search</li><li>Delayed rewards</li></ul><p>물론 그 전에 environment를 인지할 수 있는 센서가 있어서 상황을 바탕으로 위 두 가지 특징이 발현된다.</p><h2 id="Overview-of-Reinforcement-Learning"><a href="#Overview-of-Reinforcement-Learning" class="headerlink" title="Overview of Reinforcement Learning"></a>Overview of Reinforcement Learning</h2><p>Supervised learning과 unsupervised learning은 다음과 같은 특성을 지닌다.</p><ul><li>숫자로된 테이블 형태의 데이터가 존재한다. ($X: N × D$$, $$Y: N × 1$) 이 두가지 데이터를 모두 넣고 모델을 학습하게 된다.</li><li>Static한 학습만 가능하다. 데이터를 추가하려면, 기존 데이터도 모두 넣고 fine-tuning 해야 하는 경우가 많다. 따라서, online-learning에 매우 불리하다.</li><li>시간이라는 개념이 없다(Sequence라는 개념은 있어도…). 그저 $X$$를 받으면 $$Y$를 줄 뿐. </li><li>Supervised learning의 경우, $Y$$가 $$X$ 동시에 주어지기에, 즉각적인 피드백이 있다.</li></ul><p>반면, reinforcement learning은 다음과 같은 차이점이 있다.</p><ul><li>데이터가 매우 비정형적이다. 로봇의 경우, environment로부터 받은 카메라 정보와 여라가지 센서 정보나 environment representation 정보 등이 있을 수 있다.</li><li>Dynamic하게 학습한다. Reinforcement learning은 데이터를 모아서 데이터셋을 만들어서 학습하는 형태가 아니라, environment에서 action을 취한 결과 피드백을 얻고 학습하는 형태이다. 즉, 그 자체가 그냥 online learning이다.</li><li>어떤 액션을 취하면 즉각적인 피드백이 없을 수 있고, 게임이 끝날 때 까지 피드백을 얻지 못할 수도 있다. 따라서, 상대적으로 시간이라는 개념이 존재한다. 즉, 액션과 리워드가 동시에 주어지지 않고 중간에 일정 시간이 있을 수 있다.</li></ul><h3 id="Unusual-amp-Unexpected-Stretagy-in-RL"><a href="#Unusual-amp-Unexpected-Stretagy-in-RL" class="headerlink" title="Unusual &amp; Unexpected Stretagy in RL"></a>Unusual &amp; Unexpected Stretagy in RL</h3><p>Reinforcement learning은 최종 value를 최대화하면서 학습한다. 그리고, 최종 value를 가장 높게 하는 방법을 알아서 찾아나가는데, 이때, 그 방법이 소위 말해서 수단과 방법을 가리지 않는 방법일 수  있다. 또한, agent가 취하는 액션은 나중에 보면 최대 reward를 받는 방법이었다는 것이 드러나지만, 액션 하나하나를 보면 인간이 전혀 이해하지 못하는 방향의 액션일 수도 있다.</p><h3 id="Supervised-Learning-as-Reinforcement-Learning"><a href="#Supervised-Learning-as-Reinforcement-Learning" class="headerlink" title="Supervised Learning as Reinforcement Learning?"></a>Supervised Learning as Reinforcement Learning?</h3><p>액션을 취하고 리워드를 얻는다는 것은 어떻게 보면 supervised learning과 연관지을 수도 있을 것이다. Environment가 $X$가 되고 그에 적절한 optimal action이 $Y$가 되는 것이다.</p><p>하지만, supervised learning을 쓰지 않고 reinforcement learning을 쓰는 이유가 있다.</p><ul><li>계산 불가능할 정도로 많은 environment/state 경우의 수</li><li>Supervised learning은 $X$$와 $$Y$를 동시에 필요로 하지만, $Y$가 있긴 한데, $X$ 동시에 주지 못하는 경우가 있다. 이때는 supervised learning을 할 수 없다.</li></ul><h2 id="Exploitation-Exploration-Dilema"><a href="#Exploitation-Exploration-Dilema" class="headerlink" title="Exploitation-Exploration Dilema"></a>Exploitation-Exploration Dilema</h2><p>Reinforcement learning에서의 agent는 최대한 많은 reward를 얻으면서 문제를 해결해야 한다. 이미 알고 있는 문제 해결 방법중에서 가장 큰 reward를 얻을 수 있는 방법을 선택해서 문제를 해결하는 것이 합리적일 것이다(exploitation). 그러나, agent는 새로운 길을 탐색해 나가면서 더 나은 길을 찾을 필요가 있다(exploration). 하지만, exploration과정은 많은 비용이 들 수도 있고 탐험 결과가 좋은 reward를 주는 경로가 아닐 수도 있다. 그럼에도 exploration은 필요하다.</p><ul><li><p>Exploitation</p><p>이미 찾은 문제 해결 방법중에서 가장 나은 방법을 선택하는 것. 즉, 최대 reward를 찾아가는 것.</p></li><li><p>Exploration</p><p>새로운 길을 탐색하는 것. 많은 비용이 들지만, 새로운 길이 지금까지 가지고 있었던 해결 방법들 보다 더 나은 reward를 줄 수도 있다.</p></li></ul><p>Exploration은 많은 비용이 들기 때문에, 당장은 reward를 얻지 못할 수도 있다. Exploitation을 하면 당장은 많은 reward를 얻을 수 있다. 이를 exploitaiton-exploration dilema라고 부른다.</p><h2 id="Elements-of-Reinforcement-Learning"><a href="#Elements-of-Reinforcement-Learning" class="headerlink" title="Elements of Reinforcement Learning"></a>Elements of Reinforcement Learning</h2><p>크게 4가지로 나눌 수 있다.</p><ul><li><p>Policy</p><p>어떤 주어진 환경/상황에서 어떤 동작을 취해야 하는지에 대한 규칙 또는 정책, 또는 매핑 함수이다. RL에서 핵심 역할을 하며, 단순한 매핑 테이블일수도, 아주 복잡한 함수나 확률적인 모델일 수도 있다.</p></li><li><p>Reward signal</p><p>액션에 대한 결과적인 상황에 따라 agent가 어떤 reward를 받을지에 대한, 즉, 시스템의 목표를 어떻게 할 것인지에 대한 것이다.</p></li><li><p>Value</p><p>Reward는 액션마다 주어질 수 있는 것으로, 이것만 있으면 greedy하게 갈 수 있다. 이를 방지하기 위해 reward를 누적하고 시스템 전체의 reward를 바라볼 수 있게 하는 것이 value이다. 즉, 어떤 state $s_{i}$에 대한 value $value(s_i)$는 그 상태 이후, 미래의 상태들 $s_{i+1}, s_{i+2},…$로부터 얻을 수 있는 reward 기댓값이다. 즉, $\text{argmax} ~ value(s)$라 함은, 당장 greedy한 선택이 아니라, 미래에 총 reward가 높은 방향으로 액션을 선택할 수 있게 해 준다.</p><p>value-function은 당장 앞에 놓인 action에 대해 value를 계산해주는 함수?</p><p>Reinforcement learning의 주요 task는 이 value function을 추정하는 것이다. agent는 value function이 가장 높은 value를 리턴해주는 액션을 선택하면 되니까.</p></li><li><p>Model of environment</p><p>Agent가 상호작용하는 환경을 정의한 것.</p></li></ul><p>이외에, episode라는 것이 있다. episode란, agent가 게임을 시작하고 끝날 때 까지의 기간, 즉, 한 게임을 의미한다. 다만, 게임같이 “한 게임”이라는 개념이 존재하는 episodic task가 있는 반면(바둑, 스타크래프트), “한 게임”을 정의할 수 없는, continuous task도 존재한다(로봇은 수명이 다할 때 까지 끊임없이 환경과 통신함). 이 경우에는 episode가 없다.</p><p>Reinforcement learning은 궁극적으로 reward를 최대화하는 것이지만, agent는 어떤 상황에서 높은 reward를 고르는게 아니라 value가 높은 쪽을 골라야 한다.</p><p>이 value function을 정의하는 방법은 두 가지가 있을 수 있다.</p><ul><li><p>Tabular solution method</p><p>Value function은 deterministic하다. 보통 deterministic한 함수들은 입력과 출력 매핑을 테이블 형태로 표현가능하다. 따라서, deterministic한 방법을 tabular method라고 부른다.</p><ul><li>Markov Decision Process</li></ul></li><li><p>Approximate solution method</p><p>Value function은 확률적이다.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Reinforcement Learning이란, 무엇을 
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Reinforcement Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Reinforcement-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="ReinforcementLearning" scheme="https://jaeyoung-blog.github.io/tags/ReinforcementLearning/"/>
    
  </entry>
  
  <entry>
    <title>My Interpretation of Machine Learning</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/My-interpretation-of-Machine-Learning/</id>
    <published>2020-03-02T23:51:55.000Z</published>
    <updated>2020-03-03T00:54:07.086Z</updated>
    
    <content type="html"><![CDATA[<h1 id="My-Interpretation-of-Machine-Learning"><a href="#My-Interpretation-of-Machine-Learning" class="headerlink" title="My Interpretation of Machine Learning"></a>My Interpretation of Machine Learning</h1><p>통계학을 분류하는 방법으로는 여러 가지가 있지만, 다음과 같이 통계학을 분류할 수도 있다.</p><ul><li><strong>Frequentist Statistics</strong></li><li><strong>Bayesian Statistics</strong></li></ul><p>각 통계학에서 machine learning의 방법론은 약간씩 차이가 있는 듯 하다. 하지만, 기본적으로 각 통계학의 궁극적인 목표 중 하나는 우리가 모르는, sample space distribution을 최대한 추정하는 것이다.</p><p>대충, 추정하는 방법은 먼저, 우리가 수집한 데이터셋 $D$의 likelihood를 최대화 하는 분포가 진짜 sample space를 가장 유사하게 추정하는 방법이라고 가정한다.</p><p>다시 한번 반복하자면, 통계학의 목표중 하나는 우리가 모르는, sample space의 분포를 추정하는 것이다.</p><h3 id="Machine-Learning-in-Frequentist-Statistics"><a href="#Machine-Learning-in-Frequentist-Statistics" class="headerlink" title="Machine Learning in Frequentist Statistics"></a>Machine Learning in Frequentist Statistics</h3><p>Frequentist statistics에서는 많은 데이터 샘플을 뽑는 시행을 한 후, 데이터를 이용해서 모분포를 추정하는 것이라는 목표가 있다. 이것을 하기 위해 데이터셋을 바탕으로 확률 분포가 대충 어떻게 생겼을지 모델링하게 되며, 이때, 확률 분포를 모델링하는데 쓰이는 것이 바로 <strong>머신러닝</strong>이다.</p><p>데이터셋 $D$가 있다고 하자. 이 데이터셋에 있는 각 샘플들 $d_i$들은 어쨌든 모 분포(sample space distribution)에서 나올 확률이 어느 정도 높으니까 샘플링되어 우리 손에 들어왔을 것이다. Frequentist statistics에서는 바로 이것에 주목한다. 우리가 모은 데이터셋은 모 분포를 반영해서, 확률이 높은 애들이 많이 뽑히고 낮은 애들이 적게 뽑힐 것이다. 따라서, 우리가 모은 데이터셋이 뽑혀왔을 확률, 즉, likelihood를 최대화 하는 분포를 찾는다면, 이 분포가 모분포와 매우 유사할 것이라고 가정한다.</p><p>그 전에, 조건이 있다.</p><ol><li>각 데이터 샘플 $d_i$는 반드시 모든 샘플과 동일한 분포에서 샘플링되어야 한다. 즉, 서울에서 온도측정하고 북극가서 온도 측정하면 안 된다. -&gt; identical</li><li>각 데이터 샘플 $d_i$는 모두 독립적인 시행으로 인한 샘플링이어야 한다. 즉, 첫 번째 동전던지기가 세 번째 동전던지기에 영향을 주지 않는다. 이와 같이, 이 조건을 자동으로 만족시키는 샘플링도 있다. -&gt; independent</li></ol><p>위 두 가지 조건을 합쳐서, “각 데이터 샘플 $d_i$는 <strong>iid</strong>(Identical independent distribution) 하에서 샘플링 되어야 한다”고 말한다.</p><p>iid를 만족시킴으로써, 우리는 하나의 모분포를 추정하기만 하면 된다. 즉, 한 분포의 파라미터 $\theta$를 추정하기만 하면 된다.</p><p>iid를 만족시키는 시행으로 얻어진 데이터셋의 joint distribution, 즉, 데이터셋을 얻었을 확률을 다음과 같이 계산할 수 있다.</p><p>$$<br>P(D|\theta) = P(d_1, d_2,…,d_n|\theta) = P(d_1|\theta)P(d_2|\theta)\cdots P(d_n|\theta)​<br>$$</p><h4 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h4><p>앞서 말했듯이, frequentist statistics에서는 모분포(sample space distribution)을 추정하기 위해서, likelihood를 모델링하고 likelihood를 최대화 하는 확률분포를 계산한다. 이 확률분포가 모분포의 추정이 된다. Likelihood라고 함은, 우리의 데이터 셋이 샘플링되어왔을 확률이라고 보면 된다.</p><p>$$<br>\text{Likelihood Dist.} = P(d_1|\theta)P(d_2|\theta)\cdots P(d_n|\theta) = P(D|\theta)<br>$$<br>여기서 $\theta$는 모분포의 파라미터이다.</p><p>이 likelihood를 최대화하는 분포를 찾는다면, 즉, 모분포의 파라미터 $\theta$의 추정값 $\hat{\theta}$를 찾는다면, 이 분포가 모분포와 유사할 것이다 라는 것이다.</p><p>$$<br>\hat{\theta} = argmax_{\theta} P(D|\theta)​<br>$$<br>이와 같이 likelihood를 최대화 시키는 분포 파라미터 $\hat{\theta}$를 찾고, 그것을 파라미터로 하는 분포는 모분포와 가깝다는 것이 frequentist statistics에서 모분포를 추정하는 대표적인 방법이다. 이것을 MLE(Maximum likelihood estimation)라고 부른다.</p><p>하지만, 어떻게 최대화 시킬까? 무언가를 최대화 최소화시키는데 가장 먼저 떠오르는건 미분을 통해 극점을 찾는 것이다. 하지만, likelihood가 주어지지 않아서 미분또한 할 수 없다. 따라서 우리는 먼저 likelihood를 모델링해야한다. 이것은 잠시 후에 설명한다.</p><h4 id="How-to-Maximize-Likelihood"><a href="#How-to-Maximize-Likelihood" class="headerlink" title="How to Maximize Likelihood"></a>How to Maximize Likelihood</h4><p>Likelihood를 최대화하는 분포를 구한다면, 그것이 모분포와 비슷해질 것이라는 것은 알겠다. 그렇다면, 어떻게 최대화시키는지 알아야 할 것이다. </p><p>그에 앞서서 동전을 100번 던지는 시행을 예로 들자. 우리는 동전을 100번 던졌을 때, 앞면이 몇번 나올까에 대한 확률 분포를 추정하고 싶다. 이때, <strong>각 시행은 동전 1번 던지는게 아니라 100번 던지는게 1번의 시행이다.</strong> 우리는 시행 1회에 대한 확률을 먼저 모델링해야 한다. 이것은 binomial distribution으로 모델링할 수 있을 것이다.</p><p>$$<br>\text{i-th experiment} = P(d_i|\theta) = \begin{pmatrix} 100 \ n_i \end{pmatrix} \theta^{n_i} (1 - \theta)^{100-n_i}​<br>$$<br>$i$번째의 시행에서는 100번 동전을 던지고 $n_i$회의 앞면이 나왔다. 그리고 동전이 앞면이 나올 확률은 $\theta$이다.</p><p>모든 시행은 iid조건을 만족한다면, identical한 distribution에서 샘플링된 데이터이므로 모든 시행에서 $\theta$는 같다. 이 시행을 1000회 해서 1000개의 데이터를 모았다고 가정한다. 그럼 likelihood는 이들을 곱한 것이다.</p><p>$$<br>P(D|\theta) = P(d_1|\theta)P(d_2|\theta)\cdots P(d_1000|\theta)<br>$$<br>이때, likelihood는 파라미터 $\theta$에 대한 함수가 된다. Likelihood는 이처럼 분포 파라미터의 함수가 된다. 그런데, 동전던지기는 우리가 미리 잘 알고있다시피 베르누이 시행이고, 이들을 100번 던졌을때 앞면이 몇번 나올까에 대한 것은 binomial distribution을 따른다. 따라서 <strong>시행 1회를 binomial distribution으로 모델링할 수 있었지만, 일반적으로 데이터 샘플링을 모델링할때는 무슨 distribution으로  모델링을 해야 할지 알 수 없다.</strong>  이때 등장하는게 바로 Machine Learning이다. Machine Learning은 이 “시행”을 모델링하는데 사용한다. 더 나아가 likelihood를 모델링하는데 사용한다!</p><p>Likelihood를 모델링했다면, 이 likelihood는 우리가 추정하고자 하는 parameter인 $\theta$에 대해 미분이 가능해진다(parameter인 $\theta$를 구한다는 것은 likelihood를 추정하는 것이다. $\theta$는 likelihood를 나타내는 파라미터이기 때문이다.). 미분이 가능하다면, $\theta$에 따른 likelihood의 극점을 찾을 수 있다는 것이다. 많은 분들이 아시다시피 다음 조건을 만족할때, 극점이라고 부른다.</p><p>$$<br>\frac{dP(D|\theta)}{d\theta} = 0​<br>$$<br>하지만, 이 식은 극점을 가르처주지만, 그 점이 극대인지, 극소인지 가르쳐주지는 않는다. 이 것을 해결하기 위한 것이 <strong>gradient(기울기)</strong>이다.</p><h4 id="Negative-Log-Likelihood-NLL-amp-Gradient-Descent"><a href="#Negative-Log-Likelihood-NLL-amp-Gradient-Descent" class="headerlink" title="Negative Log Likelihood(NLL) &amp; Gradient Descent"></a>Negative Log Likelihood(NLL) &amp; Gradient Descent</h4><p>흔히, 우리는 gradient descent를 machine learning 알고리즘의 최적화 방법론으로 알고 있다. 그런데 이것이 왜 machine learning 알고리즘을 최적화 할 수 있는 것인지 알아보려고 한다.</p><p>likelihood를 모델링했고, 극대점을 찾아야 한다는 것도 알았다. 그런데, 단순히 미분값이 0인 $\hat{\theta}$를 찾는 것만으로는 극대인지, 극소인지 알 수 없다. 이때 사용하는 것이 gradient인데, 방법은 다음과 같다.</p><ol><li><p>일단 $\theta$를 임의로 초기화한다.</p></li><li><p>현재 $\theta$값에 대해서 likelihood를 미분해본다.</p></li><li><p>미분해서 나온 값<strong>(gradient라고 부른다)</strong>의 부호가 (+)이라는 의미는 $\theta$를 증가시키면 likelihood가 증가한다는 의미이다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303084924874.png" alt="image-20200303084924874"></p></li><li><p>반대로, gradient가 (-)부호라는 것은 $\theta$를 감소시켜야 likelihood가 증가한다는 의미이다.</p><p><img src="../../../../../../Notes/note-images/Principle_of_Machine_Learning/image-20200303084905099-1583192979660.png" alt="image-20200303084905099"></p></li></ol><p>따라서 likelihood의 완전한 maximum 지점을 찾을 수는 없더라도 현재 위치에서 어느 방향으로 가야 likelihood를 증가시키는지 알 수 있다. gradient를 이용해서 likelihood 산을 오른다는 느낌으로, gradient ascent 라는 용어가 있을 수 있겠다. 하지만, 문제가 있다. Likelihood는 확률을 데이터 샘플 수만큼 곱한 것이다. 즉, 매우매우 0에 가까운 값으로, 일반적으로 데이터 샘플 수는 1만개, 10만개가 넘어가는 경우도 많다. 이들을 다 곱하면 컴퓨터에게는 그냥 0이다. 따라서 미분을 하기도 전에 이미 likelihood는 표현조차 불가능하다.</p><p>이것을 해결하기 위해 likelihood에 로그를 씌워서 log likelihood를 만든다. 확률값은 0과 1 사이값이므로, log를 씌우면 상당히 절댓값이 큰 (-)값이 나올 가능성이 높다. 즉, 너무너무 작아서 컴퓨터로 표시되지 않는 문제를 해결할 수 있다. 또한, log를 씌운다고 해서 씌우기 전에 대소관계가 씌운 후에 바뀐다거나 하지 않는다. (log함수는 monotonically 증가하는 함수이다.) 다른 의미로, log likelihood를 최대화하는 $\hat{\theta}$와 likelihood를 최대화하는 $\hat{\theta}$가 같다.</p><p>좋은 예시로, $y=-(x-1)^2+1$를 최대화 하는 x값이나, $\text{log}[y] = \text{log}[-(x-1)^2+1]$를 최대화하는 x값은 똑같이 1이다.</p><p>그런데, likelihood는 확률이므로, 0과 1사이 값이다. 따라서 log를 씌우면 무조건 0보다 작거나 같다. 이 모양이 조금 이상하니까, -1를 곱해서 negative log likelihood를 도입한다. 즉 다음과 같다.</p><p>$$<br>\text{NLL} = -\text{log}[P(D|\theta)]​<br>$$<br>그런데, 정보 이론을 공부해보신 분이라면 어디서 많이 본 모양일 것이다. 정보 이론에서 entroy와 cross entropy라는 개념이 있다. 다음과 같다.</p><p>$$<br>\text{Entropy} = \mathbb{E}_p[-\text{log}~p], \</p><p>\text{Cross Entropy} = \mathbb{E}_p[-\text{log}~q]​<br>$$<br><strong>정보 이론에서 엔트로피란, 불확실성의 높고 낮음을 나타낸다.</strong> (다른 의미로 정보량이 적고 많음을 의미한다) 즉, 확률 분포 p가 uniform distribution과 같이 뭐가 샘플링될지 전혀 알 수 없을수록, 엔트로피는 증가한다. 반대로, 어느 지점에서 확률이 매우 높은(분산이 매우 작은 normal distribution을 떠올리자) 분포는 우리가 어느 정도 뭐가 나올지 알고, 여러개 샘플링해보면 데이터의 다양성이 떨어진다. 따라서 엔트로피가 감소한다.</p><p><strong>Cross entropy란, 어느 분포 p에 대해서 다른 분포 q의 기댓값이다.</strong> 무슨 의미냐면, p분포와 q분포가 많이 다르게 생기면 생길수록 cross entropy가 증가한다. 반면, p분포와 q분포가 비슷하게 생길수록 cross entropy가 감소한다. 이는 두 분포간의 거리를 계산한다는 KL-divergence의 개념과도 거의 유사하며, 사실상 다 이어져 있는 개념이다. KL-divergence는 실제로 entropy와 cross entropy의 합이다.</p><p>왜 이 개념을 말했냐면, negative log likelihood에서, 모분포를 $\mathbb{P}$라고 했을 때, 우리 손에 들어온 모든 데이터 샘플이 나왔을 확률을 모두 같다고 가정해보자. 왜 모두 같다고 해도 되냐면, 데이터 샘플에는 중복되어 샘플링 된 샘플이 많을 것이다. 동전 던지기를 10번 해서 앞면이 7번 나왔다면, 그 10번 시행에 모두 같은 비중치를 뒀지만, 다 더해보면 앞면의 비중치는 0.7, 뒷면의 비중치는 0.3으로, 많이 샘플링된 얘들은 높은 비중치를 가지게 된다. 따라서, 모든 데이터 샘플들에 비중치를 같다고 둬도, 중복된 샘플들 덕분에 실제로 비중치는 데이터 확률 분포를 반영하게 되는 것이다.</p><p>그렇게 되면 각 비중치 $\alpha$를 통해 다음과 같이 표현될 수 있다.</p><p>$$<br>\text{NLL} = \Sigma_i-\alpha\text{log}P(d_i|\theta) = [\text{예시}]: \Sigma_i[-0.1<em>\text{log}\theta] = -0.7<em>\text{log}\theta - 0.3</em>\text{log}(1-\theta)<br>$$<br>각 비중치 위 모양은 cross entropy와 정확히 일치한다. 참고로 $\alpha$를 곱해준다고 해서 likelihood를 최대화시키는 $\hat{\theta}$값은 변하지 않는다. $-(x-1)^2$를 최대화 시키는 x나, $-0.1</em>(x-1)^2$을 최대화 시키는 x는 모두 1이다. 같은 이유이다.</p><p>따라서, negative log likelihood는 cross entropy라고도 부른다.</p><p>종합해보면 다음 문장들은 모두 같은 의미이다.</p><ul><li>Likelihood를 최대화 시키는 파라미터를 구한다.</li><li>Log likelihood를 최대화 시키는 파라미터를 구한다.</li><li>Negative log likelihood를 최소화 시키는 파라미터를 구한다.</li><li>Cross entropy를 최소화 시키는 파라미터를 구한다.</li></ul><p><strong>그리고, likelihood를 최대화 시키기 위해서 gradient ascent를 해야 했지만, negative로 만듦으로써, 이번엔 negative likelihood를 하강해야 하므로, gradient descent를 해야 하는 것이다.</strong></p><p>하지만, 보다시피, gradient descent 방법으로는 theta를 증가시킬지, 감소시킬지는 가르쳐주지만, 어느정도 감소시켜야 할지, 증가시켜야 할지는 알려주지 않는다. 따라서 theta를 조금씩 증감하면서 gradient가 0이 되는 극점을 찾는 것이 gradient descent optimization이고, 극점을 찾는 과정을 training/learning(학습)이라고 부른다.</p><h4 id="Machine-Learning-in-Frequentist-Statistics-1"><a href="#Machine-Learning-in-Frequentist-Statistics-1" class="headerlink" title="Machine Learning in Frequentist Statistics"></a>Machine Learning in Frequentist Statistics</h4><p>앞서 말했듯이, 데이터 샘플을 뽑는 1회 시행은 우리가 알고 있는 normal distribution이나, bernoulli distribution같은 것이 아닐 가능성이 있다. 따라서 우리는 유연하게 “시행”을 모델링해야 할 필요가 있다.</p><p>Machine learning이라고 하면, weights $w$로 parameterize되며, 데이터 샘플 하나 $x$를 입력으로 받으면, 그 데이터 샘플이 어느 부류인지에 대한 확률 $P(x|w)$을 계산한다. 즉, machine learning이라는 이름의 방법론 속에 숨어있는 확률 분포로부터 입력으로 넣은 데이터 샘플이 뽑힐 확률을 계산하는 것이다.</p><p>이들을 모든 데이터 샘플들에 반복해서 모두 곱하면,</p><p>$$<br>P(x_1|w)P(x_2|w)\cdots P(x_n|w) = P(X|w)​<br>$$<br>즉,  likelihood를 모델링한 것이다. machine learning은 데이터 샘플 1개가 샘플링되는 모 확률 분포라고 가정하고, 그 모 확률분포를 모델링한 것에 지나지 않는다.</p><h4 id="Generalization-Issue"><a href="#Generalization-Issue" class="headerlink" title="Generalization Issue"></a>Generalization Issue</h4><p>머신 러닝으로 데이터 샘플들의 확률 분포를 모델링했다(말 그대로 모델링한 것이지, 진짜 모 확률 분포가 아니다. 추정일 뿐이다.). 그런데, 우리는 수집된 데이터 셋만을 이용해서 모 확률분포를 추정했는데, 우리가 모은 샘플들이 모 분포에서 나올 수 있는 모든 샘플들을 포함할까? 절대 아니다. 그럼, 샘플링되지 않은 놈들이 있을 수 있는데, 이들에 대해서도 잘 작동하는지는 어떻게 보장하나?</p><p>지금 우리는 frequentist statistics에서의 machine learning을 말하고 있다. 답은 <strong>frequentist statistics</strong>에 있다. Frequentist statistics에서는 가능한 많은 데이터 샘플을 뽑고 분포를 추정하게 된다. 여기서, <strong>“가능한 많은 데이터 샘플”</strong>이 핵심이다. 가능한 많은 데이터 샘플들을 뽑게 되면, 어쨌든 샘플링될 확률이 높은 데이터샘플은 많이 뽑힐 것이고, 적은 확률로 샘플링된 녀석들도 적은 개수나마 샘플링될 것이다. 즉, 데이터셋을 바탕으로 확률 분포를 모델링할때, 실제로 높은 확률을 가지는 샘플은 수가 많아서 모델링된 분포에도 높은 확률을 가질 것이고, 실제로 낮은 확률을 가지는 샘플은 수가 적어서 모델링된 분포에도 낮은 확률을 가질 것이다. 즉, 데이터 샘플수가 충분히 많다면, 모 확률분포와 매우매우 유사해진다. 그래서 머신러닝에서 <strong>“데이터셋을 많이 모아라~”</strong> 하는 것이다.</p><p>또한, 데이터 샘플들의 수가 많아질수록 매우 다양한 샘플들이 샘플링되어 있을 것이며, 이들 만으로도 충분히 모 확률분포에서 나올 수 있는 샘플들을 커버할 수 있다는 것이다. 따라서, 데이터 샘플 수가 적당히 많다면, 이들을 이용하면 모 확률분포와 매우 유사하게 모델링이 가능하고, 그럼, 미처 샘플링되지 못한 샘플들에 대해서도 잘 작동할 것이라는 이론이 있다.</p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>정리해보면, frequentist statistics에서의 machine learning이란, 일단은 <strong>함수</strong>이다. 그런데, 확률값을 반환하는 <strong>확률 함수</strong>이다. 좀 더 정확히 말해보면 데이터 샘플들의 진짜 모 확률분포를 모델링한, <strong>확률분포</strong>이다.</p><p>Machine learning은 parameter(weight)를 이용해서 이 확률 분포를 모델링하고, likelihood를 모델링한다. Likelihood를 최대화하는 parameter를 계산(정확히는 “추정”)한다. 이때, gradient descent를 이용한다. parameter를 계산하는 과정을 학습이라고 한다. 학습이 끝나면, machine learning은 모분포를 잘 추정한다고 가정하며, 일반화도 잘 이루어 졌을 것이라고 가정한다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;My-Interpretation-of-Machine-Learning&quot;&gt;&lt;a href=&quot;#My-Interpretation-of-Machine-Learning&quot; class=&quot;headerlink&quot; title=&quot;My Interpretation 
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Machine Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Machine-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="MachineLearning" scheme="https://jaeyoung-blog.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>08. Bayesian Modeling</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/</id>
    <published>2020-03-01T13:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.082Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayesian-Modeling"><a href="#Bayesian-Modeling" class="headerlink" title="Bayesian Modeling"></a>Bayesian Modeling</h1><h2 id="Statistical-Modeling"><a href="#Statistical-Modeling" class="headerlink" title="Statistical Modeling"></a>Statistical Modeling</h2><p>Bayesian modeling은 statistical modeling의 일종이다. Statistical modeling이란, 데이터가 생성/샘플링되는 프로세스를 모델링하는 것을 의미한다.</p><p>Bayesian modeling은 이러한 모델링을 할때, 베이지안 방법론을 적용한 것을 말한다. 전체적인 모델링 프로세스는 bayesian modeling이나 frequentist modeling이나 같다.</p><ol><li>문제 이해</li><li>데이터 수집</li><li>데이터 관찰</li><li>모델 구성</li><li>모델의 구현 및 fit</li><li>샘플공간 분포 파라미터 추정</li><li>테스트 및 예측성능 검사</li><li>5~7번 반복</li><li>모델의 이용.</li></ol><p>이때, frequentist modeling과 bayesian modeling에서의 차이는 모델의 구현과 fit, 파라미터 추정에 있다.</p><h2 id="Model-Specification-모델-구성"><a href="#Model-Specification-모델-구성" class="headerlink" title="Model Specification - 모델 구성"></a>Model Specification - 모델 구성</h2><p>모델의 구성은 계층적으로 적어 내려가면서 파악하는게 어느정도 쉽다. 일단, likelihood를 적고 likelihood에 영향을 미치는 random variable 또는 parameter를 찾는다.</p><p>어느 학교의 학생들의 키(height)의 분포를 예로 들자. 키의 분포는 normal distribution을 따른다고 가정하고 likelihood를 만든다.<br>$$<br>f(y|\theta) = \mathbb{N}(\mu, \sigma^2)<br>$$<br>그리고 $\mu$, $\sigma^2$의 분포가 필요한데, 이들의 prior를 정한다.<br>$$<br>\mu \approx \mathbb{N}(\mu_0, \sigma_0^2)<br>$$<br>$$<br>\sigma^2 \approx \mathbb{IG}(\nu_0, \beta_0)<br>$$</p><p>여기서 $\mathbb{IG}$는 inverse-gamma distribution을 뜻한다. 그리고 각 prior는 독립이라고 가정하면, $p(\mu,\sigma^2) = p(\mu)p(\sigma^2)$일 것이다. 모델로 그려보면 다음과 같다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303093449313.png" alt="image-20200303093449313"></p><p>이렇게, 우선 데이터가 어떻게 생성되었을지에 대해 그 생성 과정을 모델링하는데, likelihood부터 적고, 아래 파라미터까지 노드를 뻗어 나간다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayesian-Modeling&quot;&gt;&lt;a href=&quot;#Bayesian-Modeling&quot; class=&quot;headerlink&quot; title=&quot;Bayesian Modeling&quot;&gt;&lt;/a&gt;Bayesian Modeling&lt;/h1&gt;&lt;h2 id=&quot;Stati
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>09. Monte Carlo Estimation</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/</id>
    <published>2020-03-01T13:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.082Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Monte-Carlo-Estimation"><a href="#Monte-Carlo-Estimation" class="headerlink" title="Monte Carlo Estimation"></a>Monte Carlo Estimation</h1><p>쉽게 말하면, 몬테 카를로 추정법이란, 어떤 특정한 파라미터를 얻기 위해서, 파라미터의 distribution으로부터 많은 샘플링 시뮬레이션을 한 후, 그 샘플들의 평균을 계산한 것을 파라미터의 기댓값으로 추정하는 것이다.</p><p>예를 들어, 어떤 파라미터 $\theta$는 어떤 분포 $p(\theta)$를 따른다. 우리는 파라미터 $\theta$의 기댓값 $E[\theta]$를 계산하고 싶다. 이를 계산하기 위해서는 원래 $E[\theta] = \int p(\theta) \cdot \theta ~~ d\theta$를 계산해야 한다. 하지만, 이 계산은 불가능하거나 매우 힘들 수 있다.</p><p>$E[\theta]$를 계산하는 대신 추정하는 방법으로 몬테 카를로 추정법을 이용한다. 우선, 컴퓨터로 $p(\theta)$로부터 $\theta$를 많이 샘플링한다. 그리고 그들의 평균 $\bar{\theta} = \frac{1}{m}\sum_i^m \theta_i$를 계산하고 $\bar{\theta}$를 $E[\theta]$로 추정하는 것이다.</p><p>분포 $p(\theta)$로부터 높은 확률의 $\theta$가 많이 샘플링되고 낮은 확률의 $\theta$는 적게 샘플링 되었을 것이다. 따라서 이 추정법은 유효할 수 있다. 다른 방식으로 해석하면, central limit theorem에 의해 샘플평균 $\bar{\theta}$는 실제 평균인 $E[\theta]$를 평균으로 하고 $\frac{1}{m}Var[\theta]$를 분산으로 하는 normal distribution을 따른다. 특히, 샘플수가 많아질수록, 계산한 샘플평균은 실제 평균값과 매우 유사할 확률이 높다.</p><p>$h(\theta)$의 기댓값 $E[h(\theta)]$를 추정하고 싶다. 그러면, $\theta$를 많이 샘플링해서 각 샘플로 $h(\theta)$를 계산하고 평균을 내면 $E[h(\theta)]$의 추정값이 된다.</p><h3 id="Monte-Carlo-Error"><a href="#Monte-Carlo-Error" class="headerlink" title="Monte Carlo Error"></a>Monte Carlo Error</h3><p>CLT(Central Limit Theorem)에 의해 파라미터 $\theta$에 대해 모은 샘플들은 $\mathbb{N}(E[\theta],\frac{Var[\theta]}{m})$를 따른다. $Var[\theta]$는 $\theta$의 분산으로, 다음으로 대체한다.<br>$$<br>Var[\theta] = \frac{1}{m}\sum_i (\bar{\theta} - \theta_i)^2<br>$$<br>그리고, $\frac{Var[\theta]}{m}$값을 <strong>monte carlo error</strong>라고 한다. Monte carlo estimation 값($E[\theta]$의 추정값인 $\bar{\theta}$)이 진짜 $E[\theta]$로부터 어느정도로 오차가 있을지에 대한 term이라고 볼 수 있다.</p><h3 id="Monte-Carlo-Marginalization"><a href="#Monte-Carlo-Marginalization" class="headerlink" title="Monte Carlo Marginalization"></a>Monte Carlo Marginalization</h3><p>Paremter가 hierarchical하게 연결된 경우도 있다. 예를들어, 데이터 $Y$는 베르누이 분포 $\text{Bern}(\phi)$를 따르는데, 이 $\phi$가 또 베타분포 $\text{Beta}(2, 2)$를 따른다고 하자. 데이터 $Y$의 기댓값 $E[Y]$를 몬테 카를로 추정법으로 추정하기 위해서는 다음의 과정이 필요하다.</p><ol><li>$\text{Beta}(2, 2)$로부터 $\phi$를 샘플링한다.</li><li>샘플링한 $\phi$를 가지고 $Y|\phi$를 샘플링한다.</li><li>이제, ($Y,\phi$)한 쌍이 생성되었다.</li><li>반복한다.</li></ol><p>이 과정의 특징이, 샘플 ($Y,\phi$)가 자연스럽게 $P(Y,\phi)$의 joint distribution을 반영한다는 것이다.</p><p>그런데, 위에서 샘플링한 $\phi$를 그냥 무시하고 $Y$만 취하면 그게 $\phi$에 대해 marginalization한 것과 같다. 즉, prior predictive distribution을 취한 것이다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Monte-Carlo-Estimation&quot;&gt;&lt;a href=&quot;#Monte-Carlo-Estimation&quot; class=&quot;headerlink&quot; title=&quot;Monte Carlo Estimation&quot;&gt;&lt;/a&gt;Monte Carlo Estimati
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>13. Hierarchical Models</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/</id>
    <published>2020-03-01T13:08:00.000Z</published>
    <updated>2020-03-01T13:45:11.257Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hierarchical-Models"><a href="#Hierarchical-Models" class="headerlink" title="Hierarchical Models"></a>Hierarchical Models</h1><p>데이터 생성 프로세스를 계층적으로 모델링한 것을 의미한다. 즉, likelihood의 파라미터는 또 다른 파라미터를 갖는 분포를 가지는 형태. 예를들어 likelihood는 poisson 분포를 따른다고 모델링하고, 그 파라미터 $\lambda$는 또 다른 파라미터 $\alpha, \beta$를 가지는 gamma 분포를 따른다고 모델링했다고 하자. 이 경우가 계층적 모델링에 속한다.<br>$$<br>y_i|\lambda_{j} \sim \text{Pois}(\lambda_{j}) \<br>\lambda_j|\alpha,\beta \sim \text{Gamma}(\alpha, \beta) \<br>\alpha \sim p(\alpha), \beta \sim p(\beta)<br>$$<br>이때, 각 $\lambda$는 여러개가 있고, 그중 하나에서 $y$가 생성되지만, $\lambda$는 모두 같은 분포에서 나온 녀석들이라고 모델링 한 것이다. $p(\alpha),p(\beta)$는 각각 $\alpha,\beta$의 prior이다.</p><p>이 경우의 장점은, 데이터가 모두 독립이지 않고 같은 성질을 갖는 놈들(같은 $\lambda$에서 나온 놈들에 해당)은 비슷하고 다른 성질은 갖는 놈들은 조금 다르다는, 약간의 correlation이 있는 데이터를 모델링할 수 있다.</p><p>$\alpha,\beta$는 고정적으로 줘도 될 것인데, 왜 궂이 prior를 할당해서 샘플링하는가? 이건 $$\alpha,\beta$$에 대한 uncertainty(불확실성) 때문이다.</p><p>$$\alpha,\beta$$는 독립적으로 샘플링된다. 그러나, 샘플링된 $\lambda$들 끼리는 독립이 아니다. 대신, $\alpha,\beta$가 주어진다면, 각 $\lambda$끼리는 해당 특정한 $\alpha, \beta$를 파라미터로 하는 분포에서 독립적으로 샘플링됬을 것이므로 조건부 독립이다. $y$끼리도 독립이 아니지만, $\lambda$가 주어진다면 $y$들 끼리 독립(조건부 독립)이다. $\lambda$가 주어졌다는 의미는 어느 한 그룹으로 좁혔고, 그 그룹 내에서 샘플들끼리는 독립이기 때문이다(그렇게 모델링 했으니까).</p><p>이렇게 함으로써, 다른 그룹은 다른 모델로 모델링하기 보다는 계층적인 하나의 모델로 모델링할 수 있다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hierarchical-Models&quot;&gt;&lt;a href=&quot;#Hierarchical-Models&quot; class=&quot;headerlink&quot; title=&quot;Hierarchical Models&quot;&gt;&lt;/a&gt;Hierarchical Models&lt;/h1&gt;&lt;p&gt;데이
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>11. Linear Regression</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/11_Linear_Regression/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/11_Linear_Regression/</id>
    <published>2020-03-01T13:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.084Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>선형 회귀라고도 불리며, 예측해야할 dependent variable이 continuous할때, 유용하다.</p><p>선형 회귀는 다음과 같다.<br>$$<br>y_i \sim \mathbb{N}(\mu_i, \sigma^2) \<br>\mu_i = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k<br>$$<br>또는 다음과 같이 표현할 수 있다.<br>$$<br>y_i = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + \epsilon \<br>\epsilon \sim \mathbb{N}(0, \sigma^2)<br>$$<br>즉, 예측값 $y_i$는 일직선 값에 오차값(residual) $\epsilon$을 더한 값이며, 이 residual값은 normal distribution을 따른다.</p><p>베이지안 통계에서는 $\beta$들은 분포를 갖는 random variable들이다. 따라서, 선형 회귀를 학습시킬때, 각 $\beta$에 대해 prior를 설정하고 posterior를 계산하게 된다.</p><p>각 $\beta$값에는 normal prior를 주는게 보통이지만, 다른 prior도 상관없다.</p><p>단, 어떤 variable $x_i$가 $y_i$에 영향을 주는 놈인지 알고 싶다면, $\beta_i$의 prior로 <strong>Laplace prior</strong>를 설정하기도 한다. Laplace distribution은 double exponential 이라고도 불리며, 0점에서 뾰족한 모양이고 $y$축 대칭이다.</p><p>만약, 어떤 $i$번째 $\beta_i$의 posterior 분포가 그냥 Laplace처럼 0점에 뾰족한 모양에 가깝다면, 그 $\beta_i$에 대응되는 $x_i$는 영향력이 거의 없다고 할 수 있다. 이런 방법을 <strong>Lasso</strong> 라고 부른다.</p><p>JAGS 문법으로 표현하면 다음과 같다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rjags</span></span><br><span class="line"></span><br><span class="line">mod.string &lt;- <span class="string">"model &#123;</span></span><br><span class="line"><span class="string">    # likelihood</span></span><br><span class="line"><span class="string">    for (i in 1:length(y)) &#123;</span></span><br><span class="line"><span class="string">        y[i] ~ dnorm(mu[i], prec)</span></span><br><span class="line"><span class="string">        mu[i] = b0 + b[1]*x1[i] + b[2]*x2[i]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    # prior</span></span><br><span class="line"><span class="string">    # prec의 사후샘플들의 effective size가 5, variability가 2라고 기대하는 경우의</span></span><br><span class="line"><span class="string">    # inverse-gamma는 다음과 같다.</span></span><br><span class="line"><span class="string">    prec ~ dgamma(5/2, 5*2/2)</span></span><br><span class="line"><span class="string">    b0 ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    for (i in 1:2) &#123;</span></span><br><span class="line"><span class="string">        b[i] ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;"</span></span><br></pre></td></tr></table></figure><h2 id="Poison-Regression"><a href="#Poison-Regression" class="headerlink" title="Poison Regression"></a>Poison Regression</h2><p>선형 회귀의 일종으로, response variable인 $y$가 count 값인 경우에 사용되는 경우가 있다. poison 분포는 0보다 크거나 같은 값을 도메인으로 가지므로, $y$역시 0보다 같거나 큰 값이어야 한다. 반대로, $y$의 범위가 0이상이라면, Poison regression을 생각해 볼 수 있다.</p><p>Poison regression은 likelihood가 poison distribution으로 모델링된 형태이다. 그런데, 이 경우, $y$가 0 이상 값이므로, 선형 회귀처럼 $y_i = \beta_0 + \beta_1 x_{1,i}$로 할 수 없다. 대신, $y$를 적절히 변형해서 $[-\infin, \infin]$범위로 만들어 준다면, 선형 함수로 적용이 가능할 것이다. 이때 사용하는 것이 $\text{log}$함수이다. 즉, 다음과 같다.<br>$$<br>\text{log}~y_i = \beta_0 + \beta_1x_{1,i} + \beta_2 x_{2,i} + \cdots \<br>y_i = \text{exp}(\beta_0 + \beta_1x_{1,i} + \beta_2 x_{2,i} + \cdots)<br>$$<br>위의 방법으로 모델링한다. Poison distribution의 파라미터 $\lambda$는 곧 분포의 기댓값이다. 즉, $y_i = \lambda_i$로 생각하면 된다.  JAGS 문법으로 표현하면 다음과 같다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rjags</span></span><br><span class="line"></span><br><span class="line">mod.string &lt;- <span class="string">"model &#123;</span></span><br><span class="line"><span class="string">    # likelihood</span></span><br><span class="line"><span class="string">    for (i in 1:length(y)) &#123;</span></span><br><span class="line"><span class="string">        y[i] ~ dpois(lambda[i])</span></span><br><span class="line"><span class="string">        log(lambda[i]) = b0 + b[1]*x1[i] + b[2]*x2[i]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    # prior</span></span><br><span class="line"><span class="string">    b0 ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    for (i in 1:2) &#123;</span></span><br><span class="line"><span class="string">        b[i] ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Linear-Regression&quot;&gt;&lt;a href=&quot;#Linear-Regression&quot; class=&quot;headerlink&quot; title=&quot;Linear Regression&quot;&gt;&lt;/a&gt;Linear Regression&lt;/h1&gt;&lt;p&gt;선형 회귀라고도 불
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>10. Markov Chain Monte Carlo</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/</id>
    <published>2020-03-01T13:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.083Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Markov-Chain-Monte-Carlo"><a href="#Markov-Chain-Monte-Carlo" class="headerlink" title="Markov Chain Monte Carlo"></a>Markov Chain Monte Carlo</h1><p>MCMC라고도 불린다. 파라미터 $\theta$의 분포 $p(\theta)$를 추정하고자 한다. 그러기 위해, bayesian inference를 하려고 하는데, 그러려면, 데이터를 수집한 후 posterior $p(\theta|Y)$를 계산해야 한다. 그러나, 이 posterior를 계산하기에 상당히 어려울 수 있기 때문에(특히, normalization constant) 대신, posterior를 추정하기로 한다. 이 posterior를 추정할때 쓰일 수 있는 알고리즘 중 하나가 MCMC이다.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>우리가 추정하고자 하는 파라미터 $\theta$의 분포인 $\mathbb{p}(\theta)$를 추정하고자 한다. 그래서 $\theta$를 파라미터로 하는 어떤 데이터 분포 $y|\theta$로부터 $y_1,…y_k$를 샘플링했다. 이를 바탕으로 $\theta$의 prior $p(\theta)$를 설정하고, 데이터로 posterior $p(\theta|y_1,…y_k)$를 계산해서 $\mathbb{p}(\theta)$에 대한 베이지안 추론을 하고자 한다. 그런데, posterior인 $p(\theta|y_1,…,y_k)$를 계산하는게 매우 어렵거나 불가능한 경우가 있다. 이때, posterior 분포 $p(\theta|y_1,…,y_k)$를 추정하기 위한 방법으로 MCMC가 사용될 수 있다.</p><h3 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h3><p>MCMC는 posterior의 분포 $p(\theta|y_1,…y_k)$를 추정하기 위해 마치 이 posterior로부터 샘플링됬을 법한 샘플 $\theta^<em>_1,…,\theta^</em>_m$을 생성해 준다. 이들은 posterior로부터 샘플링 되었을 거라고 가정하고 posterior를 monte carlo estimation으로 추정한다.</p><p>MCMC의 알고리즘으로 여러 개가 있다고 하는데, 대표적으로 Metropolis-Hastings 알고리즘이 있다.</p><h2 id="Matropolis-Hastings-Algorithms"><a href="#Matropolis-Hastings-Algorithms" class="headerlink" title="Matropolis-Hastings Algorithms"></a>Matropolis-Hastings Algorithms</h2><p>알고리즘은 다음과 같다.</p><ol><li><p>시작하기에 앞서 posterior $p(\theta|y_1,…,y_k)$를 정확히 계산할 수는 없더라도, 이 posterior에 비례하는 어떤 함수는 알고 있어야 한다. 즉, 다음을 만족하는 $g(\theta)$는 알고 있어야 한다.<br>$$<br>p(\theta|y_1,…,y_k) \propto g(\theta)<br>$$</p></li><li><p>$\theta$와 도메인이 같거나 최대한 비슷한 분포 아무거나 고른다. 이 분포는 마르코프 체인을 만족하면 좋다. 즉, $q(\theta^*|\theta_{i-1})$.</p></li><li><p>적당히 큰 수 $m$번을 반복하는데, $m$개의 $\theta^*$를 1개씩 샘플링할 것이다.</p><ol><li><p>$\theta^<em>$를 $q(\theta^</em>|\theta_{i-1})$로부터 1개를 샘플링한다.</p></li><li><p>다음을 계산한다.<br>$$<br>\alpha = \frac{g(\theta^<em>)q(\theta^</em>|\theta_{i-1})}{g(\theta_{i-1})q(\theta_{i-1}|\theta^*)}<br>$$</p></li><li><p>$\alpha \geq 1$이면, $\theta_i \leftarrow \theta<em>$로 accept한다. $0 \leq \alpha &lt; 1$이면, $\alpha$의 확률로 $\theta_i \leftarrow \theta^</em>$로 accept하고, reject되면 $\theta_i \leftarrow \theta_{i-1}$한다.</p></li></ol></li></ol><p>분자에 $g(\theta^*)$가 있고, 분모에 $g(\theta_{i-1})$가 있어서, 이전에 뽑은 $\theta$보다 현재 뽑은 $\theta$가 더 $p(\theta|y_1,…,y_k)$에서 확률이 높다면, $\alpha \geq 1$이 되어서 accept된다. $g$가 $p$에 비례하기 때문에 그렇다.</p><p>이렇게 뽑은 $\theta^*$는 초반 샘플링된 놈들을 제외하면, posterior $p(\theta|y_1,…,y_k)$에서 샘플링된 것처럼 역할을 할 수 있다. 분포로부터 샘플링된 놈이 있으므로 posterior에 대해 monte carlo estimation이 가능해진다.</p><h2 id="Random-Walk-Algorithm"><a href="#Random-Walk-Algorithm" class="headerlink" title="Random Walk Algorithm"></a>Random Walk Algorithm</h2><p>Matropolis-hastings 알고리즘에서, proposal distribution $q(\theta^<em>|\theta_{i-1})$를 $\theta_{i-1}$을 평균으로 하는 normal distribution으로 놓은 것을 말한다. Normal distribution은 대칭 분포이기 때문에, $\alpha=\frac{g(\theta^</em>)}{g(\theta_{i-1})}$이 된다.</p><h2 id="Gibbs-Sampling"><a href="#Gibbs-Sampling" class="headerlink" title="Gibbs Sampling"></a>Gibbs Sampling</h2><p>파라미터가 여러개라면, gibbs sampling이 Metropolis-hastings 알고리즘 보다 편할 수 있다. Metropolis-hastings 알고리즘에서는 파라미터 $\theta_1, …, \theta_k$에 대해 proposal distribution을 각 파라미터마다 정의하고, accept, reject과정을 거칠 테지만, gibbs sampling과정에서는 이 과정을 없앴다. 대신 다음의 과정이 있다.</p><p>이때, parameter $\theta_1,…,\theta_k$를 모두 업데이트 1번씩 하는 과정을 1번의 iteration이라고 하자.</p><ol><li><p>일단, $p(\theta_1,…,\theta_k|y) \propto g(\theta_1,…,\theta_k)$를 만족하는 $g(\theta_1, …, \theta_k)$를 알고 있어야 한다. $p(\theta_1,…,\theta_k|y) \propto p(y|\theta_1,…,\theta_k)p(\theta_1,…,\theta_k)$를 활용.</p></li><li><p>하나의 parameter에 대한 full conditional distribution의 proportion을 계산해야 하는데, 다음과 같이 posterior 분포에 비례하므로(Bayes’ rule에 의해), $g$에 비례한다.<br>$$<br>p(\theta_i|\theta_1,…,\theta_{i-1},\theta_{i+1},…,\theta_k,y) \propto p(\theta_1,…,\theta_k|y) \propto g(\theta_1,…,\theta_k)<br>$$<br>그리고, 나머지 파라미터는 모두 주어진 것으로 가정한다. 나머지 파라미터는 초기값이거나 가장 최근에 업데이트한 값으로 들어간다.</p></li><li><p>그렇게 되면, $g$에서 $\theta_i$에 의해 parameterize되지 않는 항은 모두 constant로 취급할 수 있으며, proportion에서 제외할 수 있다. 그럼 $g$가 간소화된다.</p></li><li><p>이렇게 되면, $\theta_i$에 대한 full conditional distribution이 우리가 아는 분포, 즉 샘플링이 가능한 분포가 되는 경우가 있다. 이럴 경우, 그냥 그 분포에서 샘플링하면 되기 때문에 accept, reject과정이 필요가 없다. 하나를 샘플링하고 $\theta_i$를 업데이트한다.</p></li><li><p>파라미터 $\theta_{i+1}$에 대해 같은 과정을 반복하는데, $\theta_{1,…i}$은 이전 iteration의 값이 아니라, 현재 iteration값을 이용한다.</p></li><li><p>만약, 4번 과정에서 샘플링이 가능한 표준적인 분포가 아니라면, 그 안에서, $\theta_i$ 하나에 대해서 matropolis-hastings 알고리즘의 방식을 사용해서, 하나의 샘플을 accept 혹은 reject로 업데이트한다.</p></li><li><p>업데이트 이전 값은 어디다가 저장해두자. 그 값들이 샘플들이다.</p></li></ol><h2 id="Assessing-Convergence-of-MCMC"><a href="#Assessing-Convergence-of-MCMC" class="headerlink" title="Assessing Convergence of MCMC"></a>Assessing Convergence of MCMC</h2><p>MCMC알고리즘에서 샘플링한 샘플들 $\theta^<em>_1, …, \theta^</em>_k$의 평균값 $\bar{\theta^<em>}$이 $\theta$의 posterior 분포 $p(\theta|Y)$를 잘 추정하려면, 마르코프 체인이 충분히 수렴해야 하고, 수렴한 체인으로부터 $\theta^</em>$가 충분히 샘플링되어야 한다. 하지만, 마르코프 체인이 언제 수렴할지를 모르기 때문에 몇 개의 샘플까지가 수렴이 안된 상태의 샘플인지, 몇 개가 유용한 샘플인지 알 수가 없다.</p><h3 id="Stationary-Distribution"><a href="#Stationary-Distribution" class="headerlink" title="Stationary Distribution"></a>Stationary Distribution</h3><p>마르코프 체인이 추정하고자 하는 target distribution(parameter의 posterior가 된다)을 최대한 추정한 distribution을 의미하며, 마르코프 체인이 충분히 수렴한 상태에서의 distribution을 의미한다. 당연히 알 수 없으며, 여기서 마르코프체인으로 샘플링만 가능하다.</p><h3 id="Monte-Carlo-Effective-Sample-Size"><a href="#Monte-Carlo-Effective-Sample-Size" class="headerlink" title="Monte Carlo Effective Sample Size"></a>Monte Carlo Effective Sample Size</h3><p>진짜 Stationary distribution으로부터 독립적으로 샘플링한 샘플을 $\theta_{eff}$이라고 하자. 즉, 이들은 실제로 posterior로부터 샘플링한 샘플과 매우 유사할 것이다.</p><p>우리가 마르코프 체인으로부터 샘플링한 샘플의 개수를 $n$이라고 하자. 하지만, 수렴이 제대로 되지 않은 상태에서 뽑은 것은 독립적인 샘플일 수가 없고, 마르코프 체인이기에, 완전히 독립적이기는 어렵다. 따라서, 유용한 샘플들은 일부일 것이다.</p><p>이 $n$개의 샘플이 가지고 있는 정보가 과연 몇 개의 $\theta_{eff}$들이 가지는 정보와 같은지를 나타내는게 monte carlo effective sample size이다. 즉, $n=1000000$개의 샘플을 뽑았는데, effective sample size $n_{eff}=500$이라고 하면, 이 100만개의 샘플들은 실제로 posterior에서 500개를 샘플링한 것과 같은 정보를 가진다.</p><p>이는, 마르코프 체인이 posterior를 완전히 추정하지 못하기 때문이다. 또한, $n_{eff}$이 너무 작다면, 수렴 속도가 느린 것일수도 있다.</p><h3 id="Auto-correlation"><a href="#Auto-correlation" class="headerlink" title="Auto-correlation"></a>Auto-correlation</h3><p>마르코프 체인에서 샘플링한 한 샘플이 과거 샘플들과 얼마나 많은 dependency가 있는가를 나타낸다. [-1,1] 범위의 값을 가지며, 0에 가까울수록 그 샘플은 과거 샘플들과 관계없는, 독립적인 샘플들이다. monte carlo sample size를 증가시키려면, 이 독립적인 샘플들이 필요하다.</p><p>마르코프 체인으로부터 샘플링을 하면, 초기 몇 개의 샘플까지는 수렴이 되지 않아서 correlation이 높다. 초기 correlation이 0에 가까운 값이 되는 지점까지의 샘플은 버리는 것도 방법(<strong>burn-in 이라고 부름</strong>).</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/1567820594976.png" alt="1567820594976"></p><p>Auto-correlation이 0과 가까운 값이 적으면 effective sample size가 감소한다.</p><h3 id="Gelman-Rubin-Diagnostic"><a href="#Gelman-Rubin-Diagnostic" class="headerlink" title="Gelman-Rubin Diagnostic"></a>Gelman-Rubin Diagnostic</h3><p>마르코프 체인으로부터 샘플링한 샘플들을 주면, 실수값을 반환하는데, 1에 가가우면 수렴이 된 것이고, 1보다 많이 크면, 수렴이 아직 안된 것이다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Markov-Chain-Monte-Carlo&quot;&gt;&lt;a href=&quot;#Markov-Chain-Monte-Carlo&quot; class=&quot;headerlink&quot; title=&quot;Markov Chain Monte Carlo&quot;&gt;&lt;/a&gt;Markov Chain M
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>12. Prior Sensitivity Analysis</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/</id>
    <published>2020-03-01T13:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.085Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prior-Sensitivity-Analysis"><a href="#Prior-Sensitivity-Analysis" class="headerlink" title="Prior Sensitivity Analysis"></a>Prior Sensitivity Analysis</h1><p>Prior sensitivity analysis란, 하나의 파라미터에 대해 여러가지 prior 분포를 적용해보고 만든 여러 모델의 성능을 분석하는 과정을 말한다.</p><p>여러가지 prior를 적용해서 여러 모델을 만들었다고 하자. 그럼 여러 모델들이 내놓는 결과를 바탕으로 다음과 같이 해석할 수 있다. (Prior sensitivity analysis의 결과로 다음 두 가지의 경우가 나온다)</p><ul><li><p>결과가 prior-sensitive하다.</p><p>어떤 prior를 선택하느냐에 따라 추정 성능 및 결과가 크게 달라지는 경우를 말한다. 즉, 데이터보다는 prior가 결과에 영향을 많이 미치는 경우이다. 이 경우, 내가 왜 이 prior를 선택해야 하고 이 모델을 선택해야 하는지 팀에게 설명할 필요가 있다.</p></li><li><p>결과가 prior-insensitive하다.</p><p>이 경우, prior보다는 데이터가 결과에 지대한 영향을 미치는 경우로, prior의 선택에 큰 힘을 쏟을 필요가 없음을 보일 수 있다.</p></li></ul><p>Prior sensitivity analysis는 내가 선택한 prior가 적절하다는 가정을 증명하기 위해서도 사용한다(즉, 가설검정에 사용할 수 있음). 내가 원하는 prior를 선택해서 모델을 구성하고, 내가 원하지 않는 prior를 선택해서 모델을 구성하게 된다. 이때, 내가 원하지 않는 prior를 <strong>skeptical prior</strong>라고 한다. 만약, skeptical prior로 시도해본 여러 모델들이 모두 내가 원한 prior 모델보다 일정 이상 성능이 좋지 않으면 나의 prior 선택을 증명 또는 설명할 수 있다.</p><p>Prior sensitivity analysis를 할때, 해당 prior로 posterior estimation을 통해 성능을 측정하게 된다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Prior-Sensitivity-Analysis&quot;&gt;&lt;a href=&quot;#Prior-Sensitivity-Analysis&quot; class=&quot;headerlink&quot; title=&quot;Prior Sensitivity Analysis&quot;&gt;&lt;/a&gt;Prior Se
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>03. Frequentist Inference</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/03_Frequentist_inference/</id>
    <published>2020-03-01T12:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Frequentist-Inference"><a href="#Frequentist-Inference" class="headerlink" title="Frequentist Inference"></a>Frequentist Inference</h1><p>Frequentist statistics에서는 sample space의 분포 파라미터 $\theta$를 추정할 때, 다음과 같은 방법으로 추론할 수 있다.</p><ol><li>일단 데이터를 많이 모은다. $X_1 = d_1, X_2 = d_2, …, X_n=d_n$</li><li>Central limit theorem을 이용해서 데이터의 평균치 또는 합을 계산한다. $\bar{X} = \frac{1}{n}\sum\limits_i X_i$</li><li>이 평균치는 $\theta$에 대한 함수일 것이고(애초에 $X_i$가 $\theta$에 대한 함수임) 이 평균치는 $\mathbb{N}(\bar{X}, \frac{\sigma}{\sqrt{n}})$의 분포를 이룬다. (합의 경우는 $\mathbb{N}(n\bar{X},\sigma)$)</li><li>이 분포에 대해 confidence interval을 계산하고, $\bar{X}$주위 그 interval 안에 해당 confidence ($p-value$) 의 자신감으로, 진짜 $\mu$가 있다고 가정한다.</li></ol><p>주의할 점은, $p-value$는 $\mu$가 그 confidence의 확률로 interval안에 있다는 것이 아니다. $\mu$는 고정되어 있는 값이라서 그 interval 안에 있을 확률은 0 아니면 1이다. 다만, $\mu$가 거기에 있을 것이라는 95%($p-value=95$)의 자신감이 있을 뿐이다.</p><h2 id="Confidence-Interval"><a href="#Confidence-Interval" class="headerlink" title="Confidence Interval"></a>Confidence Interval</h2><p>동전던지기 시행에서 앞면이 나올 확률 $p$를 알고 싶다.</p><p>100번 던져본다. 각 시행은 $X_i$이다. 이때, 100번의 시행을 모두 더한 random variable $Y=\sum\limits_i X_i$를 정의한다. 그럼 $Y$는 다음의 분포를 따른다.<br>$$<br>Y \approx \mathbb{N}(100p, 100p(1-p))<br>$$<br>$Y = \frac{1}{n} \sum\limits_i X_i$라고 정의했다면, $Y \approx \mathbb{N}(p, \frac{p(1-p)}{\sqrt{n}})$가 되겠다.</p><p>어쨌든, 55번의 H, 45번의 T이 나왔다면, frequentist statistics의 확률 정의에 의해 $\hat{p}=0.55$이고 이 추정치는 95%, 97%, 99% confidence interval로 어느정도 true $p$에 가깝다고 확신을 내릴 수 있다. 95%를 예로 들면,<br>$$<br>55 - 1.96 * 100 * 0.55 * 0.45 \leq 100p \leq 55 + 1.96 * 100 * 0.55 * 0.45<br>$$<br>로 $100p$에 대한 confidence interval을 계산할 수 있다.</p><h2 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h2><p>데이터를 확률분포 $p(\mathbb{D}|\theta)$로부터 샘플링했을 때, 가지고 있는 데이터가 샘플링 됬을 확률을 $p(D|\theta)$라고 표현한다면, 이를 liklihood라고 한다. 이 likelihood를 최대화하는 파라미터 $\theta$를 찾으면, 즉, likelihood를 최대화하는 분포를 구하면, 그것이 sample space분포인 $p(\mathbb{D}|\theta)$와 매우 유사할 것이라는 것이라고 가정한다. 따라서 likelihood를 최대화하는 파라미터 $\theta$를 찾고, 나아가 sample space distribution을 추정하는 방법을 MLE(Maximum likelihood estimation)라고 부른다.</p><p>Likelihood를 최대화하는 $\hat{\theta}$를 구하는데 이용하는 방법은 미분하고 derivatives를 0으로 하는 $\theta$를 구하는 것이다. 즉, 극점을 구하는 것이다.</p><p>예를 들어, 동전이 fair한지, loaded인지 구하려고 한다. 만약, fair한 동전이라면 앞 뒷면이 나올 확률은 0.5로 같다. loaded 동전이라면 앞면이 나올 확률은 0.7, 뒷면이 나올 확률은 0.3이라고 하자.</p><p>동전을 다섯 번 던져서 5개의 데이터를 얻었다. 이때, 2번은 앞면, 3번은 뒷면이 나왔다.</p><p>이때, liklihood는 동전이 fair일때와, loaded일때에 대해서 각각 구할 수 있다.<br>$$<br>p(D|\theta) = \begin{cases} \begin{pmatrix} 5 \ 2 \end{pmatrix} * 0.5^5 &amp; \text{if } \theta \text{ is fair} \ \begin{pmatrix} 5 \ 2 \end{pmatrix} * 0.7^2 * 0.3^2 &amp; \text{if } \theta \text{ is loaded} \end{cases}<br>$$<br>결과를 구해보면, $\theta$가 fair일때의 $p(D|\theta)$가 더 높다는 것을 알 수 있다. 즉, $\theta$가 fair일때, likelihood가 더 높다. 따라서 MLE에 의해 likelihood가 최대화되는 $\theta=\text{fair}$ 이라고 추정할 수 있다.</p><p>그런데, 동전은 물리적인 물체이므로 데이터가 주어졌을 때의 동전이 fair할 확률 $p(\theta=\text{fair}|D)$은 $p(\theta=\text{fair})$와 같다. 동전이 fair한지 안하는지는 변하지 않는 것이고 데이터셋과 상관없이 결정된 것이기 때문이다. 따라서 다음과 같다.<br>$$<br>p(\theta=\text{fair}|D) = p(\theta=\text{fair}) \in {0, 1}<br>$$</p><p>즉, frequentist inference는 다음과 같이 정리할 수 있다.<br>$$<br>\hat{\theta} = argmax_{\theta} ~p(D|\theta)<br>$$</p><p>다른 예시로, 개와 고양이를 구분하는 classifier를 구현하고 싶다고 하자. MLE 방법에서는 $\theta \in {개, 고양이}$이고, 사진을 보여주고 frequentist inference를 한다고 하자. 만약, 개라면 사진처럼 생겼을 확률과 고양이라면 사진처럼 생겼을 확률을 비교하고, 개라면 사진처럼 생겼을 확률이 높으면 개라고 판단하고, 고양이라면 사진처럼 생겼을 확률이 높다면 고양이로 판단한다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Frequentist-Inference&quot;&gt;&lt;a href=&quot;#Frequentist-Inference&quot; class=&quot;headerlink&quot; title=&quot;Frequentist Inference&quot;&gt;&lt;/a&gt;Frequentist Inference&lt;/
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>01. Probability</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/01_Probability/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/01_Probability/</id>
    <published>2020-03-01T12:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.075Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>시행이라고도 불리며, 데이터 샘플 한개를 샘플링하는 행위를 말한다.</p><h3 id="Sample-Space"><a href="#Sample-Space" class="headerlink" title="Sample Space"></a>Sample Space</h3><p>Sample space란, 데이터의 도메인을 의미한다. <strong>어떤 시행에 의해 나올 수 있는 모든 결과의 집합이다.</strong></p><p>즉, 동전 던지기를 10번 하는 experiment에서 앞면의 개수를 표현하는 sample space는 ${0, 1, 2, …, 10}$을 포함한다. 주의할 점은, sample space가 정확히 저거다 라고 말할 수는 없다. Sample space에서 샘플링될 확률이 높아서 샘플링된 데이터도 있지만, 확률이 극도로 낮아서 샘플링되지 못하고 있는 데이터도 있기 마련이다.</p><p>한마디로 말하면, sample space는 인간이 정확히 정의할 수 없는 영역이다.</p><h3 id="Events"><a href="#Events" class="headerlink" title="Events"></a>Events</h3><p>사건 이라고도 불리며, 어떤 experiment로부터 나올 수 있는 결과 하나하나를 의미한다.</p><p>동전을 10번 던지는 시행 1회에서는,  다음과 같은 event들이 있을 수 있다.</p><ul><li>앞면이 0번 나오는 event.</li><li>앞면이 1번 나오는 event.</li><li>…</li></ul><h3 id="Random-Variables"><a href="#Random-Variables" class="headerlink" title="Random Variables"></a>Random Variables</h3><p>확률 변수라고도 불리며, 어떤 experiment를 수행하면, 확률적으로 어떤 event가 발생할 것이다. 이러한, experiment &lt;-&gt; event 와의 확률 매핑 관계를 random variable이라고 한다.</p><p>하버드 어느 통계학 강의에 따르면, random variable을 sample space의 event들을 integer number 값으로 매핑하는 함수라고 정의한다.</p><p>확률 변수는 변수이고, 그 확률 변수의 sample space내의 어떠한 event가 될 수 있다.<br>$$<br>P(X=x_k)​<br>$$<br>이때, $x_k$는 어떤 이벤트이고, $X$는 random variable이다.</p><p>하나의 experiment는 하나의 sample space를 가지며, 하나의 random variable과 대응된다. 그 sample space안에 여러개의 event가 있을 수 있다.</p><h2 id="Definition-of-Probability"><a href="#Definition-of-Probability" class="headerlink" title="Definition of Probability"></a>Definition of Probability</h2><p><strong>확률은 불확실성을 정량화하는 도구이다.</strong></p><p>0과 1사이 값을 가지는 값이다.</p><p>어떤 experiment(시행)에 대한 random variable(확률변수) $X$가 있고, $X$는 $x_1,…,k$의 event를 가질 수 있을 때,<br>$$<br>0 \leq P(X=x_i) \leq 1<br>$$<br>를 $x_i$가 일어날 확률이라고 정의한다.</p><p>일단, 기본적인 확률의 정의는 위와 같다.</p><h3 id="Odds"><a href="#Odds" class="headerlink" title="Odds"></a>Odds</h3><p>어떤 event a에 대한 odds는 $O(a) = \frac{P(a)}{P(a^C)}$라고 정의한다. 즉, 동전던지기에서 앞면이 나올 확률이 0.3이라면, 앞면이 나올 event에 대한 odds는 $O(X=h)=\frac{P(X=h)}{P(X \not = h)} = \frac{0.3}{0.7} = \frac{3}{7}$이다.</p><h2 id="How-to-Compute-Probability"><a href="#How-to-Compute-Probability" class="headerlink" title="How to Compute Probability"></a>How to Compute Probability</h2><p>Frequentist statistics와 bayesian statistics, 두 통계학에서는 확률을 계산하는 방법에 차이가 있다.</p><h3 id="Classical-Method"><a href="#Classical-Method" class="headerlink" title="Classical Method"></a>Classical Method</h3><blockquote><p>Equally Likely Probability</p></blockquote><p>Sample space에서 모든 event들은 일어날 확률이 같다고 정의한다.</p><p>동전 1번 던지는 시행에서 sample space는 앞면, 뒷면만 있다고 가정한다. 그럼 앞면이 나올 확률은 0.5이고, 뒷면이 나올 확률 역시 0.5이라고 정의한다.</p><h3 id="Probability-in-Frequentist-Statistics"><a href="#Probability-in-Frequentist-Statistics" class="headerlink" title="Probability in Frequentist Statistics"></a>Probability in Frequentist Statistics</h3><blockquote><p>Relative Rates of Events in Infinite Sequence</p></blockquote><p>Frequentist statistics에서는 어떤 event의 확률을 “수많은 시행 가운데 그 event가 일어난 비율”이라고 정의한다.</p><p>즉, 동전 던지기에서 앞면이 나올 확률을 계산하고 싶다면, 일단 동전을 무수히 많이 던져본다. 1000번을 던진 후, 651번의 앞면이 나왔다면, 동전 던지기 시행에서 앞면이 나올 확률은 0.651 로 정의한다.</p><h3 id="Probability-in-Bayesian-Statistics"><a href="#Probability-in-Bayesian-Statistics" class="headerlink" title="Probability in Bayesian Statistics"></a>Probability in Bayesian Statistics</h3><blockquote><p>Personal Perspective</p></blockquote><p>Frequentist statistics에서 확률의 문제점은 샘플링이 가능해야 확률을 정의할 수 있다는 점이다. 그런데, 실제로는 샘플링 없이 확률을 정의해야 하는 경우가 많다.</p><p>예를 들어, 내일 비가 올 확률은 frequentist statistics 에서는 설명이 불가능하다. 샘플링을 하기 위해서는 타임머신이 필요해 보인다.</p><p>또 다른 예시로, 주사위가 fair할 확률 $P(fair)$을 보자. frequentist statistics에서는 주사위는 물리적인 물체이기 때문에 공장에서 만들어졌을 때 부터 fair한것이면 fair한것이고 아니면 아닌 것이다. 즉, deterministic한 요소이며, 확률적으로 어쩔때는 fair하고 어쩔때는 unfair하고 그러지 않는다는 것이다. 따라서 $P(fair) = {0,1}$이다.</p><p>Bayesian statistics에서는 모든 것은 deterministic하지 않다. 즉, 0%, 100%는 존재하지 않으며 불확실성이 항상 존재한다고 가정한다.</p><p>Bayesian statistics에서의 확률이란, 개인의 믿음에 관련되어 있다. 예를 들어, 동전 던지기 시행에서 앞면이 나올 확률은 “내가 생각하기에는 0.3일 것이다.”이면, 0.3인 것이다. 다만, 이 확률을 정할 때, fair bet을 만족해야 한다.</p><p>예를 들어, 만약, 앞면이 나오면 7달러를 얻고, 뒷면이 나오면 3달러를 잃는다고 하자. 그럼 다음을 만족해야 한다.<br>$$<br>E[gain] = p*7 + (1-p) * (-3) = 0​<br>$$<br>즉, 내가 앞면이 나올 확률에 어느정도 베팅을 할 수 있는지에 대한 자신감을 고려해서 최대한 공평하게 확률을 선정해야 한다. 이때 $p=0.3$이 되야 한다.</p><h3 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h3><p>다음을 bayes’ rule이라고 정의한다.<br>$$<br>P(A=a_1|B) = \frac{P(B|A=a_1)P(A=a_1)}{\sum\limits_{a \in A} P(B|A=a)P(A=a)}​<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Probability&quot;&gt;&lt;a href=&quot;#Probability&quot; class=&quot;headerlink&quot; title=&quot;Probability&quot;&gt;&lt;/a&gt;Probability&lt;/h1&gt;&lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Backgro
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>05. Credible Intervals</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/05_Credible_Intervals/</id>
    <published>2020-03-01T12:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Credible-Intervals"><a href="#Credible-Intervals" class="headerlink" title="Credible Intervals"></a>Credible Intervals</h1><p>Prior로 $\theta$의 distribution $p(\theta)$를 정한 후, 데이터를 이용해서 bayesian inference과정을 거쳤다고 하자. 따라서, $p(\theta|D)$를 구했다. 그리고, 이 posterior속에서 파라미터 $\theta$가 어디쯤에 위치할지, credible interval을 계산할 수 있다. 이는 frequentist statistics에서의 confidence interval과 매우 유사하지만, 다음의 차이점이 있다.</p><ol><li>Confidence interval은 $\theta$는 고정되어 있고 bound 경계가 random variable이다. 반면, credible interval에서는 $\theta$가 random variable이고 bound가 고정된 값이다.<ul><li>confidence interval은 이 구간 사이에 모 파라미터 $\theta$가 있을 것이라는 자신감이 있을 뿐, $\theta$가 그 구간에 위치할 확률이 p-value가 되는 것이 아니다. Frequentist statistics에서는 $\theta$는 고정되어 있고 변하지 않는다. </li><li>반면, credible interval은 그 구간 내에 $\theta$가 있을 확률을 의미한다.</li></ul></li></ol><p>Posterior를 충분한 데이터로 구했다면, 사전 지식과 합쳐서 credible interval을 계산하고 $\theta$가 어느 범위에 있을 확률을 구하는 것이다.</p><h3 id="Equal-Tailed-Interval"><a href="#Equal-Tailed-Interval" class="headerlink" title="Equal-Tailed Interval"></a>Equal-Tailed Interval</h3><p>95%의 credible interval을 구하고 싶다면, 한쪽 끝에서 2.5%의 bound를 계산하고 다른 한 쪽 끝에서 2.5%의 bound를 계산한다. 그리고 그 사이가 equal tailed interval이 된다.</p><h3 id="Highest-Posterior-Density"><a href="#Highest-Posterior-Density" class="headerlink" title="Highest Posterior Density"></a>Highest Posterior Density</h3><p>양쪽 끝을 같은 확률로 자르지말고, 확률이 높은 구간을 최대한 포함하자는 것이다. 만약, $p(\theta|D) = 2\theta$의 경우, 오른쪽 꼬리는 매우 확률이 높은 구간인데, 자르기 아깝다는 것이다. 따라서 확률이 낮은 왼쪽 꼬리만 잘라서 interval을 구한다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Credible-Intervals&quot;&gt;&lt;a href=&quot;#Credible-Intervals&quot; class=&quot;headerlink&quot; title=&quot;Credible Intervals&quot;&gt;&lt;/a&gt;Credible Intervals&lt;/h1&gt;&lt;p&gt;Prior로
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>04. Bayesian Inference</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/04_Bayesian_inference/</id>
    <published>2020-03-01T12:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayesian-Inference"><a href="#Bayesian-Inference" class="headerlink" title="Bayesian Inference"></a>Bayesian Inference</h1><p>우리가 추정하고자 하는 parameter $\theta$에 대해 $\theta$가 어떻게 분포되어있는지 사전 지식 또는 정보가 있다면 그것을 이용하는게 좋을 것이다. 하지만, frequentist inference에서는 사전 정보를 이용하기 어렵다.</p><p>사전 정보를 이용해서 $p(\theta)$를 초기화한후(prior), 데이터를 수집하면서 얻은 정보(posterior)를 이용해서 $p(\theta)$분포를 $p(\theta|D)$로 업데이트한다. 이렇게 $p(\theta)$을 추정해 가는 방식을 <strong>bayesian inference</strong>라고 한다. 그리고, 얻은 데이터를 바탕으로 $p(\theta|D)$를 최대화하는 $\hat{\theta}$를 선택하는 것을 <strong>Maximize A Posterior(MAP)</strong> 추정이라고 한다.</p><p>즉, 다음과 같다.<br>$$<br>\hat{\theta} = argmax_{\theta} ~p(\theta|D)<br>$$</p><p>개와 고양이를 판별하는 classifier를 만든다고 치자. 역시 $\theta \in {개, 고양이}$이고, 사진을 주고 bayesian inference로 개인지, 고양이인지 판단을 한다고 하면,  사진처럼 생겼을 경우 개일 확률과 사진처럼 생겼을 경우 고양이일 확률을 비교한다. 사진처럼 생겼을때, 고양이일 확률이 개일 확률보다 높으면 고양이라고 추정하는 방식이 MAP이다.</p><p>그런데, $p(\theta|D)$는 다음과 같이 계산한다.<br>$$<br>p(\theta|D) = \frac{p(D|\theta)p(\theta)}{\sum_ip(D|\theta_i)p(\theta_i)}<br>$$<br>즉, 사후확률 $p(\theta|D)$는 관찰된 데이터의 likelihood $p(D|\theta)$와 사전확률 $p(\theta)$을 이용해서 계산된다. 그리고 계산된 사후확률 $p(\theta|D)$를 이용해서 $p(\theta)$를 업데이트한다(단순 대입, $p(\theta) \leftarrow p(\theta|D)$). 이렇게 데이터를 모은 정보를 바탕으로 prior를 posterior로 업데이트 해 가면서 $\theta$에 대한 분포 $p(\theta)$를 추정해 나가는 방식을 bayesian inference라고 한다.</p><p>주의할 점은 prior $p(\theta)$를 어느 특정 지점에서 0 또는 1로 설정하면, posterior에서도 그 지점은 0 또는 1이 된다. 따라서 왠만하면 0 또는 1을 어떤 지점에 할당하지 않도록 한다.<br>$$<br>p(\theta|D) \propto p(D|\theta)p(\theta) = p(\theta)<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayesian-Inference&quot;&gt;&lt;a href=&quot;#Bayesian-Inference&quot; class=&quot;headerlink&quot; title=&quot;Bayesian Inference&quot;&gt;&lt;/a&gt;Bayesian Inference&lt;/h1&gt;&lt;p&gt;우리가 추정
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>07. Priors</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/07_Priors/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/07_Priors/</id>
    <published>2020-03-01T12:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.081Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Priors"><a href="#Priors" class="headerlink" title="Priors"></a>Priors</h1><p>Prior를 어떤 분포로 선택할지에 대해서는 conjugatation을 고려해야 한다. prior으로 선택한 distribution이 likelihood와 곱해져서 posterior가 되었을 때도 그 distribution이 되어야 한다는 의미이다.</p><h2 id="Effective-Sample-Size"><a href="#Effective-Sample-Size" class="headerlink" title="Effective Sample Size"></a>Effective Sample Size</h2><p>Effective sample size란, 설정한 prior의 영향이 posterior에 영향을 50%만 미치는 순간의 샘플 개수를 말한다. 샘플 개수가 적으면 prior는 posterior에 영향을 많이 미칠 테지만, effective sample size이상의 샘플을 모은다면, prior가 posterior에 미치는 영향이 50% 이내일 것이다.</p><h2 id="Priors-in-Binomial-Likelihood"><a href="#Priors-in-Binomial-Likelihood" class="headerlink" title="Priors in Binomial Likelihood"></a>Priors in Binomial Likelihood</h2><p>예를들어, binomial distribution으로 likelihood를 모델링하는 경우, 즉, 제품 생산과정에서 불량의 빈도수 같은 경우는 prior를 beta distribution으로 불량일 확률 $\theta$를 모델링한다.</p><p>이때, beta distribution의 parameter $\alpha$와 $\beta$를 어떻게 정해야 할까, beta distribution의 평균은 $\frac{\alpha}{\alpha+\beta}$라는 것을 기억한다. 만약, 우리가 0.3%확률로 불량이 나올 것이라고 믿는다면, $\frac{\alpha}{\alpha + \beta}=0.3$이 되게끔 정하면 된다. 다만, 이러면 경우의 수가 많은데, $\alpha$와 $\beta$값이 커지면 그 믿음에 자신감이 있는 것이다.</p><p>30%의 확률로 불량이 있다고 생각해서 불량률 $\theta$에 대해 $\theta \approx \text{beta}(6, 14)$으로 prior를 설정했다고 하자. 그리고 10번의 생산 후 6개의 불량이 나왔다. 이때, $\theta$의 posterior는 $\text{beta}(6+6, 14+4) = \text{beta}(12, 18)$이 된다. <strong>$\alpha$는 불량인 것의 개수와 관련있고, $\beta$는 불량이 아닌 것과 관련이 있는 것이다.</strong> 실제로 계산해봐도 그렇다.<br>$$<br>P(\theta) = \frac{\Gamma(6 + 20)}{\Gamma(6)\Gamma(14)}\theta^{6-1}(1-\theta)^{14 - 1}<br>$$<br>$$<br>P(X|\theta) = \begin{pmatrix} 10 \ 6 \end{pmatrix}\theta^6(1-\theta)^4<br>$$</p><p>$$<br>P(\theta|X) = P(X|\theta)P(\theta) = \frac{25!10!}{5!19!6!4!}\theta^{12-1}(1-\theta)^{18-1}<br>$$</p><p>$$<br>P(\theta|X) \propto \text{beta}(12, 18)<br>$$</p><p>앞의 상수들은 다 상수일 뿐. 어쨌든 beta distribution에 근사된다.</p><h3 id="Effective-Sample-Size-1"><a href="#Effective-Sample-Size-1" class="headerlink" title="Effective Sample Size"></a>Effective Sample Size</h3><p>Binomial likelihood에서 beta distribution을 $\theta$의 prior로 했을 경우, effective sample size는 $\alpha+\beta$가 된다.</p><p>Posterior는 다음과 같다.<br>$$<br>\text{Posterior}(\theta|X) = \text{Beta}(\alpha+\sum_i^n x_i, \beta + n - \sum_i^n x_i)\<br>$$<br>이때, posterior mean은 $\frac{\alpha + \sum_i^n x_i}{\alpha + \beta + n}$인데, 이를 더 decompose해보면, 다음 식이 나온다.<br>$$<br>\frac{\alpha + \sum_i^n x_i}{\alpha + \beta + n} = \frac{\alpha + \beta}{\alpha + \beta + n} \cdot \frac{\alpha}{\alpha + \beta} + \frac{n}{\alpha + \beta + n} \cdot \frac{\sum_i^n x_i}{n}<br>$$<br>이것은 prior mean과 data mean과의 weighted sum으로 해석할 수 있다. 즉, posterior mean은 prior mean과 data mean에 의해 영향을 받는다. 그런데, 이때, 샘플 개수 $n$이 작으면, prior의 영향력이 커진다. 반면, $n$이 커지면, data의 영향력이 커진다. $n \geq \alpha+\beta$ 일때, prior보다 데이터의 영향력이 커진다. 따라서, effective sample size는 $\alpha + \beta$이다.</p><p>Prior를 정의할때, $\alpha, \beta$를 크게 잡던, 작게 잡던, $\alpha$와 $\alpha+\beta$의 비율이 같으면, prior mean은 같지만, 값들이 크면, prior의 영향력이 강해지기 때문에 sample 개수를 많이 모아야 한다.</p><h2 id="Priors-in-Poisson-Distribution"><a href="#Priors-in-Poisson-Distribution" class="headerlink" title="Priors in Poisson Distribution"></a>Priors in Poisson Distribution</h2><p>Poisson distribution을 likelihood로 취하는 experiment에 대해서는 parameter가 $\lambda$가 된다. 즉, $\lambda$에 대한 prior가 필요한데, 이때는 Gamma distribution으로 $\lambda$의 prior를 모델링한다. Poisson distribution으로 likelihood를 모델링 할 수 있는 경우, Gamma distribution이 conjugate한 distribution이다.</p><p>이때, Gamma distribution의 두 파라미터 $\alpha$와 $\beta$를 정할때, gamma distribution의 평균은 $\frac{\alpha}{\beta}$인 것을 생각하자. <strong><del>$\alpha$는 event 발생 횟수, $\beta$는 총 시행 횟수와 관련이 있다.</del></strong></p><p>이때, poisson이므로, 1번의 시행에서 event가 여러번 발생할 수 있다. 특정 시간 안에 몇 번의 버스가 오는가?</p><h3 id="Effective-Sample-Size-2"><a href="#Effective-Sample-Size-2" class="headerlink" title="Effective Sample Size"></a>Effective Sample Size</h3><p>Beta distribution을 prior로 삼고, posterior도 역시 beta distribution이기 때문에, effective sample size는 $\alpha+\beta$이다.</p><h2 id="Priors-in-Exponential-Distribution"><a href="#Priors-in-Exponential-Distribution" class="headerlink" title="Priors in Exponential Distribution"></a>Priors in Exponential Distribution</h2><p>Exponential distribution도 역시 $\lambda$를 파라미터로 하며, gamma distribution을 prior로 하면 conjugate인 prior를 만들 수 있다.</p><h3 id="Effective-Sample-Size-3"><a href="#Effective-Sample-Size-3" class="headerlink" title="Effective Sample Size"></a>Effective Sample Size</h3><p>Gamma prior로 conjugate인 likelihood의 경우, posterior도 gamma distribution이다. 이런 경우, effective sample size는 $\beta$이다.<br>$$<br>\text{Posterior}(\lambda|X) = \text{Gamma}(\alpha + \sum_i^n x_i, \beta + n)<br>$$</p><p>$$<br>\text{mean}(posterior) = \frac{\alpha + \sum_i^n x_i}{\beta + n}<br>$$</p><p>$$<br>\frac{\alpha + \sum_i^n x_i}{\beta + n} = \frac{\beta}{\beta + n} \cdot \frac{\alpha}{\beta} + \frac{n}{\beta + n} \cdot \frac{\sum_i^n x_i}{n}<br>$$</p><h2 id="Priors-in-Normal-Distribution"><a href="#Priors-in-Normal-Distribution" class="headerlink" title="Priors in Normal Distribution"></a>Priors in Normal Distribution</h2><p>Normal distribution의 파라미터 $\mu$는 $\sigma$에 의존함과 동시에 normal distribution prior에서 conjugate한다. $\sigma$는 주어젔다고 가정하는 경우가 많으며, 그렇지 않을 경우, inverse-gamma distribution에서 conjugate한다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Priors&quot;&gt;&lt;a href=&quot;#Priors&quot; class=&quot;headerlink&quot; title=&quot;Priors&quot;&gt;&lt;/a&gt;Priors&lt;/h1&gt;&lt;p&gt;Prior를 어떤 분포로 선택할지에 대해서는 conjugatation을 고려해야 한다. prior
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>06. Prior Predictive Distribution</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/06_Prior_Posterior_predictive/</id>
    <published>2020-03-01T12:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.080Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prior-Predictive-Distribution"><a href="#Prior-Predictive-Distribution" class="headerlink" title="Prior Predictive Distribution"></a>Prior Predictive Distribution</h1><p>Prior정보만으로, 즉, 데이터 없이 데이터가 어떻게 분포할지,즉 $p(D)$를 추정해본 분포이다.</p><p>데이터 분포 $p(D)$는 다음과 같이 쓸 수 있다.<br>$$<br>p(d) = \int_0^1 p(d|\theta)p(\theta) ~d\theta<br>$$<br>이때, 데이터 없이 사전 정보만으로 $p(D)$를 추정하는데, 사전정보로 추정한 데이터 분포 $p(D)$를 추정한 것을 prior predictive distribution이라고 부른다.</p><p>Prior predictive distribution은 데이터 수집 전에, prior정보만을 이용해서 데이터 sample space distribution을 추정한 것이라고 할 수 있다.</p><h1 id="Posterior-Predictive-Distribution"><a href="#Posterior-Predictive-Distribution" class="headerlink" title="Posterior Predictive Distribution"></a>Posterior Predictive Distribution</h1><p>데이터를 수집한 후, 데이터의 분포를 추정한 분포를 말한다.</p><p>데이터 $d_1$를 수집했다고 치자. 그럼 다음에 샘플링될 $d_2$의 확률분포는 다음과 같다.<br>$$<br>p(d_2|d_1) = \int_0^1 p(d_2|d_1,\theta)p(\theta|d_1)d\theta<br>$$<br>이때, $d_1 \perp d_2$이므로, 다음과 같다.<br>$$<br>p(d_2|d_1) = \int_0^1 p(d_2|\theta)p(\theta|d_1)d\theta<br>$$<br>Prior predictive distribution과 다른 점은 prior 자리에 posterior가 들어갔다는 점이다.</p><p>Posterior predictive distribution은 데이터를 관찰한 후, 그 정보를 이용해서 데이터 sample space 분포를 추정한 것이라고 할 수 있다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Prior-Predictive-Distribution&quot;&gt;&lt;a href=&quot;#Prior-Predictive-Distribution&quot; class=&quot;headerlink&quot; title=&quot;Prior Predictive Distribution&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>02. Distribution</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/02_Distribution/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/02_Distribution/</id>
    <published>2020-03-01T12:08:00.000Z</published>
    <updated>2020-03-03T00:54:07.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Distribution"><a href="#Distribution" class="headerlink" title="Distribution"></a>Distribution</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Indicator-Function"><a href="#Indicator-Function" class="headerlink" title="Indicator Function"></a>Indicator Function</h3><p>어떤 조건이 만족했을때, 1을 반환하고, 만족하지 못하면 0을 반환하는 함수이다.<br>$$<br>\mathbb{I}<em>{\text{cond}(x)}(x) = \begin{cases} 1 &amp; \text{if cond(x) is True} \ 0 &amp; \text{if cond(x) is False} \end{cases}<br>$$<br>Indicator function은 다음과 같이 동전 던지기에 대한 확률같은 것을 표현할 때 용이하다.<br>$$<br>P(X|\theta) = \theta \cdot \mathbb{I}</em>{\text{head}} + (1 - \theta) \cdot \mathbb{I}_{\text{tail}}<br>$$</p><h3 id="Expected-Values"><a href="#Expected-Values" class="headerlink" title="Expected Values"></a>Expected Values</h3><p><strong>기댓값</strong>이라고도 불리며, 확률적인 관점에서 본 <strong>“평균”</strong>이다.<br>$$<br>E[X] = \sum\limits_{x} p_x x<br>$$<br>각 샘플들에 비중치를 곱해서 모두 더한 것이다.</p><p>일반적으로 생각할 수 있는 평균값은 모든 샘플들에 같은 비중치를 둔 기댓값과 같다.</p><h3 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h3><p><strong>분산</strong>이라고도 불리며, 샘플들이 평균 또는 기댓값으로부터 얼마나 떨어져서 분포하는가, 즉, <strong>샘플들이 얼마나 넓게 퍼져있는가</strong>를 나타낸다.<br>$$<br>\text{Var}[X] = E[(X-\mu)^2] = \sum\limits_{x} p_x(x - \mu)^2<br>$$<br>평균과의 거리의 제곱을 평균한 것인데, 단위가 제곱이 된다. 따라서 단위를 일치시키기 위해 제곱근을 씌우는데, 이를 <strong>standard deviation(표준편차)</strong>라고 부른다.<br>$$<br>\text{Std.}[X] = \sigma(X) = \sqrt{\text{Var}[X]}<br>$$</p><h3 id="Scaling-Random-Variable-vs-Many-Random-Variables"><a href="#Scaling-Random-Variable-vs-Many-Random-Variables" class="headerlink" title="Scaling Random Variable vs Many Random Variables"></a>Scaling Random Variable vs Many Random Variables</h3><p>Random variable을 scaling 한다는 것은, 분포를 넓게 피는것을 의미한다. random variable $$X$$를 $$c$$배 scaling하는 것은 $$cX$$로 표기한다.</p><p>반면, random variable $$X$$를 여러번 시행하는 것은 $$\sum_i^n X_i$$로 표기한다. 둘이 분명히 다른데, $$cX$$의 경우, 1번 샘플링하는 것이고, $$\sum X$$는 여러번 샘플링 하는 것이다.</p><p>예를들어, $$X$$가 베르누이 분포를 따르고, 1일 확률이 0.7 이라면, $$10X$$는 0일 확률이 0.3, <strong>10일 확률</strong>이 0.7인 것이다.</p><p>반면, $$\sum_i^{10} X_i$$는 1이 나올 확률이 0.7인 분포에서 10번 샘플링하는 것이다.</p><h2 id="Discrete-Distribution"><a href="#Discrete-Distribution" class="headerlink" title="Discrete Distribution"></a>Discrete Distribution</h2><h3 id="Bernoulli-Distribution"><a href="#Bernoulli-Distribution" class="headerlink" title="Bernoulli Distribution"></a>Bernoulli Distribution</h3><p>Sample space의 크기가 2(event 개수가 2개)라고 추정되는 random variable $X$가 있을때, 이 $X$는 Bernoulli distribution(베르누이 분포)을 따른다. 베르누이 분포를 따르는 random variable의 1회 experiment을 베르누이 시행이라고 한다.<br>$$<br>\text{Bern}(X|\theta) = \theta<em>\mathbb{I}<em>{X=1} + (1 - \theta) * \mathbb{I}</em>{X=0}<br>$$<br><strong>Expected value:</strong><br>$$<br>E[X] = \theta<br>$$<br><strong>Variance:</strong><br>$$<br>\sigma^2(X) = \theta * (1 - \theta)^2 + (1-\theta)</em>(0 - \theta)^2 = \theta(1-\theta)<br>$$</p><h3 id="Binomial-Distribution"><a href="#Binomial-Distribution" class="headerlink" title="Binomial Distribution"></a>Binomial Distribution</h3><p>베르누이 분포를 따르는 experiment를 여러번 시행했을 때, 한 결과가 몇 번이 나왔는가에 대한 분포이다. 흔히, 동전을 10번 던졌을때, 앞면이 몇 번 나오는지에 대한 분포라고 이해하면 편하다. 동전 던지기 1회는 베르누이 시행이다.<br>$$<br>\text{Binom}(n, x|\theta) = \begin{pmatrix} n \ x \end{pmatrix} \theta^x(1-\theta)^{n-x}<br>$$<br><strong>Expected value:</strong><br>$$<br>E[X] = n\theta<br>$$<br><strong>Variance:</strong><br>$$<br>\sigma^2(X) = n\theta(1-\theta)<br>$$</p><h3 id="Geometric-Distribution"><a href="#Geometric-Distribution" class="headerlink" title="Geometric Distribution"></a>Geometric Distribution</h3><p>베르누이 시행을 여러번 반복하는데, 어떤 event가 최초로 일어날때 까지의 시행한 횟수는 geometric distribution을 따른다. (기하분포). $\theta$는 베르누이 시행 1회에서 그 event가 성공할 확률이다.<br>$$<br>\text{Geom}(x|\theta) = \theta * (1-\theta)^{x-1}<br>$$<br><strong>Expected value:</strong><br>$$<br>E[X] = \frac{1}{\theta}<br>$$</p><h3 id="Multinomial-Distribution"><a href="#Multinomial-Distribution" class="headerlink" title="Multinomial Distribution"></a>Multinomial Distribution</h3><p>시행이 베르누이 시행이 아니라, 여러 개의 event가 나올 수 있는 시행일 때의 binomial distribution을 의미한다.<br>$$<br>\text{Multinom}(n, x_1,…,x_{n}|\theta_1, \theta_2, …, \theta_{n}) = \begin{pmatrix} n \ x_1, x_2, …, x_n \end{pmatrix}\theta_1^{x_1}<em>\theta^{x_2}</em> \cdots*\theta^{x_n}<br>$$</p><h3 id="Poisson-Distribution"><a href="#Poisson-Distribution" class="headerlink" title="Poisson Distribution"></a>Poisson Distribution</h3><p>포아송 분포는, 어느 시간 간격 내에 그 event가 몇 번 일어날지에 대한 분포이다. 해당 시간 간격동안에 event가 발생하는 횟수를 $\lambda$라고 하면, 다음과 같다.<br>$$<br>\text{Poisson}(x|\lambda) = \frac{\lambda^xe^{-\lambda}}{x!}<br>$$<br>Expected value:<br>$$<br>E[X] = \lambda<br>$$<br>(애초에 $\lambda$ 정의가 그냥 기댓값이다.)</p><p>여기서, 만약에 binomial distribution을 따르는데, 동전이 앞면이 나올 확률이 너무나도 희박하고, 동전 던지기 experiment를 무한번 한 경우, 그 무한번의 experiment를 일정 기간의 시간이라고 간주하게 되면 poisson distribution와 같다.</p><h2 id="Continuous-Distribution"><a href="#Continuous-Distribution" class="headerlink" title="Continuous Distribution"></a>Continuous Distribution</h2><p>Sample space가 continuous한 경우의 distribution을 말함.</p><h3 id="Exponential-Distribution"><a href="#Exponential-Distribution" class="headerlink" title="Exponential Distribution"></a>Exponential Distribution</h3><p>특정 일이 일어날 때 까지 걸린 시간 또는 기다린 시간의 분포는 exponential distribution을 따른다.<br>$$<br>\text{Exp}(\lambda) = \lambda e^{-\lambda x} \mathbb{I}_{ x \geq 0 }<br>$$<br>여기서 $\lambda$는 어떤 시간 동안에 사건이 발생하는 횟수의 비율을 말한다. 예를 들어, 10분 동안 버스가 3대 오면 $\lambda = 0.3$이다(시간을 10분 단위로 했을 때).</p><h3 id="Gamma-Distribution"><a href="#Gamma-Distribution" class="headerlink" title="Gamma Distribution"></a>Gamma Distribution</h3><p>버스가 올때까지 걸리는 시간을 측정하는 시행이 여러 번 있고, 그들의 총합 시간은 gamma distribution을 따른다. 쉽게 말해서, Gamma distribution을 따르는 $Y$는 exponential distribution을 따르는 $X_i$의 합과 같다.<br>$$<br>Y = \sum_i X_i \<br>p(y|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}y^{\alpha-1}e^{-\beta y} \mathbb{I}_{y \geq 0}(y)<br>$$<br>감마분포는 $\alpha$와 $\beta$를 파라미터로 삼으며, $\alpha = n$, $\beta = \lambda$가 된다.</p><p>$$\alpha$$는 shape parameter로, $$\alpha=1$$이면, exponential distribution이 된다. 또한, $$\alpha$$가 0에 가까워질수록 right-skewed가 된다. $$\alpha$$가 커질수록 normal distribution에 가까워지면서 skewness가 줄어든다(한쪽으로 치우치지 않는다).</p><p>$$\beta = \lambda$$는 rate parameter로, $$\theta = \frac{1}{\beta}$$는 scale parameter이다. 서로 역수 관계이며, 감마 분포를 표기할때, $$(\alpha, \beta)$$로 parameterize하기도 하고 $$(k, \theta)$$로 parameterize하기도 한다. $$\alpha=k$$이지만, $$\beta = \frac{1}{\theta}$$이다. $$\theta$$는 scale parameter로, rate의 역수이다. Scale parameter는 분산의 scaling 정도이며, 클수록 넓게 퍼진다. 즉, rate가 작을수록 넓게 퍼지며, random variable $$X$$의 $$c$$배 scale인 $$cX$$는 $$(k, c\theta)$$가 되는 셈.</p><p>$$\sum_i^{n} X_i$$는 $$(nk, \theta)$$ 효과를 얻는다! 이걸 보면 $$\alpha$$가 exponential의 횟수와 관련이 있을지도 모른다.</p><p><strong>Expected Value:</strong><br>$$<br>E[X] = \frac{\alpha}{\beta}<br>$$<br><strong>Variance:</strong><br>$$<br>\sigma^2(X) = \frac{\alpha}{\beta^2}<br>$$</p><h3 id="Uniform-Distribution"><a href="#Uniform-Distribution" class="headerlink" title="Uniform Distribution"></a>Uniform Distribution</h3><p>모든 sample space범위의 단위 interval에서 확률이 같다.<br>$$<br>\text{Uni}(X) = \frac{1}{b-a}\mathbb{I}_{ a \leq x \leq b }<br>$$<br><strong>Expected Value:</strong><br>$$<br>E[X] = \frac{a+b}{2}<br>$$<br>(a~b까지 나올 확률이 같으므로 샘플링 여러번 하다 보면 평균값은 중앙값인 $\frac{a+b}{2}$이 된다.)</p><p><strong>Variance:</strong><br>$$<br>\sigma^2(X) = \frac{(b-a)^2}{12}<br>$$</p><h3 id="Beta-Distribution"><a href="#Beta-Distribution" class="headerlink" title="Beta Distribution"></a>Beta Distribution</h3><p>Sample space가 0과 1 사이인 분포. 따라서 확률을 모델링할때 이용하기도 한다.<br>$$<br>\text{Beta}(\alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}<br>$$<br>여기서, $\Gamma(x) = (x-1)!$이다. 앞의 $\Gamma$ term들을 풀어보면 binomial coefficient와 비슷하게 생겨서 나중에 binomial distribution을 적분할때, gamma distribution을 이용하면 매우 유용하다.</p><p><strong>Expected Value:</strong><br>$$<br>E[X] = \frac{\alpha}{\alpha + \beta}<br>$$<br><strong>Variance:</strong><br>$$<br>\sigma^2(X) = \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}<br>$$</p><h3 id="Normal-Distribution"><a href="#Normal-Distribution" class="headerlink" title="Normal Distribution"></a>Normal Distribution</h3><p>정규 분포.<br>$$<br>p(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})<br>$$<br><strong>Expected Value:</strong><br>$$<br>E[X] = \mu<br>$$<br><strong>Variance:</strong><br>$$<br>\text{Var}(X) = \sigma^2<br>$$<br>만약, iid(independent identical distribution)에서 샘플링된 여러 샘플들, 즉, 똑같은 분포로부터 독립적인 시행으로 샘플링한 여러 샘플들의 평균 $\bar{X}$은 정규 분포를 따른다. 샘플의 개수를 $n$, 그 샘플들을 샘플링한, 즉, 하나의 샘플을 샘플링한 분포의 실제 평균을 $\mu$, 분산을 $\sigma^2$이라고 했을 때, 다음을 만족한다.<br>$$<br>\bar{X} \sim \mathbb{N}(\mu, \frac{\sigma^2}{n})<br>$$<br>이를 central limit theorem(CLT) 이라고 부른다. 평균 $\mu$는 추정 대상이라서 모르지만, $\sigma^2$는 샘플들의 분산으로 대체한다. 즉, 우리가 샘플링한샘플들의 평균은 실제 샘플 평균으로부터 어느정도 가깝다는 것이다. 또한, 분산은 샘플수에 반비레하는데, 이는 샘플이 많을수록, 진짜 샘플 평균에 가까워진다는 것을 알 수 있다.</p><h3 id="t-Distribution"><a href="#t-Distribution" class="headerlink" title="t-Distribution"></a>t-Distribution</h3><p>Student-t 분포, test용 분포라고도 한다. </p><p>CLT에서, 샘플 평균의 분포를 standarize시키면 standard normal distribution이 아니라, t-distribution이 나온다. 분산값인 $\sigma^2$가 샘플 분산인 $S^2$으로 대체되기 때문이다.<br>$$<br>S^2 = \frac{\sum_i (\bar{X}-X_i)^2}{n-1}<br>$$<br>이러면, $\bar{X}$의 분포는 더 이상 normal distribution이 아닌, t-distribution을 따른다. $\nu = n-1$이라고 했을 때,<br>$$<br>\text{t}(x) = \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}(1+\frac{x^2}{\nu})^{-(\frac{\nu+1}{2})}<br>$$<br>이때, $\nu$는 자유도, degree of freedom이라고 부른다.</p><p><strong>Expected Value: **<br>$$<br>E[X] = 0 <del>~</del>\text{if } \nu \geq 1<br>$$<br>**Variance:</strong><br>$$<br>\text{Var}(X) = \frac{\nu}{\nu-2} <del>~</del>\text{if } \nu \geq 2<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Distribution&quot;&gt;&lt;a href=&quot;#Distribution&quot; class=&quot;headerlink&quot; title=&quot;Distribution&quot;&gt;&lt;/a&gt;Distribution&lt;/h1&gt;&lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Bac
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>Init Blog</title>
    <link href="https://jaeyoung-blog.github.io/wiki/Blog-Init/"/>
    <id>https://jaeyoung-blog.github.io/wiki/Blog-Init/</id>
    <published>2020-03-01T03:39:05.000Z</published>
    <updated>2020-03-03T00:54:07.074Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Blog-Init"><a href="#Blog-Init" class="headerlink" title="Blog Init"></a>Blog Init</h2><p>재영이 화이팅~~</p><p><img src="https://user-images.githubusercontent.com/26294469/74609940-0b6b0880-5132-11ea-9616-d6f8293cc8aa.gif" alt="개장!"></p><p>$$<br>jaeyoung - taeuk<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Blog-Init&quot;&gt;&lt;a href=&quot;#Blog-Init&quot; class=&quot;headerlink&quot; title=&quot;Blog Init&quot;&gt;&lt;/a&gt;Blog Init&lt;/h2&gt;&lt;p&gt;재영이 화이팅~~&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-im
      
    
    </summary>
    
      <category term="Log" scheme="https://jaeyoung-blog.github.io/categories/Log/"/>
    
    
      <category term="Log" scheme="https://jaeyoung-blog.github.io/tags/Log/"/>
    
  </entry>
  
</feed>
