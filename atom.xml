<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jaeyoung&#39;s Blog</title>
  
  <subtitle>Jaeyoung&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jaeyoung-blog.github.io/"/>
  <updated>2020-03-03T01:58:11.178Z</updated>
  <id>https://jaeyoung-blog.github.io/</id>
  
  <author>
    <name>Lee Jaeyoung</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>KL Divergence</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/KL%20Divergence/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/KL Divergence/</id>
    <published>2020-03-03T13:28:59.000Z</published>
    <updated>2020-03-03T01:58:11.178Z</updated>
    
    <content type="html"><![CDATA[<h1 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL-Divergence"></a>KL-Divergence</h1><p>KL-Divergence에는 두 가지가 있다.</p><ul><li>Forward KL-Divergence</li><li>Reverse KL-Divergence</li></ul><p>기본적으로 KL-divergence라고 하면 forward 방식을 가리키며, variational autoencoder에는 reverse방식을 사용한다.</p><h2 id="Forward-KL-Divergence"><a href="#Forward-KL-Divergence" class="headerlink" title="Forward KL-Divergence"></a>Forward KL-Divergence</h2><p>다음이 forward KL-divergence의 식이다.<br>$$<br>KLD(P||Q) = \sum_x P(x) \cdot log(\frac{P(x)}{Q(x)})<br>$$<br>KL-divergence는 두 확률분포 $P,Q$의 유사도를 나타낼 수 있다. 즉, $P,Q$가 서로 비슷한 모양으로 분포된 확률분포라면, KLD값은 낮다.</p><p>이 KL-divergence는 entropy와 관련이 있는데, entropy는 정보량의 기댓값으로, 정보량은 두 확률 사이의 차이가 크면 큰 값을 가진다. 즉, 확률값이 많이 다르면 entropy가 높다.</p><p>두 확률분포간 거리를 최소화하는게 목적이 아니라면, $P,Q$에 두 확률분포를 넣고 거리를 구하면 된다. 보통 $P$는 target, true 확률분포가 들어가고 $Q$에는 측정하고자 하는 대상이 들어간다.</p><p>두 확률분포간 거리를 최소화시키고자 할때는, $P(x)$는 target 확률 분포, 즉, 목표로 하는 확률분포이며, $Q(x)$는 최적화시키고자 하는 확률분포, 즉, 파라미터가 있는 확률분포이다. 그리고 KLD 식을 최소화하는 $Q(x)$를 수정한다. 즉, $P(x)$에 가깝게 $Q(x)$를 수정하게 된다.</p><h3 id="Forward-KLD의-특징"><a href="#Forward-KLD의-특징" class="headerlink" title="Forward KLD의 특징"></a>Forward KLD의 특징</h3><p>Forward KLD는 $P(x)&gt;0$인 모든 지점에 대해서 확률 분포간의 차이를 줄이려고 노력한다. 최적화된 결과, <strong>$P(x)&gt;$*를 만족하는 모든 $x$의 범위를 $Q(x)$가 커버하게 된다.</strong></p><p>다만, 다음처럼, 최소화된 이후의 KLD 값은 상당히 클 수가 있다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/1569470523821.png" alt="1569470523821"></p><p>(그림 출처: <a href="https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/</a>)</p><p>위의 경우, KLD를 최소화한 결과지만, 결과값도 상당히 큰 KLD값을 가진다. 왜냐하면, $P(x)&gt;0$인 전체 범위를 커버하려고 하기 때문에, $Q(x)$를 정확하게 모델링하지 않으면(위의 경우, 두 가우시안의 mixture model로 해야 할 것이다) 위와 같은 문제가 생긴다.</p><h2 id="Reverse-KL-Divergence"><a href="#Reverse-KL-Divergence" class="headerlink" title="Reverse KL-Divergence"></a>Reverse KL-Divergence</h2><p>다음이 Reverse KL-divergence의 식이다.<br>$$<br>RKLD(Q||P) = \sum_x Q(x) \cdot log(\frac{Q(x)}{P(x)})<br>$$</p><h3 id="Reverse-KLD의-특징"><a href="#Reverse-KLD의-특징" class="headerlink" title="Reverse KLD의 특징"></a>Reverse KLD의 특징</h3><p>만약, 두 분포간의 거리를 측정하고자 하면, forward 방식과 별 다를게 없다. 다만, 값의 차이는 있다. KLD는 대칭함수가 아니기 때문이다.</p><p>하지만, 최소화하려고 할 경우, 이번엔 파라미터가 있는 $Q(x)$분포와 target 분포 $P(x)$의 자리가 바뀌었다. 이때는, $Q(x)$가 굳이 $P(x)&gt;0$를 만족하는 모든 $x$범위를 커버하려고 하지 않는다. 식에서 보면, $Q(x) \approx 0$으로 맞춰버리면 그 $x$범위는 최소화가 된다. 즉, 필요한 곳만 볼록 솟게 해서 그 범위에서 최소화를 시키고 나머지 봉우리는 $Q(x) \approx 0$으로 해버리므로, <strong>특정 부분만 캡쳐해서 분포간 거리를 최소화한다.</strong></p><p>따라서 다음 그림처럼 된다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/1569470891541.png" alt="1569470891541"></p><p>(그림 출처: <a href="https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/</a>)</p><h2 id="어떤-KLD를-사용해야-할까"><a href="#어떤-KLD를-사용해야-할까" class="headerlink" title="어떤 KLD를 사용해야 할까"></a>어떤 KLD를 사용해야 할까</h2><p>만약, 모델링한 $Q(x)$가 target 분포 $P(x)$와 매우 가깝다고 자신이 있을 경우, 또는 $P(x)&gt;0$인 모든 $x$를 커버해야 할 경우, forward KLD를 사용하자.</p><p>하지만, 모델링한 $Q(x)$가 target 분포 $P(x)$와 가깝다는 자신이 없고, 분포의 major한 부분만 캡쳐해도 좋은 경우, Reverse KLD를 사용하자.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;KL-Divergence&quot;&gt;&lt;a href=&quot;#KL-Divergence&quot; class=&quot;headerlink&quot; title=&quot;KL-Divergence&quot;&gt;&lt;/a&gt;KL-Divergence&lt;/h1&gt;&lt;p&gt;KL-Divergence에는 두 가지가 있다.&lt;
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Machine Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Machine-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="MachineLearning" scheme="https://jaeyoung-blog.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Markov Models 2</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Hidden_Markov_Models-2/</id>
    <published>2020-03-03T13:28:57.000Z</published>
    <updated>2020-03-03T01:58:01.723Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h1><p>Udemy 강좌: <a href="https://www.udemy.com/course/unsupervised-machine-learning-hidden-markov-models-in-python" target="_blank" rel="noopener">https://www.udemy.com/course/unsupervised-machine-learning-hidden-markov-models-in-python</a></p><p>Hidden Markov model(HMM)은 다음과 같이 maximum likelihood estimation을 이용해서 파라미터를 추정하게 된다.<br>$$<br>\theta^* = \underset{\theta}{\text{argmax}} ~ p(x|\theta)<br>$$<br>다음으로, HMM의 파라미터가 무엇인지 적어본다. $\theta = ?$</p><h3 id="Parameters-of-HMM"><a href="#Parameters-of-HMM" class="headerlink" title="Parameters of HMM"></a>Parameters of HMM</h3><p>Markov model에서의 parameter는 initial distribution vector $$\pi$$와 state transition matrix $A$였다. HMM에서는 state-to-observation matrix $B$가 추가된다. 즉, $\pi, A, B$가 학습 parameter가 된다.</p><ul><li><p>$\pi$</p><p>Initial distribution. row vector이며, hidden state 개수가 $M$개일때, $\pi$는 $(1, M)$ 모양이다. $\pi(i)$하면, $i$번재 state의 initial 확률이다.</p></li><li><p>$A$</p><p>Hidden state transition matrix. 간단하게 state transition matrix이라고도 하며, $t$에서의 hidden state가 주어졌을 때, $t+1$에서의 hidden state의 확률분포이다. 즉, $p(s_{t+1}|s_t)$을 표현한다. 따라서, observation의 종류가 $D$개일때, $M \rightarrow D$이므로, $(M, D)$ 모양이다. $A(i, j)$의 원소는 $p(s_{t+1} = j | s_t = i)$를 의미한다.</p></li><li><p>$B$</p><p>Observation transition matrix이며, $t$에서의 hidden state가 주어졌을 때, $t$에서의 observation의 확률분포이다. $p(x_t|s_t)$를 표현한다. $B(j, k)$의 원소는 $p(x_t = k|s_t = j)$를 의미한다.</p></li></ul><p>이들을 이용한 연산의 예를 잠깐 몇개 들어보면, (Sequence의 시작 index는 1이다.)</p><ul><li>$\pi B = \sum_i \pi(i) B(i,:) = \sum_{z_1} p(z_1)p(x_1|z_1) =  p(x_1)$이다.</li><li>$\pi A B = \sum_{i,j} \pi(i) B(i,j) A(j,:) = \sum_{z_1, z_2} p(z_1)p(z_2|z_1)p(x_2|z_2) = p(x_2)$이다.</li></ul><h2 id="Algorithms-of-HMM"><a href="#Algorithms-of-HMM" class="headerlink" title="Algorithms of HMM"></a>Algorithms of HMM</h2><p>HMM에서도 다른 확률 모델과 마찬가지로 forward propagation, backward propagation과정이 존재한다.</p><h3 id="Forward-Algorithms"><a href="#Forward-Algorithms" class="headerlink" title="Forward Algorithms"></a>Forward Algorithms</h3><p>HMM의 forward 알고리즘은 데이터셋의 확률, 즉, likelihood를 계산하는 알고리즘으로 대표된다. Markov model과는 달리, HMM의 likelihood는 곧바로 파라미터로 나타낼 수가 없어서 likelihood를 적절히 변형해야 한다. 그리고 단순히 변형해도, 그 계산의 time complexity가 매우 커서 계산 최적화를 위한 작업을 해 줘야 한다.</p><h3 id="Problem-1-Find-Likelihood-Distribution"><a href="#Problem-1-Find-Likelihood-Distribution" class="headerlink" title="Problem 1: Find Likelihood Distribution"></a>Problem 1: Find Likelihood Distribution</h3><p>파라미터 $\pi, A, B$를 바탕으로 likelihood를 계산할 수 있어야 한다. Likelihood가 있어야 ML 추정법을 적용할 수 있기 때문. Likelihood는 $p(x|\pi, A,B)$와 같으며, observation $x$의 joint distribution에 해당한다.</p><p>파라미터는 수식의 모든 term에 존재하므로, 생략한다. 먼저, likelihood는 다음과 같다. $T$는 전체 sequence 길이이다. 이 likelihood를 파라미터에 대한 식으로 바꿔주어야 한다.<br>$$<br>p(x) = p(x_1, x_2, …, x_T)<br>$$<br>이것을 hidden Markov model 구조(확률 그래프 모델이니까)에 따라 factorize하기 위해, hidden variable $z$를 삽입한다 (Marginalize).<br>$$<br>p(x) = \sum_{z_1} \sum_{z_2} \cdots \sum_{z_T}p(x_1, x_2, …, x_T, z_1, z_2, …, z_T)<br>$$<br>이제 factorize한다.<br>$$<br>p(x) = \sum_{z_1} \sum_{z_2} \cdots \sum_{z_T} p(z_1) p(x_1|z_1) \prod_{t=2}^{T} p(z_{t}|z_{t-1})p(x_t|z_t)<br>$$<br>이제 parameter에 대한 식으로 바꿀 수 있다.<br>$$<br>p(x) = \sum_{z_1} \sum_{z_2} \cdots \sum_{z_T} \pi(z_1) B(z_1, x_1) \prod_{t=2}^T A(z_{t-1}, z_t) B(z_t, x_t)<br>$$<br>그런데, 이 식의 time complexity를 봐야 한다. 위 식은 결국, 모든 hidden state 조합을 더하는 것이다. Hidden state의 개수는 $M$개이고, 이게 $T$-time 만큼 있으므로, $M^T$개의 hidden state조합이 존재한다. 또한, 하나의 hidden state 조합을 구하기 위해서는 $O(T)$시간이 걸리며, 총 $O(TM^T)$ 시간이 걸리게 된다. 이것은 exponential한 time으로, 매우 비효율적이다.</p><h3 id="Answer-to-Problem-1-Forward-Backward-Algorithm"><a href="#Answer-to-Problem-1-Forward-Backward-Algorithm" class="headerlink" title="Answer to Problem 1: Forward/Backward Algorithm"></a>Answer to Problem 1: Forward/Backward Algorithm</h3><p>그런데, 위 $p(x)$에는 겹치는 연산이 상당히 많다. 이것을 factorize해서(인수분해) 좀 더 효율적으로 $p(x)$를 계산할 수 있을 것 같다.</p><p>우선, $T=2, M=2$인 경우를 생각해본다.<br>$$<br>p(x) = \sum_{z_1} \sum_{z_2} \pi(z_1)B(z_1, x_1)\prod_{t=2}^T A(z_{t-1}, z_t)B(z_t, x_t) \</p><p>= \pi(1)B(1, x_1)A(1, 1)B(1, x_2) \</p><ul><li>\pi(1)B(1, x_1)A(1, 2)B(2, x_2) \</li><li>\pi(2)B(2, x_1)A(2, 1)B(1, x_2) \</li><li>\pi(2)B(2, x_1)A(2, 2)B(2, x_2) \<br>$$</li></ul><p>그런데, 중복된 연산이 너무 많다. 따라서, factorize를 해 주자.<br>$$<br>p(x) = \<br>\pi(1)B(1, x_1)[A(1, 1)B(1, x_2) + A(1, 2)B(2, x_2)] \<br>\pi(2)B(2, x_1)[A(2, 1)B(1, x_2) + A(2, 2)B(2, x_2)]<br>$$<br>$T=3, M=2$인 경우도 마찬가지로 할 수 있다. 수식으로 보면 다음처럼 표현할 수 있다.<br>$$<br>p(x) = \sum_{z_1} \sum_{z_2} \sum_{z_3} p(z_1) p(x_1|z_1) \prod_{t=2}^3 p(z_t|z_{t-1})p(x_t|z_t) \<br>= \sum_{z_1} \sum_{z_2} \sum_{z_3} p(z_1)p(x_1|z_1)p(z_2|z_1)p(x_2|z_2)p(z_3|z_2)p(x_3|z_3) \<br>= \sum_{z_3} p(x_3|z_3) \sum_{z_2} p(x_2|z_2)p(z_3|z_2) \sum_{z_1} p(z_1)p(x_1|z_1)p(z_2|z_1)<br>$$<br>위 식을 다음처럼 변형해본다.<br>$$<br>\sum_{z_3} p(x_3|z_3) \sum_{z_2} p(z_3|z_2) [p(x_2|z_2) \sum_{z_1} p(z_2|z_1)[p(x_1|z_1) p(z_1)]]<br>$$<br>여기서, $\alpha$라고 하는 놈을 정의한다.<br>$$<br>\alpha(3, z_3) = p(x_3|z_3) \sum_{z_2} p(z_3|z_2) \alpha(2, z_2) \<br>\alpha(2, z_2) = p(x_2|z_2) \sum_{z_1} p(z_2|z_1) \alpha(1, z_1) \<br>\alpha(1, z_1) = p(x_1|z_1)p(z_1)<br>$$<br>이때, $p(x)$는 다음처럼 된다.<br>$$<br>p(x) = \sum_{z_3}\alpha(3, z_3)<br>$$<br>즉, 다음처럼 일반화가 가능하다.<br>$$<br>p(x) = \sum_{z_T} \alpha(T, z_T) \<br>\alpha(t, z_t) = p(x_t|z_t) \sum_{z_{t-1}} p(z_t|z_{t-1}) \alpha(t-1, z_{t-1}) \<br>\alpha(1, z_1) = p(x_1|z_1)p(z_1)<br>$$<br>이렇게 되면, likelihood $p(x)$를 계산하는데, $O(MT)$면 끝이 난다.</p><h3 id="Problem-2-Find-the-Most-Likely-Sequence-of-Hidden-States"><a href="#Problem-2-Find-the-Most-Likely-Sequence-of-Hidden-States" class="headerlink" title="Problem 2: Find the Most Likely Sequence of Hidden States"></a>Problem 2: Find the Most Likely Sequence of Hidden States</h3><p>Likelihood를 구했다면, 이번엔 가장 probable한 hidden states의 sequence를 찾을 수 있어야 한다. 즉,<br>$$<br>z^* = \underset{z}{\text{argmax}} ~ p(z|x)<br>$$</p><p>를 만족하는 hidden states $$z$$의 joint distribution을 계산할 수 있어야 한다.</p><p>그런데, 이때, 위 식은 다음처럼 정리가 가능하다.<br>$$<br>z^* = \underset{z}{\text{argmax}} ~ p(z|x) = \underset{z}{\text{argmax}} ~ \frac{p(x,z)}{p(x)} = \underset{z}{\text{argmax}} ~ p(x, z)<br>$$<br>그런데, 여기서, $p(x, z)$는 $p(x)$를 구하는 식에서 marginalization만 빼면 된다. 즉,<br>$$<br>p(x, z) = p(z_1)p(x_1|z_1) \prod_{i=2}^T p(z_{t}|z_{t-1})p(x_t|z_t)<br>$$<br>이다. 하나의 joint probability를 계산하려면 $O(T)$시간이 걸리는 셈. 그러면, observations들에 맞게 가장 그럴듯한 hidden state들을 찾으려면, hidden state의 모든 조합을 저 식에 넣어보고 가장 큰 확률값을 주는 조합을 고르면 될 것이다. 그러나, 이 방법은 $O(TM^T)$가 걸린다.</p><h3 id="Answer-to-Problem-2-Viterbi-Algorithm"><a href="#Answer-to-Problem-2-Viterbi-Algorithm" class="headerlink" title="Answer to Problem 2: Viterbi Algorithm"></a>Answer to Problem 2: Viterbi Algorithm</h3><p>지금, $p(x, z)$가 가장 큰 $z$조합을 구해야 한다. HMM은 Markov model이기 때문에 $t-1$까지 최적의 $z$ sequence를 구해놨다면, $t$에서의 $z_t$는 greedy하게 선택하면 $t$까지의 $z$ sequence는 optimal이다. 즉, $t=1$에서, $p(z_1)p(x_1|z_1)$이 최대가 되는 $z_1$를 구하고, $t=2$에서, $p(z_2|z_1)p(x_2|z_2)$이 최대가 되는 $z_2$를 구하고 이런식으로 앞에서부터 greedy하게 선택해도 된다는 것이다.</p><h3 id="Problem-3-Training-HMM"><a href="#Problem-3-Training-HMM" class="headerlink" title="Problem 3: Training HMM"></a>Problem 3: Training HMM</h3><p>다음을 만족하는 parameter $$\pi, A, B$$를 계산한다.<br>$$<br>A^<em>, B^</em>, \pi^* = \underset{A,B,\pi}{\text{argmax}} ~ p(x|A,B,\pi)<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hidden-Markov-Models&quot;&gt;&lt;a href=&quot;#Hidden-Markov-Models&quot; class=&quot;headerlink&quot; title=&quot;Hidden Markov Models&quot;&gt;&lt;/a&gt;Hidden Markov Models&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Machine Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Machine-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="MachineLearning" scheme="https://jaeyoung-blog.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Hidden Markov Models 1</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Hidden_Markov_Models-1/</id>
    <published>2020-03-03T13:28:55.000Z</published>
    <updated>2020-03-03T01:57:54.961Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h1><p>Udemy 강좌: <a href="https://www.udemy.com/course/unsupervised-machine-learning-hidden-markov-models-in-python" target="_blank" rel="noopener">https://www.udemy.com/course/unsupervised-machine-learning-hidden-markov-models-in-python</a></p><h2 id="Markov-Assumption"><a href="#Markov-Assumption" class="headerlink" title="Markov Assumption"></a>Markov Assumption</h2><p>Markov property라고도 부르며, time-series 데이터나, 상태 기반 데이터에서, 현재의 상태는 오로지 바로 이전 상태만으로부터 영향을 받는다는 가정이다. 즉 다음과 같다.<br>$$<br>P(s_t|s_{t-1}s_{t-2}\cdots s_1) = P(s_t|s_{t-1})<br>$$<br>이전 상태들이 주어졌을 때, 현재 상태의 확률 분포는 오로지 바로 앞전 상태만으로부터 영향을 받는다. 즉, $s_{t-1}$이 주어진다면, $s_t$는 $s_{t-2},…,s_1$와 독립이다(Conditional independence).</p><p>Markov assumption은 상당히 강력한 가정으로, 많은 분야에 응용되지만(자연어와 같은 time-series, state machine 기반 모델 등), 바로 이전 상태를 제외한 그 이전 상태들을 모두 무시하므로, 성능에 한계가 있다.</p><p>보통 markov assumption하면 first-order markov assumption을 의미하며, 이전 몇 개의 데이터로부터 영향을 받게 할 것인가에 따라 second-order, third-order 등이 있다.</p><p>Second-order markov assumption은 다음과 같다.<br>$$<br>P(s_t|s_{t-1}, \cdots, s_1) = P(s_t|s_{t-1}, s_{t-2})<br>$$<br>Third-order markov assumption은 다음과 같다.<br>$$<br>P(s_t|s_{t-1},\cdots,s_{1}) = P(s_t|s_{t-1},s_{t-2},s_{t-3})<br>$$<br>그런데, 예상하다시피, 마르코프 가정으로 구현한 모델은 이전 모든 상태에 영향을 받게 모델링한 모델보다 성능이 떨어질 가능성이 높다. 그럼에도 불구하고 사용하는 이유는, 우리가 관심있는것은 지금까지 지나온 상태들의 joint distribution인데, 마르코프 가정이 없다면, joint distribution계산 과정이 매우 복잡해진다. 그래서, 쉽게 모델링하기 위해 마르코프 가정을 사용하며, 성능도 쓸만한 편이다.</p><h2 id="Markov-Models"><a href="#Markov-Models" class="headerlink" title="Markov Models"></a>Markov Models</h2><p>마르코프 가정(Markov assumption)을 바탕으로 모델링한 모델을 말한다. 다음과 같이 state machine도 마르코프 모델 중 하나이다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20191120062127996.png" alt="image-20191120062127996"></p><p>State machine은 일반적으로 다음 상태의 확률은 오직 현재 상태에 의해 영향을 받고 결정된다. 위와 같은 state machine에서의 transition probabilities는 행렬로 표현이 가능하며 이러한 행렬을 <strong>state transition probability matrix</strong>라고 부른다. 마르코프 모델에서는 현재에 기반한, 다음 상태 또는 다음 무언가의 확률 분포를 matrix로 표현이 가능하며, $M$개의 노드가 있을 때, transition probability matrix는 $M$x$M$행렬로 표현한다.</p><p>State transition probability matrix의 한 행 원소의 합은 1이어야 한다. $i$번째 행이 의미하는 것은 $s_i$를 기반으로 다음 state의 확률분포이기 때문이다.</p><h3 id="Starting-Position"><a href="#Starting-Position" class="headerlink" title="Starting Position"></a>Starting Position</h3><p>지금까지 지나온 상태들의 joint distribution을 계산해 보면 다음과 같다.<br>$$<br>P(s_t,s_{t-1},\cdots,s_1) = P(s_1) \prod_{i=2}^{t} P(s_i|s_{i-1})<br>$$<br>State transition probability matrix를 정의했다면, $P(s_i|s_{i-1})$는 알 수 있다. 그런데, 초기 상태인 $P(s_1)$는 행렬에 없다.</p><p>따라서, initial distribution을 정의해 주어야 하며, $1$x$M$ 벡터로 구성된다.</p><h3 id="Training-of-Markov-Models"><a href="#Training-of-Markov-Models" class="headerlink" title="Training of Markov Models"></a>Training of Markov Models</h3><p>마르코프 모델의 학습은 MLE(Maximum Likelihood Estimation)으로 이루어진다. 즉, $s_j \rightarrow s_i$로의 확률 분포는 데이터셋에서 $s_j$ 다음으로 $s_i$가 얼만큼의 비율로 등장하느냐에 따라 결정된다.</p><p>예를들어, 다음의 문장이 있다.</p><p>“I like cats”</p><p>“I like dogs”</p><p>“I love kangaroos”</p><p>그럼 상태의 집합은 ${\text{I}, \text{like}, \text{cats}, \text{dogs}, \text{love}, \text{kangaroos}}$이렇게 6개의 원소로 구성되어 있으며, initial distribution은 $P(\text{I})=1$이고, 나머지 단어의 경우, 0이다.</p><p>또한, $P(\text{like}|\text{I})=0.66, P(\text{love}|\text{I})=0.33, P(else|\text{I})=0$이다.</p><h3 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h3><p>그런데, 확률이 0이라는 것은 매우 위험하다. 반대로, 어떤 것의 확률이 1이라는 것 또한 매우 위험하다. MLE에 의해 트레이닝 데이터에 나오지 않은 것들은 모두 0이 되버리는데, 이는 오버피팅을 야기한다. 따라서 학습 데이터에 모든 경우의 수가 다 들어있기를 바래야 하는데, 이는 비현실적이다. 따라서 어떤 것에 1 또는 0의 확률을 할당하는 것을 피해야 하는데, 방법으로는 <strong>smoothing</strong>이라는 것이 있다.</p><p>smoothing이란, 0확률을 막아주는 기법을 의미하는데, 다음의 경우가 있다.</p><ul><li><p>No smoothing</p><p>기본적인, smoothing을 적용하지 않은 경우.<br>$$<br>P(s_i|s_j) = \frac{\text{count}(s_j \rightarrow s_i)}{\text{count}(s_j \rightarrow *)}<br>$$</p></li><li><p>Add-one smoothing</p><p>분자에 +1, 분모에 +$M$을 해 준다.<br>$$<br>P(s_i|s_j) = \frac{\text{count}(s_j \rightarrow s_i) + 1}{\text{count}(s_j \rightarrow *) + M}<br>$$<br>이때, $M$은 상태의 개수(자연어의 경우엔, 단어 개수)이다. 이러면, 모든 확률은 1 또는 0이 되지 않으며, $\sum_i P(s_i|s_j)=1$이 유지된다.</p></li><li><p>Add-epsilon smoothing</p><p>분자에 +1이 아니라, +$\epsilon$을 해 준다. 분모에는 +$\epsilon M$을 해 준다.<br>$$<br>P(s_i|s_j) = \frac{\text{count}(s_j \rightarrow s_i) + \epsilon}{\text{count}(s_j \rightarrow *) + \epsilon M}<br>$$<br>이때, $\epsilon$은 학습 파라미터로써, 추론해도 되고 hyper parameter로 해도 된다. Add-one 스무딩이 때로는 너무 강하거나 너무 약할때가 있다. 따라서, 스무딩의 강도를 조정하겠다는 이야기가 된다.</p></li></ul><h2 id="Markov-Chains"><a href="#Markov-Chains" class="headerlink" title="Markov Chains"></a>Markov Chains</h2><p>마르코프 모델이면서, 확률 과정(stochastic process)을 모델링한 것을 의미한다. 보통 통계에서샘플링이라 함은 샘플 하나를 얻는 과정을 말하지만, stochastic(random) process에서의 샘플링은 sequence of random variables을 얻는 과정이고, 하나의 샘플이 time series이다. 마르코프 체인 역시 stochastic process이며, 하나의 샘플은 time-series이다.</p><p>State transition probability distribution matrix를 $A$라고 하고, initial distribution을 $\pi$라고 했을 때, $t$번째 상태에서의 marginal distribution은 다음과 같다.<br>$$<br>P(s_t) = \pi A^{t}<br>$$<br>이때, $A$의 $i$번째 row는 $i$번 상태에서 다른 상태로 갈 확률분포이며, $A$는 $M$x$M$ 행렬이고, $\pi$는 1x$M$벡터이다. 따라서, 위 식은 1x$M$벡터가 나온다.</p><p>Marginal distribution에 대해 잠깐 설명해보면, 예를들어, 첫번째 상태 $s_1$의 확률분포는 다음과 같다.<br>$$<br>P(s_1) = \sum_{s_0} P(s_1,s_0) = \sum_j \pi_j A_{j,i} = \pi A<br>$$</p><h3 id="Stationary-Distribution"><a href="#Stationary-Distribution" class="headerlink" title="Stationary Distribution"></a>Stationary Distribution</h3><p>그런데, $A$를 반복해서 곱하다 보면(확률 과정을 반복), 어느 순간 marginal distribution의 변화가 다음과 같은 상태가 된다.<br>$$<br>P(s_t) = \pi A^t = P(s_{t-1}) = \pi A^{t-1}<br>$$<br>이때, $p(s)=p(s)A$를 만족한다. 이때, $p(s)$를 <strong>stationary distribution</strong>이라고 부른다. 이 stationary distribution $p(s)$을 보면, 행렬 $A$의 전치행렬인 $A^T$의 eigenvector($p(s)$는 벡터이다)와 같은 성질이다는 것을 알 수 있다. 다만, 그에 상응하는 eigen value는 1이다.</p><h3 id="Limiting-Distribution"><a href="#Limiting-Distribution" class="headerlink" title="Limiting Distribution"></a>Limiting Distribution</h3><p>그래서, 어떤 stochastic process의 최종 distribution은 무엇일까. 이 최종 distribution은 <strong>limiting distribution</strong> 또는 <strong>equilibrium distribution</strong>이라고 부른다. 즉, 다음과 같다.<br>$$<br>p(s_\infty) = \pi A^\infty<br>$$<br>그런데, 이건 stationary distribution과 같은가?</p><p>일단, <strong>limiting distribution은 stationary distribution이다. 하지만, 모든 stationary distribution이 다 limiting distribution이 되는 건 아니다.</strong> Eivenvector는 최대 $A$의 차원만큼 개수가 존재하며, 그중에서 eigen value가 1인 eigen vector는 여러개 일 수 있다. 이들 중 어느놈이 limiting distribution일까..</p><p>일단, limiting distribution이 구해지면, 그 stochastic process를 통해 앞으로 나올 time series를 샘플링할 수 있다(MCMC의 원리?).</p><h3 id="Perron-Frobenius-Theorem"><a href="#Perron-Frobenius-Theorem" class="headerlink" title="Perron-Frobenius Theorem"></a>Perron-Frobenius Theorem</h3><p>선형 대수학에서의 어떤 이론인데, stochastic process에 맞아떨어지는 이론이다.</p><p>어떤 행렬 $A = (a_{i,j})$에 대해, $A$는 $n$-by-$n$ matrix이고, 모든 원소가 양수이면, $A$의 가장 큰 양수 eigenvalue $r$이 존재하고 그와 상응하는 eigenvector의 모든 원소는 양수이다. 그리고, 모든 원소가 양수인 eigenvector는 이 eigenvector가 유일하며, 다른 eigenvector는 반드시 음수가 하나이상 포함되어 있다.</p><p>Stochastic process에서 다음 두 가지 조건을 만족시킨다면, 그 Markov chain은 반드시 유일한 stationary distribution을 가지며, 따라서, 해당 stationary distribution은 limiting distribution이라고 확신할 수 있다. Transition matrix $A$에 대해,</p><ul><li>$\sum_j a_{i,j} = 1$, 즉, 한 row의 모든 원소 합이 1이다. 하나의 row는 probability distribution이다.</li><li>$a_{i,j} \not = 0$, 어떠한 원소도 0이 아니다.</li></ul><p>여기서, transition matrix $A$의 eigenvector는 distribution으로써의 역할을 해야 하므로 모두 양수여야 하는데, 그런 조건을 만족하는 eigenvector는 오직 하나밖에 없으므로, 이놈이 limiting distribution이라고 확신할 수 있다.</p><h2 id="Application-of-Markov-Models"><a href="#Application-of-Markov-Models" class="headerlink" title="Application of Markov Models"></a>Application of Markov Models</h2><h3 id="Language-Models‌"><a href="#Language-Models‌" class="headerlink" title="Language Models‌"></a>Language Models‌</h3><p>Second-order language model을 예로 들자. 먼저, 문장의 첫 두 단어에 대한 initial distribution을 만들고 앞 두 단어가 주어졌을 때, 현재 단어에 대한 transition matrix를 만든다.‌</p><p>학습은 실제 문장들로 학습하며, 문장에서 앞 두 단어가 주어졌을 때, 현재 자리에 오는 단어의 비율을 transition matrix로 한다. 만약, 현재 단어가 끝 단어라면, 이 단어가 끝 단어일 확률 계산에 추가해준다.‌</p><p>앞 $$k$$개의 단어를 바탕으로 현재 단어를 추정하는 Markov model이다.</p><h3 id="Google’s-PageRank-Algorithms‌"><a href="#Google’s-PageRank-Algorithms‌" class="headerlink" title="Google’s PageRank Algorithms‌"></a>Google’s PageRank Algorithms‌</h3><p>Google의 페이지랭크 알고리즘은 각 페이지를 방문할 확률인 stationary distribution(정확히는 limiting distribution)이 높은 순서대로 랭크를 매기는 것을 말한다. 한 페이지에서 다른 페이지로 가는 링크가 있을 것이고, $A$페이지에서 $M$개의 링크가 있고, $B$페이지로 가는 링크가 존재한다면, $A→B$ 로의 transition probability는 $\frac{1}{M}$이 된다. 이렇게 transition matrix를 정의하고, matrix에서 0인 원소들을 smoothing을 이용해서 없앤 후, stationary distribution을 계산한다.</p><p>현재 페이지에서 다음 페이지로 갈 확률이 존재하는 Markov model이다.</p><h2 id="Hidden-Markov-Models-1"><a href="#Hidden-Markov-Models-1" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h2><p>마르코프 모델에서 hidden state상태를 추가한 형태. hidden state가 markov chain을 이루고 hidden unit에서 visible variable이 컨디셔닝 되어 나온다. 다음 그림은 markov chain과 hidden markov chain을 표현한 것인데, 노란색이 hidden unit들, 파란색이 visible unit을 표현한 것이다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20191201145914442.png" alt="image-20191201145914442"></p><p>Hidden markov model에서는 observable state $o_t$가 이전 observable state $o_{t-1}$에 영향을 받지 않는다. 대신 같은 시간의 hidden state인 $h_t$에 의해서만 영향을 받는다는 가정을 한다.</p><p>Markov model은 initial distributoin $$\pi$$와 transition probability matrix $A$가 존재하지만, hidden markov model에서는 initial distribution $$\pi$$와 hidden state transition matrix $A$, hidden state로부터 visible state로의 변환을 의미하는 transition matrix $B$가 존재한다.</p><h3 id="Application-of-HMM"><a href="#Application-of-HMM" class="headerlink" title="Application of HMM"></a>Application of HMM</h3><p>다음과 같은 application이 존재할 수 있다.</p><ul><li>Parts of Speech (POS) Tagging Systems</li><li>Stock Price Models</li></ul><h4 id="Parts-of-Speech-POS-Tagging-Systems"><a href="#Parts-of-Speech-POS-Tagging-Systems" class="headerlink" title="Parts of Speech (POS) Tagging Systems"></a>Parts of Speech (POS) Tagging Systems</h4><p>각 단어를 visible unit으로, 명사인지 동사인지, 형용사인지 등을 hidden state로 삼아서 HMM을 모델링하는 것을 말한다.</p><p>크게, 음성 시그널을 최외곽 visible variable, 단어를 hidden state로 삼아서 markov chain을 구성하는데, 이 애들이 다시 다른 HMM에 들어가는 방식이라고 생각하면 된다.</p><h4 id="Stock-Price-Models"><a href="#Stock-Price-Models" class="headerlink" title="Stock Price Models"></a>Stock Price Models</h4><p>HMM이 hidden time series($$z$$들)를 캐치할 수 있다는 것에 주목해서 stock price의 hidden factor를 HMM으로 캐치하게 한 모델을 말한다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20191201151728545.png" alt="image-20191201151728545"></p><p>이때, visible variable은 deterministic한 것이 아니라 generative하게 distribution으로 모델링할 수도 있다(위 그림처럼).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hidden-Markov-Models&quot;&gt;&lt;a href=&quot;#Hidden-Markov-Models&quot; class=&quot;headerlink&quot; title=&quot;Hidden Markov Models&quot;&gt;&lt;/a&gt;Hidden Markov Models&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Machine Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Machine-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="MachineLearning" scheme="https://jaeyoung-blog.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Distillation Methods</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Distillation-Methods/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Distillation-Methods/</id>
    <published>2020-03-03T13:07:01.000Z</published>
    <updated>2020-03-03T01:57:49.423Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Distillation-Methods"><a href="#Distillation-Methods" class="headerlink" title="Distillation Methods"></a>Distillation Methods</h1><p>다음을 참고했다.</p><p><a href="https://arxiv.org/pdf/1511.04508.pdf" target="_blank" rel="noopener">Distillation as a Defense to Adversarial<br>Perturbations against Deep Neural Networks</a></p><p><a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling knowledge in a Neural Network</a></p><h2 id="Distilling-Knowledge-in-a-Neural-Network"><a href="#Distilling-Knowledge-in-a-Neural-Network" class="headerlink" title="Distilling Knowledge in a Neural Network"></a>Distilling Knowledge in a Neural Network</h2><p>Distillation이란, [Distilling knowledge in a Neural Network]라는 논문에서 등장한 것으로 보이며, 이 논문에서는 다음과 같은 과정을 통해 <strong>네트워크의 지식</strong>을 다른 네트워크에게 전달해 줄 수 있다고 한다.</p><ol><li><p>먼저, 데이터 $(X, Y)$를 충분히 잘 학습할 수 있도록 큰 네트워크 $F$를 충분히 학습한다. 학습은 $F$의 정확도를 최대한 올리도록 진행한다.</p></li><li><p>작은 네트워크 $F_d$를 만들고, 같은 데이터셋 $(X, Y)$를 이용해서 그 네트워크를 학습하는데, 다음과 같은 과정을 거친다.</p><ol><li><p>데이터 $X$를 큰 네트워크 $F$에 통과시켜서 softmax에 들어가기 바로 전 값, 즉, logit $F(X)$를 얻는다. 큰 네트워크의 파라미터는 모두 고정시킨다.</p></li><li><p>데이터 $X$를 작은 네트워크 $F_d$에 통과시켜서 logit 값 $F_d(X)$를 얻는다.</p></li><li><p>$\sigma(F_d(X))$를 ground  truth인 $Y$와 가깝게 학습시키는 loss를 정의한다. $\sigma$는 softmax이다.<br>$$<br>L_{CE}(F_d(X), Y)<br>$$</p></li><li><p>또, 작은 네트워크가 예측한 결과는 큰 네트워크가 예측한 결과를 최대한 따라가도록 학습하도록 한다. 그에 맞는 loss를 정의한다.<br>$$<br>L_{CE}(\sigma(\frac{F_d(X)}{T}), \sigma(\frac{F(X)}{T}))<br>$$<br>이때, logit을 하이퍼파라미터 $T$로 나눠줌으로써, 조금 약하게 한다.</p><p>이것은, 작은 네트워크가 큰 네트워크의 데이터셋 $(X, Y)$를 학습한 결과를 최대한 따라가도록 만드는 효과가 있으며, 큰 네트워크의 지식을 작은 네트워크에게 전수한다고 볼 수 있다.</p></li></ol></li></ol><p>이러한 방법으로, 매우 유사한 성능을 내는 compact한 네트워크를 만들 수 있으며, 큰 네트워크 대신 작은 네트워크를 이용하면 computation complexity를 크게 줄일 수 있을 것이다.</p><h2 id="Generalization-using-Distillation"><a href="#Generalization-using-Distillation" class="headerlink" title="Generalization using Distillation"></a>Generalization using Distillation</h2><p>Distillation은 모델을 generalization하는 방법으로도 응용할 수 있다. 이 방법으로 상당한 adversarial attack 또한 방어가 가능하다(한때는 adversarial attack에 대한 state-of-the-art 기술이었다고 하는 듯 하다).</p><p>방법은 다음과 같다.</p><ol><li><p>똑같은 구조를 가지지만 weight를 공유하지 않는 두 네트워크 $F, F_d$를 생성한다.</p></li><li><p>먼저, 데이터셋 $(X,Y)$를 이용해서 $F$를 충분히 학습한다. 이후, $F$의 파라미터는 고정시킨다.</p></li><li><p>같은 데이터셋 $(X, Y)$를 이용해서 $F_d$를 다음과 같이 학습한다.</p><ol><li><p>데이터 $X$를 $F$에 통과시킨, softmax 결과 $F(X)$를 구한다.</p></li><li><p>데이터 $X$를 $F_d$에 통과시킨, softmax 결과 $F_d(X)$를 구한다.</p></li><li><p>$F(X)$과 $F_d(X)$를 가깝게 학습한다.<br>$$<br>\text{argmin} ~ KLD(F(X)||F_d(X))<br>$$<br>(KL-divergense말고 다른걸 써도 됨)</p></li></ol></li></ol><p>이 방식은, 첫 번째 네트워크 $F$를 학습할 때, one-hot label $Y$를 이용하지만, 두 번째 네트워크 $F_d$를 학습할 때는, one-hot label이 아니라 $F$의 softmax값을 사용하게 된다. One-hot label $Y$를 이용하게 되면, 해당 정답 라벨에 모델이 over-confident하게 된다. Softmax값을 이용하게 되면, 정답 라벨이 될 확률이 크게 학습되는것은 같다. 그러나, 덜 confident하게 되어 overfitting확률이 줄어든다. 이 방법으로 학습된 네트워크는 adversarial attack을 매우 효과적으로 막아냈으며,  generalization이 그 이유라고 분석되고 있는 듯 하다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Distillation-Methods&quot;&gt;&lt;a href=&quot;#Distillation-Methods&quot; class=&quot;headerlink&quot; title=&quot;Distillation Methods&quot;&gt;&lt;/a&gt;Distillation Methods&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Machine Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Machine-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="MachineLearning" scheme="https://jaeyoung-blog.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>06. Sample-based Reinforcement Learning</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/06-Sample-based-Reinforcement-Learning/</id>
    <published>2020-03-03T01:00:05.000Z</published>
    <updated>2020-03-03T04:25:37.222Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sample-based-Reinforcement-Learning"><a href="#Sample-based-Reinforcement-Learning" class="headerlink" title="Sample-based Reinforcement Learning"></a>Sample-based Reinforcement Learning</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p><h3 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h3><ul><li>지금까지 K-arm bandit problem으로 reinforcement learning의 기초를 보았고, 그를 이용해서 exploration-exploitation dilemma를 보았다.</li><li>또한, Exploration, exploitation이 충분히 이루어져서 environment dynamic에 해당하는, transition probability $p(s’,r|s,a)$가 이미 모델링되었다고 가정했을 때, MDP 환경에 한정해서 value function과 policy를 어떻게 정의하고 계산하는지 보았다.</li><li>Bellman equation을 이용해서 dynamic programming 방식으로 policy iteration, value iteration 등, optimal value function과 optimal policy를 계산하는 방법을 보았다.</li></ul><p>하지만, 여기서, 현실은 environment는 너무나도 복합적인 것이라, 모델링하기가 쉽지 않다. 그러나, 지금까지 해 왔던 방법들은 environment가 필수적으로 모델링되어 있어야 한다.</p><p>이렇게, environment를 모델링하는 데 있어서 어려움이 있다는 문제를 해결하는 방법 중 하나가 <strong>Sample-based Reinforcement Learning</strong>이다.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>이름에서 알 수 있다시피, 이 방법의 주요 특징은, environment dynamic에 해당하는 transition probability function $p(s’,r|s,a)$를 모델링하는 대신, sample-based로 추정해보겠다는 의미이다. 특히 <strong>Monte Carlo Estimation</strong>이 이용된다.</p><h2 id="Monte-Carlo-Estimation"><a href="#Monte-Carlo-Estimation" class="headerlink" title="Monte Carlo Estimation"></a>Monte Carlo Estimation</h2><p>Monte-Carlo Estimation이란, 파라미터의 기댓값을 계산하고자 할 때, 그 파라미터를 모델링하는 과정을 거치지 않고, 파라미터에 대한 샘플을 많이 얻어서 그 샘플들의 평균값을 기댓값으로 추정하는 방법을 의미한다.</p><p>샘플 개수가 많을수록, Central Limit Theorem에 의해, 샘플들의 평균은 실제 평균과 매우 가깝다고 확신할 수 있다.</p><p>여기서 직감할 수 있다시피, 정확한 기댓값을 추정하기 위해서는 상당히 많은 샘플이 필요하다.</p><h3 id="Implementation-Overview"><a href="#Implementation-Overview" class="headerlink" title="Implementation Overview"></a>Implementation Overview</h3><p>Episodic task로 예를 들려고 한다.</p><ol><li><p>일단 하나의 episode를 완주한다. 즉, 게임이 끝날 때 까지 일단 플레이를 한다. 지나온(또는 처해있었던) state, 취했던 action들, 받았던 reward들의 History는 기록해 둔다.</p></li><li><p>위 history를 $S_0, A_0, S_1, R_1, A_1, S_2, …, S_{T-1}, R_{T-1}, A_{t-1}, S_T, R_T$라는 sequence로 표현했을 떄, $T$에서 backward방향으로 reward 기댓값, 즉, value를 계산한다.</p><p>(Final state는 정의에 따라 value가 0이다)<br>$$<br>G_T = 0<br>$$</p><p>$$<br>G_{T-1} = R_{T} + \gamma G_T<br>$$</p><p>$$<br>G_{T-2} = R_{T-1} + \gamma G_{T-1}<br>$$</p><p>$$<br>\cdots<br>$$</p></li><li><p>$G_t$를 $t$일때의 state였던 놈, $S_t$에 대한 value의샘플이라고 간주한다. 하나의 episode에 그 state를 지난 횟수만큼 샘플이 생긴다</p></li><li><p>여러 episode를 플레이해본다.</p></li><li><p>모아진 value sample을 이용해서 state-value function을 추정한다(평균내기).</p></li></ol><p>하지만, 하나의 episode가 모든 state를 골고루 방문하지는 않으므로, 여러 episode를 시행해도, state마다 샘플 수는 다르다. 따라서 어떤 state는 state-value의 정확한 추정이 어려울 수 있다.</p><h3 id="Exploring-Starts"><a href="#Exploring-Starts" class="headerlink" title="Exploring Starts"></a>Exploring Starts</h3><p>Action value function도 state value function을 추정하는 방법과 똑같이 추정할 수 있다. Action value function을 굳이 쓰는 이유는 한 state에서 어떤 액션을 선택할지에 대한, 즉 policy를 찾는데 도움을 줄 수 있기 때문이다.</p><p>하지만, 만약, deterministic policy를 따르고 있으면, 각 state에서 특정 action만 수행한다. 따라서, 하나의 액션만 exploitation하게 되는데, 이러면, policy를 비교, 조정할 수 없다. 이에 대한 해결책 중 하나로, episode의 시작은 무조건 random state에서 random action을 취하도록 시작하는 것이다. 첫 번째 액션을 취한 이후로는 policy를 따르게 된다.</p><p>이 문제는 결국, exploration에 관한 문제이다.</p><h3 id="Monte-Carlo-Prediction"><a href="#Monte-Carlo-Prediction" class="headerlink" title="Monte Carlo Prediction"></a>Monte Carlo Prediction</h3><p>Reinforcement learning의 context에서, Monte Carlo prediction이란, 주어진 episode를 플레이한 history $S_0, A_0, R_1, S_1, A_1, …, R_T, S_T$를 이용해서 value function을 추정하는 것을 의미한다.</p><h2 id="Monte-Carlo-for-Policy-Control"><a href="#Monte-Carlo-for-Policy-Control" class="headerlink" title="Monte Carlo for Policy Control"></a>Monte Carlo for Policy Control</h2><p>Monte Carlo를 통해 generalized policy iteration을 구현할 수 있다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200125175225811.png" alt="image-20200125175225811"></p><ul><li>Policy Evaluation: 1번의 Episode 수행 및 Monte Carlo Estimation</li><li>Policy Improvement: 계산된 value function을 통한 policy의 greedify</li></ul><p>이전과 같이 policy evaluation과 improvement를 반복하게 되며, evaluation 단계에서 Monte Carlo prediction을 적용하여 value function을 추정하게 된다. Improvement 단계에서는 추정된 value function을 바탕으로 greedy한 policy를 생성해 낸다.</p><p>한 번의 반복(evaluation-improvement) 동안, 단 1번의 episode를 플레이하기 때문에 evaluation 단계에서 value function을 완전히 추정하지 않는다. Evaluation을 완성하려면 수많은 episode를 플레이하고 value function을 제대로 추정하고 improvement로 넘어가야 할 것이다. 하지만, 엄청 오래 걸릴 것이다.</p><p>Pseudo-code는 다음과 같다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">monte_carlo_gpi</span><span class="params">(states, actions, gamma)</span>:</span></span><br><span class="line">    pi = initialize_policies(states)</span><br><span class="line">    action_values = initialize_action_values(states, actions)</span><br><span class="line">    returns = initialize_rewards(states, actions)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        s0, a0 = exploring_starts(states, actions)</span><br><span class="line">        estimated_action_values, history, T = play_one_episode(pi)</span><br><span class="line">        G = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>, <span class="number">0</span>, by=<span class="number">-1</span>):</span><br><span class="line">            G = history[t+<span class="number">1</span>][<span class="string">"reward"</span>] + gamma * G</span><br><span class="line">            </span><br><span class="line">            state = history[t+<span class="number">1</span>][<span class="string">"state"</span>]</span><br><span class="line">            action = history[t+<span class="number">1</span>][<span class="string">"action"</span>]</span><br><span class="line">            </span><br><span class="line">            returns[state][action].append(G)</span><br><span class="line">            </span><br><span class="line">            action_values[state][action] = mean(returns[state][action])</span><br><span class="line">            pi[state] = argmax(action_values[state][action])</span><br></pre></td></tr></table></figure><h3 id="Epsilon-soft-Policy"><a href="#Epsilon-soft-Policy" class="headerlink" title="Epsilon-soft Policy"></a>Epsilon-soft Policy</h3><p>Exploring starts 방식은 deterministic policy 환경에서 출발점에서나마 랜덤으로 state와 action을 선택하게 함으로써, 모든 state들이 그래도 한번씩은 다 방문되도록 하게끔 하는 것이다. 하지만, 다음 문제점이 있다.</p><ul><li>State 개수가 너무 많을 경우, 첫 시작을 임의로 시작한다고 한들, 모든 state를 다 방문하기엔 역부족이다. 계산 불가능할 정도로 많은 시도횟수를 요구할 것이다.</li></ul><p>하지만, exploring 방법은 여전히 필요하며, exploitation만 할 수는 없다. exploring starts의 대안으로 나온 것이 바로 $\epsilon$-soft 방식이다. 이것은 $\epsilon$-greedy을 포함하는 상위 개념으로, optimal action에는 좀 높은 확률을 두고, 나머지 액션은 확률 0이 아니라 작은 확률을 설정해 두는 것이다.</p><p>구현 방법은 value evaluation에서 계산된 value function으로부터 가장 좋은 action을 뽑아내고, 그 액션에 좀 높은 확률을 주고, 나머지 액션은 작은 확률을 준다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200126105656485.png" alt="image-20200126105656485"></p><p>$$\epsilon$$-soft는 stochastic policy이다. 즉, optimal policy보다는 value 기댓값이 적다. 하지만, 적절히 greedy한 액션도 취해가면서 확률적으로 많은 state를 방문할 수 있게 해 준다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Sample-based-Reinforcement-Learning&quot;&gt;&lt;a href=&quot;#Sample-based-Reinforcement-Learning&quot; class=&quot;headerlink&quot; title=&quot;Sample-based Reinforce
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Reinforcement Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Reinforcement-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="ReinforcementLearning" scheme="https://jaeyoung-blog.github.io/tags/ReinforcementLearning/"/>
    
  </entry>
  
  <entry>
    <title>05. Policy Evaluation &amp; Control</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/05-Policy-Evaluation-vs-Control/</id>
    <published>2020-03-03T01:00:04.000Z</published>
    <updated>2020-03-03T04:25:35.573Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Policy-Evaluation-amp-Control"><a href="#Policy-Evaluation-amp-Control" class="headerlink" title="Policy Evaluation &amp; Control"></a>Policy Evaluation &amp; Control</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p><p>현재 가지고있는 policy가 좋은지 평가하고(evaluation) 더 좋은 policy로 향상시키는 작업(control)을 의미한다. 현재 가지고 있는 policy $\pi$와 dynamics of environment를 표현하는 $p(s’,r|s,a)$분포가 있으면, dynamic programming을 통해 value function을 계산해낼 수 있고, 그 value function을 이용해서 policy를 평가(evaluation)할 수 있다. 또한, dynamic programming을 통해 더 나은 policy를 찾을 수 있다(control).</p><p>Optimal policy를 찾기 위해서 policy evaluation과 control을 이용하게 된다.</p><h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p>어떤 policy가 좋은지 평가하는 방법은 value function를 보는 것이 있겠다. Policy evaluation이란, 주어진 policy에 대해 value function을 구하는 것을 말한다.</p><p>임의의 policy를 하나 설정하고, 각 액션에 대해 immediate reward를 설정한 후에는 value function을 계산할 수 있을 것이다.</p><p><strong>어찌됬든 요약하면, policy evaluation은 그 policy를 이용한 value function을 계산하는 것을 말한다.</strong></p><p>주어진 policy에 대해 value function을 정확히 계산하기보단, approximation 방법을 이용한다.</p><h3 id="Iterative-Policy-Evaluation"><a href="#Iterative-Policy-Evaluation" class="headerlink" title="Iterative Policy Evaluation"></a>Iterative Policy Evaluation</h3><p>주어진 policy를 이용하여 value function을 approximation하는 한 가지 방법으로, dynamic programming을 통한 iterative 방법이다. 처음에 모든 state의 value를 0으로(또는 임의의 아무 숫자) 초기화시킨 후, state-Bellman equation을 통해 모든 state의 value를 수렴할때까지 업데이트하게 된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterative_policy_evaluation</span><span class="params">(policy, p, states, threshold=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">    values_curr = np.zeros(states.shape)</span><br><span class="line">    max_diff = <span class="number">1e6</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        </span><br><span class="line">        values_curr = compute_state_bellman_equation(policy, p, values_curr)</span><br><span class="line">        max_diff = np.max(np.sqrt((values_next - values_curr)**<span class="number">2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> max_diff &lt; threshold:</span><br><span class="line">            <span class="keyword">return</span> values_curr</span><br></pre></td></tr></table></figure><p>일단 방법은 다음과 같다.</p><ol><li>두 개의 value matrix $V, V’$를 만든다. $V$는 현재 value function을 저장할 matrix, $V’$는 value function을 계산한 결과를 저장할 matrix이다.</li><li>State bellman equation $v_{\pi}(s)$을 통해 $V$만을 이용해서 $V’$를 계산한다.</li><li>$\epsilon = max|V - V’|^2$를 계산한다. 즉, 가장 value가 크게 변한 state를 찾는다.</li><li>어떤 작은 수 $\theta$에 대해 $\epsilon \leq \theta$이라면, value function이 충분히 수렴되었다고 간주하고 $V’$를 반환한다.</li><li>$\epsilon &gt; \theta$이라면, $V \leftarrow V’$로 대입하고 2번으로 돌아간다.</li></ol><h2 id="Policy-Control"><a href="#Policy-Control" class="headerlink" title="Policy Control"></a>Policy Control</h2><p>Policy control이란, 주어진 policy와 그것으로부터 만들어낸 value function을 가지고, optimal policy를 찾는 과정을 말한다.</p><h3 id="Policy-Improvement-Theorem"><a href="#Policy-Improvement-Theorem" class="headerlink" title="Policy Improvement Theorem"></a>Policy Improvement Theorem</h3><p>Action value를 비교하는데, 현재 상태에서 액션을 원래 policy $\pi$에 따라 선택한 후, policy $\pi$를 따르는 action value를 $q_{\pi}(s, \pi(s))$라고 하자. 또, 같은 상태에서 액션을 다른 policy $\pi’$에 따라 선택한 후, 원래 policy $\pi$를 따르는 action value를 $q_{\pi}(s, \pi’(s))$라고 하자.</p><p>$q_{\pi}(s, \pi’(s)) \geq q_{\pi}(s, \pi(s))$를 만족하면, 적어도 $\pi’$는 $\pi$보다는 좋다라는 이론이다. 만약, 두 action value가 같다면, 이미 optimal일 확률이 높다.</p><p>위 이론에 따라, 현재 policy보다 좀 더 좋은 policy를 찾는 방법은, 주어진 value function에 따라 확률적으로 action을 선택하던 현재 policy를 greedy한 deterministic한 policy로 바꾸는 것이다.<br>$$<br>\pi’ = \underset{a}{\text{argmax} } \sum_{s’,r} p(s’,r|s,a)[r + \gamma \cdot v_{\pi}(s’)] <del>~</del> \text{(for all state }s\text{)}<br>$$</p><h2 id="Policy-Iteration-Dynamic-Programming"><a href="#Policy-Iteration-Dynamic-Programming" class="headerlink" title="Policy Iteration - Dynamic Programming"></a>Policy Iteration - Dynamic Programming</h2><p><strong>Optimal policy를 찾는 알고리즘</strong>으로, 다음과 같은 과정으로 이루어진다. 일단 어떤 state에서 어떤 액션을 취하면 어떤 immediate reward를 받는지는 이미 알고 있다고 가정한다. Immediate reward의 분포 $p(r|s)$은 exploration &amp; exploitation 으로 추정해야 하거나 개발자가 이미 정해놓거나?</p><ol><li>Initialize $\pi_0$. 즉, 최초의 policy를 만들고 이를 현재의 policy $\pi$로 삼는다. 최초의 policy는 아무거나로 한다. 모든 액션을 uniform distribution에 따라 선택하는 policy로 해도 된다.</li><li>Evaluate $\pi$. 즉, 주어진 policy에 대해 value function을 계산한다.</li><li>Control $\pi$. 즉, 계산된 value function을 바탕으로 모든 상태에서 greedy한 action을 선택하는 새로운 policy $\pi’$를 만든다.</li><li>$\pi’ = \pi$라면, $\pi$를 반환하고 끝낸다. 아니라면, $\pi$에  $\pi’$를 대입하고 2번으로 간다.</li></ol><p>이 과정을 통틀어서 policy iteration 방법이라고 부른다. 다음은 전체 pseudo code.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200121183345364.png" alt="image-20200121183345364"></p><p>일단, 가장 처음 policy를 제외하고 이후 control에 의해 만들어진 모든 policy는 greedy하고 deterministic한 policy이다. 하지만, 이것은 새로 evaluate로 생성된 value function에서 greedy하지 않게 된다.</p><p>즉, evaluate과정을 거처서 만들어낸, 현 policy를 따르는 value function에서 현재 policy가 greedy하지 않게 되고,</p><p>control하는 과정을 거치면 greedy한 policy를 만들 수 있지만, 이건 또 다시 policy evalutation을 통해 더 좋은 policy가 있다는 것이 밝혀진다.</p><p>이 과정을 통해, 더 이상 좋은 policy가 없을 때 까지 수렴하게 된다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200121183703146.png" alt="image-20200121183703146"></p><h2 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h2><p>Policy Iteration의 일반적인 형태.</p><p>앞서 나온 policy iteration은 policy evaluation과 control를 번갈아가면서 수행했다. 그리고, evaluation에서는 value function이 수렴할때까지 loop를 돌렸고, 수렴한 다음에야 policy control을 시행했다. Policy control 또한 완전한 greedy한 deterministic policy를 선택했다. 하지만, generalized policy iteration은 다음처럼 작동한다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200122120728463.png" alt="image-20200122120728463"></p><p>Policy evaluation은 loop를 돌지 않고 한번만 회전하고 policy control또한 조금 완화된 greedy action을 선택하게끔 한다.</p><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><p>Generalized policy iteration의 한 방법으로, policy evaluation과 control를 번갈아서 수행하지 않고 evaluation을 policy와 관계없이 value값에 대해서만 수행해서 수렴시키고 control를 최종적으로 수행한다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200122121014540.png" alt="image-20200122121014540"></p><p>알고리즘은 위와 같은데, value funciton을 계산할 때, “어떤 state에서는 무조건 이 액션을 선택해”라고 말하는 policy를 넣지 않고, 그냥 value를 최대로 하는 액션을 선택하도록 한다. 그리고 그 최대 value로 업데이트한다. 이 과정을 반복하면 value function 혼자서 optimal에 수렴하게 되고 optimal value function을 이용해서 policy를 뽑아낸다.</p><h3 id="Asynchronous-Dynamic-Programming"><a href="#Asynchronous-Dynamic-Programming" class="headerlink" title="Asynchronous Dynamic Programming"></a>Asynchronous Dynamic Programming</h3><p>한번 value function을 업데이트할때, 모든 state를 순차적으로 다 돌지 말고, 필요한 state에 대한 value만, 순서관계없이 업데이트하자는 것이라고 한다.</p><p>또한, 모든 state를 다 업데이트하는것이 아니라 관계있는 state들만 업데이트한다.</p><h3 id="Monte-Carlo-Methods"><a href="#Monte-Carlo-Methods" class="headerlink" title="Monte Carlo Methods"></a>Monte Carlo Methods</h3><p>지금까지 dynamic programming을 통해 value function과 policy를 계산 및 추정했는데, dynamic programming을 통한 방법 외에도 여러가지 방법이 존재한다.</p><p>Monte carlo method는 하나의 state에 대해 각 액션을 많이 취해보고 Monte carlo estimation을 통해 value 추정값을 계산하자는 방법이다. 즉, 그 state에서 각각 액션을 많이 취해보고 얻은 reward들을 단순 평균내자는 이야기이다. 이 방법은 optimal policy를 매우 정확하게 찾을 것을 보장해준다.(단, action을 해서 reward를 한 trial이 많아야 한다.)</p><p>Monte carlo estimation의 단점은 모든 state에서 모든 액션을 많이 취해봐야 정확한 value function을 추정할 수 있는데, 그게 현실적으로 불가능하다.</p><h3 id="Brute-Force-Estimation"><a href="#Brute-Force-Estimation" class="headerlink" title="Brute-Force Estimation"></a>Brute-Force Estimation</h3><p>Brute-force 방법은 간단하다. 가능한 모든 deterministic policy 조합을 나열하고 그중에서 optimal policy를 찾는 것을 말한다. 이 방법 역시 optimal policy를 반드시 찾을 것을 보장해준다. 하지만, action수에 따라 가능한 policy 조합이 exponential하게 증가한다. 그래서 사실상 적용이 불가능하다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Policy-Evaluation-amp-Control&quot;&gt;&lt;a href=&quot;#Policy-Evaluation-amp-Control&quot; class=&quot;headerlink&quot; title=&quot;Policy Evaluation &amp;amp; Control&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Reinforcement Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Reinforcement-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="ReinforcementLearning" scheme="https://jaeyoung-blog.github.io/tags/ReinforcementLearning/"/>
    
  </entry>
  
  <entry>
    <title>04. Policies and Value Functions</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/04-Policies-and-Value-Functions/</id>
    <published>2020-03-03T01:00:03.000Z</published>
    <updated>2020-03-03T02:11:36.598Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Policies-and-Value-Functions"><a href="#Policies-and-Value-Functions" class="headerlink" title="Policies and Value Functions"></a>Policies and Value Functions</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p><p>RL에서의 학습이란, action을 결정하는 policy와 value function을 추정하는 과정이라고 볼 수 있을 만큼, 두 과정의 RL에의 영향은 절대적이다.</p><h3 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h3><p>우리말로, <strong>정책</strong>이라고 하며, 말 그대로, 주어진 현재 상황에서 agent가 어떤 action을 선택할지 결정해주는 정책을 의미한다.</p><p><strong>Policy는 각 state에서 가능한 action들의 probability distribution이다.</strong></p><p>Value function과 그 의미가 매우 유사해보인다. 하지만, Value function은 reward의 기댓값을 계산해주는 것일 뿐, action을 결정하는 것은 policy에게 달려 있다.</p><ul><li><p>Deterministic Policies<br>$$<br>\pi(s)= a<br>$$<br>위 식처럼, deterministic policy란 state를 입력으로 받아서 action 1개를 반환하는 함수이다. 즉, state를 action으로 매핑하는 함수라고 볼 수 있다.</p></li><li><p>Stochastic Policies<br>$$<br>\pi(a|s) = \text{ {Probability distribution matrix} }<br>$$<br>Stochastic policy는 한 state를 주게 되면, 그 state에서의 action probability distribution을 반환하는 함수이다. 따라서, 한 state를 주면, 여러 action이 나오게 된다.</p><p>이때, action probability distribution matrix는 다음을 만족해야 한다.<br>$$<br>\sum_a \pi(a|s) = 1<br>$$<br>$$<br>\pi(a|s) \geq 0<br>$$</p></li></ul><p>  즉, 행렬의 각 row가 적법한 probability distribution이어야 한다.</p><p>  action probability distribution은 각 state마다 다르다.</p><h3 id="Valid-amp-Invalid-Policies"><a href="#Valid-amp-Invalid-Policies" class="headerlink" title="Valid &amp; Invalid Policies"></a>Valid &amp; Invalid Policies</h3><p>MDP에서 policies는 반드시 현재 타임에서의 상태에 존재하는 정보를 이용해서 action을 결정해야 한다. 즉, policies의 인자는 반드시 time $t$에서는 $s_t$만이 되어야 하며, 따라서 다음과 같다.<br>$$<br>\pi(s_t) \rightarrow a_t<br>$$<br>만약, policy가 이전 time의 state나 action을 이용해서 action을 결정한다면, 적법한 policy라고 부르지 않는다.</p><h3 id="Value-Functions"><a href="#Value-Functions" class="headerlink" title="Value Functions"></a>Value Functions</h3><p>다음 두 가지로 나뉜다.</p><ul><li><p>State Value Function $v_{\pi}(s_t)$</p><p>한 상태 $s_t$에서, 앞으로 어떤 policy $$\pi$$를 따른다고 했을 때, <strong>앞으로 얻을 수 있는</strong> reward 기댓값을 의미한다.<br>$$<br>v_{\pi}(s_t) = \sum_{a_{t}} \pi(a_t|s_t) \sum_{s_{t+1},r_{t+1}} p(s_{t+1},r_{t+1}|s_{t},a_t)[r_{t+1} + \gamma \cdot v_{\pi}(s_{t+1})]<br>$$</p></li><li><p>Action Value Function $q_{\pi}(s_t, a_t)$</p><p>한 상태 $s_t$에서, 어떤 액션 $a_t$를 취하고 난 후, 앞으로 어떤 policy $\pi$를 따른다고 했을 때, <strong>앞으로 얻을 수 있는</strong> reward의 기댓값을 의미한다.<br>$$<br>q_{\pi}(s_t, a_t) = \sum_{s_{t+1}, r_{t+1}} p(s_{t+1}, r_{t+1}|s_t, a_t) [r_{t+1} + \gamma \cdot \sum_{a_{t+1}} \pi(a_{t+1}|s_{t+1})q_{\pi}(s_{t+1}, a_{t+1})]<br>$$</p></li></ul><p>둘 다 현재 어떤 상황에서 앞으로 얻을 수 있는 reward의 기댓값을 의미한다.</p><h2 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h2><p><strong>현재 시간 $t$에서의 value와 다음 시간 $t+1$에서의 value와의 관계식</strong>을 의미한다. State-value Bellman equation과 action-value Bellman equation이 존재하며, reinforcement learning 알고리즘 구현에 있어서 가장 중요한 알고리즘 중 하나이다.</p><h3 id="Bellman-Equation-vs-Value-Function"><a href="#Bellman-Equation-vs-Value-Function" class="headerlink" title="Bellman Equation vs Value Function"></a>Bellman Equation vs Value Function</h3><p>두 용어의 개념에 대한 차이는 거의 없다. Bellman equation도 value function이다. 다만,  $t$에서의 value function을 $t+1$에서의 value function에 대한 식으로 나타냈다 뿐. Recursive하게 표현한 value function을 Bellman equation이라고 부를 뿐이다.</p><h3 id="State-value-Bellman-Equation"><a href="#State-value-Bellman-Equation" class="headerlink" title="State-value Bellman Equation"></a>State-value Bellman Equation</h3><p>State-value function에 대한 Bellman equation으로, state-value function입장에서, $t$​에서의 state value와 $t+1$에서의 state value와의 관계식이다.<br>$$<br>v_{\pi}(s_t) = \sum_a \pi(a|s_t) \sum_{s_{t+1},r} p(s_{t+1}, r|s_{t}, a)[r + \gamma \cdot v_{\pi}(s_{t+1})]<br>$$</p><h3 id="Action-value-Bellman-Equation"><a href="#Action-value-Bellman-Equation" class="headerlink" title="Action-value Bellman Equation"></a>Action-value Bellman Equation</h3><p>Action-value function에 대한 Bellman equation으로, action-value funciton입장에서, $t$에서의 actionvalue와 $t+1$에서의 action value와의 관계식이다.<br>$$<br>q_{\pi}(s_{t}, a_{t}) = \sum_{s_{t+1}, r} p(s_{t+1}, r|s_t, a_t)[r + \gamma \cdot \sum_{a_{t+1}} \pi(a_{t+1}|s_{t+1}) \cdot q_{\pi}(s_{t+1}, a_{t+1})]<br>$$</p><h3 id="Compute-Value-using-Bellman-Equation"><a href="#Compute-Value-using-Bellman-Equation" class="headerlink" title="Compute Value using Bellman Equation"></a>Compute Value using Bellman Equation</h3><p>Bellman equation의 가장 큰 장점은, value function을 매우 효율적으로 계산할 수 있게 해 준다는 것이다. Value function은 정의에서 보다시피, 미래의 reward의 기댓값이다. 즉, 현재 시간 $t$이후의 모든 시간에서의 reward 기댓값인데, 이 정의로는 value를 계산할 수 없다. Bellman equation은 이 무한 수열 계산문제를 단순한 linear equation으로 바꿔준다.</p><p>다음 board를 생각해 보자.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200119160231033.png" alt="image-20200119160231033"></p><p>보드에는 $A,B,C,D$라는 4개의 공간이 있으며, 말 하나를 이 공간 내에서 움직이려 한다. 즉, 각 공간이 곧 state이며, 총 4개의 state가 있는 environment이다.</p><p>Action은 상,하,좌,우 4개의 움직임이 존재한다. Policy는 총 4개의 움직임에 대해 uniform distribution이다. 말이 $B$$로 들어오거나 B$에 머무는 움직임에 대해서만 reward +5를 부여하고 나머지는 0을 부여한다. Discount factor는 0.7로 하자.</p><p>State $A$에서의 value는 무한 수열식이지만, Bellman equation을 이용한다면, 다음 state의 value를 이용해서 계산이 가능하다.<br>$$<br>V(A) = \sum_{a} \pi(a|A) \sum_{s’,r} p(s’,r|a,A)[r + \gamma \cdot V(s’)]<br>$$<br>그런데, action이 정해지면, state는 확정(deterministic)이므로, 위 Bellman equation을 다음처럼 변경할 수 있다.<br>$$<br>V(A) = \sum_a \pi(a|A)[0 + 0.7 \cdot V(s’)] \<br>= \frac{1}{4} \cdot 0.7 \cdot V(C) + \frac{1}{2} \cdot 0.7 \cdot V(A) + \frac{1}{4} \cdot (5 + 0.7 \cdot V(B)))<br>$$<br>$V(B), V(C), V(D)$도 유사하게 $V(A), V(B), V(C), V(D)$에 대한 식으로 표현이 가능하며, 일차 연립방정식으롤 표현이 가능하다. 즉, 무한 수열을 푸는 문제가 일차 연립 방정식을 푸는 문제로 바뀐 것이다.</p><p>하지만, 현실에서는 approximation방법을 많이 이용한다. State개수가 많아서 그런가?</p><h2 id="Optimality"><a href="#Optimality" class="headerlink" title="Optimality"></a>Optimality</h2><p>Reinforcement learning의 목적은 단순히 value function과 policy를 계산하는게 아니라, optimal policy와 optimal value function, 즉, reward를 최대화하는 policy와 value function을 찾는 것이다.</p><h3 id="Optimal-Policies"><a href="#Optimal-Policies" class="headerlink" title="Optimal Policies"></a>Optimal Policies</h3><p>Optimal policy란, 모든 state에서 가장 높은 value를 반환하게 하는 policy를 말한다. 즉, 다음 그림처럼 어떤 여러개의 policies들보다 항상 큰 value를 반환하게 하는 policy는 항상 존재한다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200119163026023.png" alt="image-20200119163026023"></p><p>즉, $$\pi_1, \pi_2$$보다 항상 크거나 같은 value를 반환하는 policy는 항상 존재한다는 건데, 방법은 간단하다. $$\pi_1 \leq \pi_2$$인 state에서는 $$\pi_2$$의 policy를 따르고, $$\pi_1 &gt; \pi_2$$인 state에서는 policy $$\pi_1$$을 따르도록 하는 새로운 policy $$\pi_3$$를 만들면 된다.</p><p>이와 같은 방법으로, 언제나 모든 policy보다 크거나 같은 value를 반환하는 policy를 만들 수 있으며, 이런 policy는  unique할 수도 있고 여러개가 될 수도 있다(모든 state에 걸쳐서 똑같은 value를 반환하는 policy가 여러개일 수 있음).</p><p>어쨌든, 이런 과정을 거쳐서 가장 높은 value를 모든 state에 걸쳐서 반환하는 policy를 optimal policy라고 부른다. 방금 말했듯이, optimal policy는 반드시 존재하며, 여러개일 수 있다.</p><p>또 하나 생각할 점은, 위 그림에서 $\pi_3$은 분명히, $\pi_1, \pi_2$중 하나를 선택한 policy에 불과하므로, $\pi_1, \pi_2$둘 중 value가 높은 policy의 value와 같아야 할 것이데, 어느 지점에서는 $\pi_1, \pi_2$ 모두의 value보다 높다. 이것은, future value까지 반영해서 생기는 현상으로, 미래 state에서도 최선 policy인 $\pi_3$을 따르므로, value는 재료가 된 policy들보다 커질수도 있다.</p><h3 id="Optimal-Values"><a href="#Optimal-Values" class="headerlink" title="Optimal Values"></a>Optimal Values</h3><p>보통 optimal policy는 unknown으로, 바로 계산할 수 없다. 애초에 reinforcement learning의 목적은 optimal policy를 찾는 것이다. Optimal policy를 계산할때는 opimal value function를 이용하게 된다.</p><p>Optimal value function이란, 현재 state에서 가능한 모든 액션과 그에 다른 다음 value를 보고, 다음 value가 가장 높은 action을 deterministic하게 선택했을 때의 value function을 의미한다.<br>$$<br>v_<em>(s) = \underset{a}{\text{max}} ~ \sum_{s’,r} p(s’,r|s,a)[r + \gamma \cdot v_</em>(s’)]<br>$$<br>보다시피, action의 분포(policy)가 사라지고, 그냥 다음 state인 $s’$의 value $v_<em>(s’)$가 가장 높은 action을 무조건(deterministically) 취하게 한다. 또한, 이 value $v_</em>(s’)$만으로 $v_*(s)$를 계산하도록 한다.</p><p>앞서, value function은 두 가지가 있고, 두 가지 value function 모두 Bellman equation 형태로 바꿀 수 있었다. Optimal value function도 마찬가지이며, optimal한 value function을 Bellman equation형태로 바꾼 것을 Bellman optimality equation이라고 부른다.</p><ul><li><p><strong>Bellman optimality equation for state value function</strong><br>$$<br>v_<em>(s) = \underset{a}{\text{max}} ~ \sum_{s’,r} p(s’,r|s,a) [r + \gamma \cdot v_</em>(s’)]<br>$$</p></li><li><p><strong>Bellman optimality equation for action value function</strong><br>$$<br>q_<em>(s,a) = \underset{a}{\text{max}} \sum_{s’,r} p(s’,r|s,a)[r + \gamma \cdot \underset{a’}{\text{max}} ~ q_</em>(s’,a’)]<br>$$</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Policies-and-Value-Functions&quot;&gt;&lt;a href=&quot;#Policies-and-Value-Functions&quot; class=&quot;headerlink&quot; title=&quot;Policies and Value Functions&quot;&gt;&lt;/a&gt;Po
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Reinforcement Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Reinforcement-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="ReinforcementLearning" scheme="https://jaeyoung-blog.github.io/tags/ReinforcementLearning/"/>
    
  </entry>
  
  <entry>
    <title>03. Markov Decision Process</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/03-Markov-Decision-Process/</id>
    <published>2020-03-03T01:00:02.000Z</published>
    <updated>2020-03-05T03:24:41.945Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p><p>마르코드 결정 과정.</p><p>이름에서 유추할수 있다시피, Markov assumption에 기반한 decision process로, $t+1$에서의 상태 $s_{t+1}$는 오직 현재 $t$에서의 상태인 $s_{t}$에 있을 때, agent의 decision인 $a_{t}$에 의해서만 결정된다는 것이다.</p><h3 id="Markov-Decisoin-Process-vs-K-arm-Bandit-Problems"><a href="#Markov-Decisoin-Process-vs-K-arm-Bandit-Problems" class="headerlink" title="Markov Decisoin Process vs K-arm Bandit Problems"></a>Markov Decisoin Process vs K-arm Bandit Problems</h3><p>Markov decision process에서는 K-arm bandit에서 가정했던 여러가지 조건을 해제한 application에 적용이 가능하다.</p><ul><li>K-arm bandit problem에서는 항상 하나의 state만 존재했지만, Markov decision process는 액션을 취함에 따라 state가 변하는 application에도 적용이 가능하다.</li><li>K-arm bandit problem에는 언제나 선택할 수 있는 action 리스트가 고정되어 있었다. 하지만, Markov decision process가 적용되는 application은 그럴 필요가 없다. 각 state에 서로 다른 action list가 있을 수 있다.</li><li>K-arm bandit problem에는 state와 선택 가능한 액션 리스트가 하나임과 동시에 매 time마다 optimal action은 항상 고정되어 있었다. 하지만, 이번에 이야기할 reinforcement learning environment는 state마다 optimal action이 다를 수 있다.</li></ul><h3 id="Finite-Markov-Decision-Process"><a href="#Finite-Markov-Decision-Process" class="headerlink" title="Finite Markov Decision Process"></a>Finite Markov Decision Process</h3><p>Agent와 상호작용하는 environment에는 여러 state가 있을 수 있는데, 이 state의 개수가 finite 하며, 각 state에서 존재하는 action 개수도 finite한 경우에 적용되는 Markov decision process를 finite Markov decision process라고 한다. 물론 finite 하지 않는 경우가 매우 많다.</p><p>현재 상태를 $s$, 이 상태를 기준으로 내린 decision $a$, 그리고, 그 결정에 의해 변한 상태를 $s’$, 그로인해 받는 reward를 $r$라고 했을 때, Markov decision process의 <strong>state transition probability</strong>는 다음과 같다.<br>$$<br>p(s’,r|s,a)<br>$$</p><h3 id="Episodic-Tasks-vs-Continuous-Tasks"><a href="#Episodic-Tasks-vs-Continuous-Tasks" class="headerlink" title="Episodic Tasks vs Continuous Tasks"></a>Episodic Tasks vs Continuous Tasks</h3><ul><li><p><strong>Episodic Tasks</strong></p><p>바둑, 스타크래프트와 같은 게임처럼, “한번의 판(한 판), stage”이 존재하는 problem을 가리킨다. 따라서, terminal state 라는 것이 존재하며, 하나의 stage를 시작해서 끝난 후 최종 reward까지 받을 때 까지를 하나의 episode라고 부른다. Agent는 여러 episode를 체험해보면서 학습하게 된다. 한 episode에서 이런 선택을 했다면 다음 episode에서 다른 선택을 하면서 다른 결말 및 reward를 획득하면서 학습하게 되는 task이다.</p><p>Episode는 이전의 모든 episode와 독립적이다. 즉, 이전 episode가 어떻게 끝났던 간에, 현재 episode는 이전 episode에 의해 영향을 받지 않는다. 매 게임이 독립이라는 이야기이다.</p></li><li><p><strong>Continuous Tasks</strong></p><p>일반적인 로봇이 수행하는 작업들이 보통 continuous task이다. 이 경우, terminal state가 없으며, 그냥 life를 살아가면서 마주치는 state에서 action을 수행하면서 학습을 진행하게 된다.</p></li></ul><h2 id="Goals-of-MDP"><a href="#Goals-of-MDP" class="headerlink" title="Goals of MDP"></a>Goals of MDP</h2><p>MDP의 목적은 당장 action을 선택했을 때의 reward를 최대화 하는 것이 아닌, 현재 어떤 action을 선택한 후, 미래의 모든 reward들 합의 기댓값을 최대하하도록 하는 action을 선택하는 것이다. 즉, 다음과 같은 action $a_t^* $를 선택한다.<br>$$<br>a_t^* = \underset{a}{\text{argmax} } ~ \mathbb{E}[G_t] = \underset{a}{\text{argmax}} ~ \mathbb{E}[R_{t+1} + \cdots + R_T]<br>$$<br>이때, $T$는 final state에서의 time 이다. 즉, 한 episode의 끝일때의 time이다.</p><p>$G_t$는 random variable인데, $R_t$들이 random variable이고, random variable의 합이기 때문이다. 따라서, random variable $G_t$의 기댓값을 최대화하는 action $a$를 선택하도록 한다.</p><h3 id="Goals-of-MDP-for-Continuous-Tasks"><a href="#Goals-of-MDP-for-Continuous-Tasks" class="headerlink" title="Goals of MDP for Continuous Tasks"></a>Goals of MDP for Continuous Tasks</h3><p>위에서 소개한 action 선택법은 episodic task에만 적용이 가능하다. 미래의 모든 reward의 합의 기댓값이므로, terminal state가 존재해야 $\mathbb{E}[G_t]$가 finite($\infty$가 아님)하다. continuous task의 경우에는, $R_T$가 없고 무한히 더해지기 때문에, $\mathbb{E}[G_t] \approx \infty$가 된다. 따라서 <strong>discounting</strong>이라는 것을 통해 액션을 선택한다.<br>$$<br>a_t^* = \underset{a}{\text{argmax} } ~ G_t = \underset{a}{\text{argmax} } ~ [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots] = \underset{a}{\text{argmax} } ~ [R_{t+1} + \gamma G_{t+1} ]<br>$$<br>Discounting을 하는 이유는 $G_t$를 finite하게 만들기 위함이며, 다음과 같기 때문에 finite하다. 이때, $0 \leq \gamma &lt; 1$이어야 한다. $R_{max}$를 agent가 한 액션을 취했을때 얻을 수 있는 액션의 최대치라고 하자.<br>$$<br>G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \<br>G_t \leq R_{max} + \gamma R_{max} + \gamma^2 R_{max} + \cdots \<br>G_t \leq R_{max}(1 + \gamma + \gamma^2 + \cdots) \<br>G_t \leq R_{max} \cdot \frac{1}{1 - \gamma} <del>~</del> \text{iff } 0 \leq \gamma &lt; 1<br>$$<br>따라서, $0 \leq \gamma &lt; 1$을 만족하면, $G_t$는 $R_{max} \cdot \frac{1}{1 - \gamma}$보다 작다. 그리고, finite하다($\infty$가 아니다).</p><p>굳이 episodic task라고 해서 discounting을 사용하지 말라는 법은 없다. discount rate $$\gamma$$를 통해 미래 reward 지향적일지, 즉각적인 reward 지향적일지 정할 수있기 때문에 discounting 방법은 episodic task에서도 많이 이용된다.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>MDP란, 현재 상태만을 바탕으로 action을 취하고 reward를 얻는 환경에서의 reinforcement learning 방법 또는 decision process중 하나이다. 액션은 다음과 같이 취한다.<br>$$<br>a^* (t) = \underset{a(t)}{\text{argmax} } ~ \mathbb{E}[G_t] = \underset{a(t)}{\text{argmax} } ~ \mathbb{E}[R_{t+1} + \gamma \cdot G_{t+1} ]<br>$$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Markov-Decision-Process&quot;&gt;&lt;a href=&quot;#Markov-Decision-Process&quot; class=&quot;headerlink&quot; title=&quot;Markov Decision Process&quot;&gt;&lt;/a&gt;Markov Decision P
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Reinforcement Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Reinforcement-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="ReinforcementLearning" scheme="https://jaeyoung-blog.github.io/tags/ReinforcementLearning/"/>
    
  </entry>
  
  <entry>
    <title>02. K-arm Bandits Problems</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/02-K-arm-Bandits-Problems/</id>
    <published>2020-03-03T01:00:01.000Z</published>
    <updated>2020-03-05T03:26:24.606Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K-arm-Bandits-Problems"><a href="#K-arm-Bandits-Problems" class="headerlink" title="K-arm Bandits Problems"></a>K-arm Bandits Problems</h1><p>참고: Coursera Reinforcement Learning (Alberta Univ.)</p><p>Reinforcement learning에서 가장 기본적인 문제로, K-arm bandit problem을 들 수 있다. K개의 arm이 있고, agent는 그 arm 중 하나를 골라야 하며, 하나를 골랐다면, environment는 그 arm의 reward를 반환한다.</p><h3 id="Multi-armed-Bandits-Problem"><a href="#Multi-armed-Bandits-Problem" class="headerlink" title="Multi-armed Bandits Problem"></a>Multi-armed Bandits Problem</h3><p>슬롯 머신문제, 의사의 치료법 선택 문제 등을 통칭해서 multi-armed bandits 종류의 문제라고 부른다.</p><ul><li><p>슬롯 머신 문제</p><p>내가 어떤 슬롯 머신을 선택하면 분명히 해당 슬롯 머신의 stationary distribution에 의해 어떤 결과를 얻을 것이고 그 결과를 바탕으로 reward를 얻게 될 것이다. 하지만, stationary distribution은 인공지능이 알 수 없다. 하지만, 어떤 슬롯 머신을 많이 돌려봤다면 해당 머신에 대해서는 stationary distribution을 어느정도 추정할 수 있다. 추정한 놈들중 높은 reward를 주는 결과를 뱉는 머신을 선택할 수도 있고 새로운 머신을 돌려서 그 머신의 stationary distribution을 추정해 볼 수 있다. 그 머신이 지금까지 알고 있는 머신보다 더 좋은 결과를 줄 수도 있기 때문에 새로운 머신도 조사해봐야 한다.</p></li><li><p>의사의 치료법 선택 문제(이름은 신경쓰지 말자)</p><p>의사는 심각한 환자에게 신생 치료법을 제안할 수도 있고 최선이라고 알려진 방법을 선택할 수도 있다. 신생 치료법은 최선이라고 알려진 방법보다 나을수도, 쪽박일 수도 있다.</p></li></ul><p>이런 종류의 문제를 K-armed bandits 문제라고 부른다.</p><h3 id="K-arm-Bandit-vs-Reinforcement-Learning"><a href="#K-arm-Bandit-vs-Reinforcement-Learning" class="headerlink" title="K-arm Bandit vs Reinforcement Learning"></a>K-arm Bandit vs Reinforcement Learning</h3><p>K-arm bandit problem은 reinforcement learning에서 다음과 같은 제약조건을 걸어버린, 쉬운 문제에 속한다.</p><ul><li>오직 하나의 state만 존재한다. 즉, 어떤 arm을 선택하는 액션을 취한다고 해서 state가 변하지는 않는다.</li><li>취할 수 있는 action 목록은 항상 동일하다. 시간이 지난다고 해서 가능한 액션이 늘어난다거나 줄어들지 않는다.</li><li>항상 optimal action이 일정하다. 시간에 따라 optimal action이 변하지 않는다.</li><li>유한한 환경이다. 즉, 선택 가능한 액션 개수, state개수 등이 finite하다.</li></ul><p>일반적으로 reinforcement learning은 위 제약조건들이 없다.</p><h3 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h3><p>$A_t$: 어떤 시간 $t$에서 취한 액션</p><p>$R_t$: 어떤 시간 $t$에서 취한 액션으로 얻은 reward</p><p>$q_* (a)$: 어떤 임의 액션 $a$를 취해서 얻는 reward의 기댓값 즉,<br>$$<br>q_* (a) = \mathbb{E}(R_t|A_t=a)<br>$$<br>(당연히 $q_* (a)$는 인공지능이 알 수 없는 값이다. 이걸 알면 기댓값이 높은 길만 선택하면 된다.)</p><p>$Q_* (a)$: 인공지능이 exploitation, exploration을 바탕으로 추정해낸 분포로 계산한, 임의 액션 $a$를 취했을 때의 reward 기댓값. 인공지능은 적절한 exploration을 통해 $Q_* (a)$를 업데이트해서 $q_* (a)$와 가깝게 추정해야 한다.</p><h2 id="Exploitation-vs-Exploration"><a href="#Exploitation-vs-Exploration" class="headerlink" title="Exploitation vs Exploration"></a>Exploitation vs Exploration</h2><p>Value function을 말하기 앞서서, exploitation과 exploration은 서로 균형을 이뤄야 한다. 이 둘의 균형을 맞춰야 적절히 최대 reward를 찾아가는 결정을 하면서 새로운 길을 개척할 수 있다.</p><p>이것을 달성하는 방법으로 다음과 같은 것들이 있다.</p><ul><li><p>$\epsilon$-greedy methods (Epsilon-greedy)</p></li><li><p>Optimal Initial Values</p></li><li><p>UCB(Upper Confidence Bound) Methods</p></li><li><p>Bayesian/Thompson Sampling</p></li></ul><h3 id="Trade-off-Between-Exploration-and-Exploitation"><a href="#Trade-off-Between-Exploration-and-Exploitation" class="headerlink" title="Trade-off Between Exploration and Exploitation"></a>Trade-off Between Exploration and Exploitation</h3><p>일단, agent는 어떤 결정을 할때, exploration과 exploitation을 동시에 수행할 수 없다. 반드시 exploration을 할지, exploitation을 할지 선택하고 액션을 취해야 하는데, exploration을 하게 되면 당장 최선의 결과를 얻을 수 없으며, exploitation을 하게 되면 당장 최선의 결과를 얻을 수 있으나, 더 최상이 되는 다른 것을 찾아나설 수 없다.</p><h3 id="epsilon-greedy-Methods"><a href="#epsilon-greedy-Methods" class="headerlink" title="$\epsilon$-greedy Methods"></a>$\epsilon$-greedy Methods</h3><p>매번 action을 선택할 때, $\epsilon$의 확률로 exploration을 하고, $1-\epsilon$의 확률로 exploitation을 하게 구현한 방법이다. 이 방식의 단점은 충분히 수렴한 뒤에도 $\epsilon$의 확률로 exploration을 하게 된다는 것이다(하지만 non-stationary distribution인 환경에서는 이게 더 도움이 된다).</p><p>$\epsilon$이 크면 빠르게 optimal한 action 기댓값을 찾도록 exploration을 할 수 있지만, 지나치게 exploration을 많이 하고 수렴 이후에는 greedy action을 해야 하는데 exploration을 하고 있는 경우를 볼 수 있다.</p><p>그렇다고 $\epsilon$이 작으면 global optimal을 찾는 속도가 너무 느리다.</p><h3 id="Optimistic-Initial-Values"><a href="#Optimistic-Initial-Values" class="headerlink" title="Optimistic Initial Values"></a>Optimistic Initial Values</h3><p>$\epsilon$-greedy 방법에서, 각 액션의 초기 기댓값을 어떻게 정하느냐에 따라 알고리즘의 성능이 달라지기도 한다. 가장 기본적인 방법은 초기 기댓값을 0으로 두는 것으로, 이렇게 되면, $\epsilon$값을 0보다 크게 둬서 어느정도 exploration을 유도해야 한다.</p><p>하지만, 최대 reward를 알고, initial value값을 최대 reward보다 높은 값을 설정해 두면, 오직 greedy하게 선택하게 하는 것으로도 초기 iteration에서 적절히 exploration이 가능하다. 하지만, 어느정도 지나게 되면 exploration을 하지 않는다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20191230212326720.png" alt="image-20191230212326720"></p><p>초기 reward 기댓값을 높게 설정하면, 아무리 좋은 reward를 얻는 액션이라 하더라도, 기댓값이 감소하기에, 다른 액션의 기댓값이 높아지게 된다. 따라서 자연스럽게 exploration을 하게 된다. 하지만, 초창기 iteration이 거의 exploration이 차지하기 때문에 수렴이 다소 느릴 수 있으나, 그 다음부터는 매우 빠르게 optimal에 수렴한다.</p><p>optimal에 수렴한 이후부터는 exploitation만 수행하게 되기 때문에(이미 최적 expectation을 계산해서 낮은 기대치를 갖는 액션은 취하지 않는다.) 만약, optimal reward expectation이 변하는 환경, 즉, stationary distirbution이 변하는 환경에선 이 방법은 적합하지 않다. 시간이 지남에 따라 최적의 액션이 바뀌게 되면 이 방법이 소용이 없어진다.</p><p>다음을 만족하는 action을 선택한다.<br>$$<br>a^* = \underset{a}{\text{argmax}} ~ q^* (a)<br>$$<br>Initial value estimation이 높기 때문에 오직 greedy하게 액션을 선택한다.</p><h3 id="UCB-Upper-Confidence-Bound"><a href="#UCB-Upper-Confidence-Bound" class="headerlink" title="UCB (Upper Confidence Bound)"></a>UCB (Upper Confidence Bound)</h3><p>Confidence interval을 이용해서 액션을 선택하는 방법으로, 각 action의 value 기댓값을 추정한 후, 그 기댓값의 confidence interval를 계산한다. 그리고, 특정한 p-value에 대해 confidence interval의 upper bound를 구한 후, 가장 높은 upper bound를 가지는 action을 선택하는 방식이다. </p><p>이 방법의 장점은, 굳이 $\epsilon$의 확률을 정해놓고 exploration을 하게 하는 것이 아니라, exploration이 얼마 이루어지지 않아 불확실한 confidence boundary를 가지는 것을 알아서 선택하게 하고, 많이 exploration되어 확실하지만, 높은 기댓값으로 확실한 값을 선택하게 함으로써, exploration과 exploitation을 자동으로 조절해서 선택하게 한다.</p><p>시간이 지날수록 수렴하게 되면 exploration은 자동으로 줄어들게 된다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200105142655161.png" alt="image-20200105142655161"></p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200105142946483.png" alt="image-20200105142946483"></p><p>즉 ,다음을 만족하는 action을 선택한다.<br>$$<br>a^* = \underset{a}{\text{argmax} } [q^* (a) + c\sqrt{\frac{\text{ln} ~ N} {N_a} } ]<br>$$<br>이때, $\underset{a}{\text{argmax}} ~ q^* (a)$는 greedy 한 선택을 위한 term, 즉, exploitation을 위한 term이고, $a^* = \underset{a}{ \text{argmax} } ~ c \sqrt{ \frac{ \text{ln} N } {N_a} }$은 exploration을 위한 term이다. 즉, confidence interval인데, 다음 식을 이용해서 유도할 수 있다고 한다.<br>$$<br>P(|\bar{X} - \mu| \geq \epsilon) \leq 2e^{-2 \epsilon^2 N}<br>$$</p><h3 id="Bayesian-Thompson-Sampling"><a href="#Bayesian-Thompson-Sampling" class="headerlink" title="Bayesian/Thompson Sampling"></a>Bayesian/Thompson Sampling</h3><p>UCB와 매우 유사하지만, value 기댓값의 confidence interval의 upper bound가 가장 큰 액션을 선택하는 것이 아니라, value의 posterior를 구하고 거기서 샘플링 한 후, 가장 큰 샘플을 가지는 액션을 선택하게 된다.<br>$$<br>a^* = \underset{a}{\text{argmax}} ~ [\bar{x} \sim \text{Posterior}(q^*(a))]<br>$$<br>이것은 UCB와 마찬가지로 confidence/credible interval과 관련되어 있는데, value 샘플링을 할 때, 가장 높은 value가 나온 경우는 두 가지로 생각할 수 있다.</p><ul><li>그 action에 대한 value estimation이 매우 불확실한 경우. 즉, credible interval이 매우 넓은 경우.</li><li>그 action의 value estimation이 그냥 높은 경우. 즉, optimal인 경우.</li></ul><p>하는 방법은 다음과 같다.</p><ol><li>한 액션의 value를 파라미터로 생각한다.</li><li>Value의  likelihood를 모델링하고, conjugate prior를 설정한다.</li><li>Value의 posterior를 계산한다.</li><li>Posterior를 계산했으면, posterior를 이용해서 value하나를 샘플링한다. Conjugate prior가 아니라면? MCMC를 써서 수렴시킨 후에 나온 샘플 하나를 가저오면 되나?</li><li>이것을 각 액션에 대해서 반복하고 가장 높은 value 샘플을 가지는 액션을 취한다.</li></ol><h2 id="Action-value-Methods"><a href="#Action-value-Methods" class="headerlink" title="Action-value Methods"></a>Action-value Methods</h2><p>어떤 액션을 골라야 할 때, 지금 현재 가지고 있는 지식만으로 각 액션을 취했을 때의 얻어지는 기댓값을 각각 계산하고, 가장 높은 기댓값을 가지는 액션을 취하는 방식이다. 즉, value function을 “가장 큰 액션의 기댓값을 가지는 액션”이라고 정의하는 것이다.</p><p>액션에 따른 reward의 기댓값 $Q_* (a)$을 계산하는 방법은, 다음과 같이 할 수도 있고, 다른 방법을 사용할 수도 있다.<br>$$<br>Q_* (a) \approx \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{I}<em>{A_i=a} } {\sum</em>{i=1}^{t-1} \mathbb{I}_{A_i=a} }<br>$$<br>즉, 이때까지 $a$라는 액션을 취했을 때, 얻었던 reward들의 평균값으로 $a$의 reward 기댓값이라고 삼는 것이다.</p><p>그리고, 다음을 만족하는 액션 $A_t$를 선택한다.<br>$$<br>A_t = \text{argmax}_a ~ Q_t(a)<br>$$<br>Action-value methods는 greedy한 방식으로, $\epsilon$-greedy와 함께 사용해서 exploration과의 균형을 맞추려고 시도해 볼 수 있다.</p><h2 id="Associative-Search-Contextual-Bandits"><a href="#Associative-Search-Contextual-Bandits" class="headerlink" title="Associative Search (Contextual Bandits)"></a>Associative Search (Contextual Bandits)</h2><p>K-arm bandit problem은 매우 간단한 reinforcement learning 예제이다. 보통 흔히 이야기하는 reinforcement learning에 다음과 같은 제약조건을 걸면 K-arm bandit problem이 된다.</p><ul><li>단 한 가지의 situation만 존재한다.</li><li>그에 따라, 액션이 situation에 영향을 미치지 않는다.(액션을 취한다고 해서 다음 time의  situation이 다른 situation으로 바뀌지는 않는다.)</li></ul><p><strong>상황(situation 또는 state)</strong>이란, agent가 상호작용하는 environment의 한 객체라고 생각해도 되며, 상황이 바뀌면 reward를 샘플링하는 value function도 바뀐다. 따라서, 이 문제는 non-stationary problem이며, 각 상황들에 할당되어 있는 value function 역시 non-stationary일 수도 있다.</p><p>Associate search를 통한 reinforcement learning은 contextual bandit problem이라고도 부른다.</p><p>반대로 이야기하면, full reinforcement learning은 다음과 같은 점때문에 K-arm bandit problem이랑 다르다.</p><ul><li>Environment안에 여러개의 situation이 state로 존재한다.</li><li>각 state에는 서로 다른 value function이 있다. 즉, 어떤 state에서는 액션 $A_i$을 취하는게 optimal이지만, 다른 state에서는 액션 $A_j$를 취하는게 optimal이 되기도 한다. state마다 취할 수 있는 액션 집합이 다를 수도 있다.</li><li>하나의 state입장에서, value function은 non-stationary distribution이 될 수 있다.</li><li>Agent가 취하는 액션은 본인이 받는 reward에도 영향을 미치지만, 다음 time에서의 state에도 영향을 줄 수 있다. 즉, 액션이 state transition을 야기하기도 한다.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;K-arm-Bandits-Problems&quot;&gt;&lt;a href=&quot;#K-arm-Bandits-Problems&quot; class=&quot;headerlink&quot; title=&quot;K-arm Bandits Problems&quot;&gt;&lt;/a&gt;K-arm Bandits Proble
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Reinforcement Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Reinforcement-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="ReinforcementLearning" scheme="https://jaeyoung-blog.github.io/tags/ReinforcementLearning/"/>
    
  </entry>
  
  <entry>
    <title>01. Introduction</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/01_Introduction/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/reinforcement-learning/01_Introduction/</id>
    <published>2020-03-03T01:00:00.000Z</published>
    <updated>2020-03-03T04:27:46.414Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Reinforcement Learning이란, 무엇을 할지에 대해 학습하는 것이다. 다르게 말하면, 어떤 상황이 입력으로 들어가서 어떤 액션이 출력되는 함수를 학습하는 것이다.</p><p>Reinforcement learning에선, 두 가지 중요한 특징이 있는데, 다음과 같다.</p><ul><li>Trails &amp; erros search</li><li>Delayed rewards</li></ul><p>물론 그 전에 environment를 인지할 수 있는 센서가 있어서 상황을 바탕으로 위 두 가지 특징이 발현된다.</p><h2 id="Overview-of-Reinforcement-Learning"><a href="#Overview-of-Reinforcement-Learning" class="headerlink" title="Overview of Reinforcement Learning"></a>Overview of Reinforcement Learning</h2><p>Supervised learning과 unsupervised learning은 다음과 같은 특성을 지닌다.</p><ul><li>숫자로된 테이블 형태의 데이터가 존재한다. ($X: N × D$$, $$Y: N × 1$) 이 두가지 데이터를 모두 넣고 모델을 학습하게 된다.</li><li>Static한 학습만 가능하다. 데이터를 추가하려면, 기존 데이터도 모두 넣고 fine-tuning 해야 하는 경우가 많다. 따라서, online-learning에 매우 불리하다.</li><li>시간이라는 개념이 없다(Sequence라는 개념은 있어도…). 그저 $X$를 받으면 $Y$를 줄 뿐. </li><li>Supervised learning의 경우, $Y$$가 $$X$ 동시에 주어지기에, 즉각적인 피드백이 있다.</li></ul><p>반면, reinforcement learning은 다음과 같은 차이점이 있다.</p><ul><li>데이터가 매우 비정형적이다. 로봇의 경우, environment로부터 받은 카메라 정보와 여라가지 센서 정보나 environment representation 정보 등이 있을 수 있다.</li><li>Dynamic하게 학습한다. Reinforcement learning은 데이터를 모아서 데이터셋을 만들어서 학습하는 형태가 아니라, environment에서 action을 취한 결과 피드백을 얻고 학습하는 형태이다. 즉, 그 자체가 그냥 online learning이다.</li><li>어떤 액션을 취하면 즉각적인 피드백이 없을 수 있고, 게임이 끝날 때 까지 피드백을 얻지 못할 수도 있다. 따라서, 상대적으로 시간이라는 개념이 존재한다. 즉, 액션과 리워드가 동시에 주어지지 않고 중간에 일정 시간이 있을 수 있다.</li></ul><h3 id="Unusual-amp-Unexpected-Stretagy-in-RL"><a href="#Unusual-amp-Unexpected-Stretagy-in-RL" class="headerlink" title="Unusual &amp; Unexpected Stretagy in RL"></a>Unusual &amp; Unexpected Stretagy in RL</h3><p>Reinforcement learning은 최종 value를 최대화하면서 학습한다. 그리고, 최종 value를 가장 높게 하는 방법을 알아서 찾아나가는데, 이때, 그 방법이 소위 말해서 수단과 방법을 가리지 않는 방법일 수  있다. 또한, agent가 취하는 액션은 나중에 보면 최대 reward를 받는 방법이었다는 것이 드러나지만, 액션 하나하나를 보면 인간이 전혀 이해하지 못하는 방향의 액션일 수도 있다.</p><h3 id="Supervised-Learning-as-Reinforcement-Learning"><a href="#Supervised-Learning-as-Reinforcement-Learning" class="headerlink" title="Supervised Learning as Reinforcement Learning?"></a>Supervised Learning as Reinforcement Learning?</h3><p>액션을 취하고 리워드를 얻는다는 것은 어떻게 보면 supervised learning과 연관지을 수도 있을 것이다. Environment가 $X$가 되고 그에 적절한 optimal action이 $Y$가 되는 것이다.</p><p>하지만, supervised learning을 쓰지 않고 reinforcement learning을 쓰는 이유가 있다.</p><ul><li>계산 불가능할 정도로 많은 environment/state 경우의 수</li><li>Supervised learning은 $X$와 $Y$를 동시에 필요로 하지만, $Y$가 있긴 한데, $X$ 동시에 주지 못하는 경우가 있다. 이때는 supervised learning을 할 수 없다.</li></ul><h2 id="Exploitation-Exploration-Dilema"><a href="#Exploitation-Exploration-Dilema" class="headerlink" title="Exploitation-Exploration Dilema"></a>Exploitation-Exploration Dilema</h2><p>Reinforcement learning에서의 agent는 최대한 많은 reward를 얻으면서 문제를 해결해야 한다. 이미 알고 있는 문제 해결 방법중에서 가장 큰 reward를 얻을 수 있는 방법을 선택해서 문제를 해결하는 것이 합리적일 것이다(exploitation). 그러나, agent는 새로운 길을 탐색해 나가면서 더 나은 길을 찾을 필요가 있다(exploration). 하지만, exploration과정은 많은 비용이 들 수도 있고 탐험 결과가 좋은 reward를 주는 경로가 아닐 수도 있다. 그럼에도 exploration은 필요하다.</p><ul><li><p>Exploitation</p><p>이미 찾은 문제 해결 방법중에서 가장 나은 방법을 선택하는 것. 즉, 최대 reward를 찾아가는 것.</p></li><li><p>Exploration</p><p>새로운 길을 탐색하는 것. 많은 비용이 들지만, 새로운 길이 지금까지 가지고 있었던 해결 방법들 보다 더 나은 reward를 줄 수도 있다.</p></li></ul><p>Exploration은 많은 비용이 들기 때문에, 당장은 reward를 얻지 못할 수도 있다. Exploitation을 하면 당장은 많은 reward를 얻을 수 있다. 이를 exploitaiton-exploration dilema라고 부른다.</p><h2 id="Elements-of-Reinforcement-Learning"><a href="#Elements-of-Reinforcement-Learning" class="headerlink" title="Elements of Reinforcement Learning"></a>Elements of Reinforcement Learning</h2><p>크게 4가지로 나눌 수 있다.</p><ul><li><p>Policy</p><p>어떤 주어진 환경/상황에서 어떤 동작을 취해야 하는지에 대한 규칙 또는 정책, 또는 매핑 함수이다. RL에서 핵심 역할을 하며, 단순한 매핑 테이블일수도, 아주 복잡한 함수나 확률적인 모델일 수도 있다.</p></li><li><p>Reward signal</p><p>액션에 대한 결과적인 상황에 따라 agent가 어떤 reward를 받을지에 대한, 즉, 시스템의 목표를 어떻게 할 것인지에 대한 것이다.</p></li><li><p>Value</p><p>Reward는 액션마다 주어질 수 있는 것으로, 이것만 있으면 greedy하게 갈 수 있다. 이를 방지하기 위해 reward를 누적하고 시스템 전체의 reward를 바라볼 수 있게 하는 것이 value이다. 즉, 어떤 state $s_{i}$에 대한 value $value(s_i)$는 그 상태 이후, 미래의 상태들 $s_{i+1}, s_{i+2},…$로부터 얻을 수 있는 reward 기댓값이다. 즉, $\text{argmax} ~ value(s)$라 함은, 당장 greedy한 선택이 아니라, 미래에 총 reward가 높은 방향으로 액션을 선택할 수 있게 해 준다.</p><p>value-function은 당장 앞에 놓인 action에 대해 value를 계산해주는 함수?</p><p>Reinforcement learning의 주요 task는 이 value function을 추정하는 것이다. agent는 value function이 가장 높은 value를 리턴해주는 액션을 선택하면 되니까.</p></li><li><p>Model of environment</p><p>Agent가 상호작용하는 환경을 정의한 것.</p></li></ul><p>이외에, episode라는 것이 있다. episode란, agent가 게임을 시작하고 끝날 때 까지의 기간, 즉, 한 게임을 의미한다. 다만, 게임같이 “한 게임”이라는 개념이 존재하는 episodic task가 있는 반면(바둑, 스타크래프트), “한 게임”을 정의할 수 없는, continuous task도 존재한다(로봇은 수명이 다할 때 까지 끊임없이 환경과 통신함). 이 경우에는 episode가 없다.</p><p>Reinforcement learning은 궁극적으로 reward를 최대화하는 것이지만, agent는 어떤 상황에서 높은 reward를 고르는게 아니라 value가 높은 쪽을 골라야 한다.</p><p>이 value function을 정의하는 방법은 두 가지가 있을 수 있다.</p><ul><li><p>Tabular solution method</p><p>Value function은 deterministic하다. 보통 deterministic한 함수들은 입력과 출력 매핑을 테이블 형태로 표현가능하다. 따라서, deterministic한 방법을 tabular method라고 부른다.</p><ul><li>Markov Decision Process</li></ul></li><li><p>Approximate solution method</p><p>Value function은 확률적이다.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Reinforcement Learning이란, 무엇을 
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Reinforcement Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Reinforcement-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="ReinforcementLearning" scheme="https://jaeyoung-blog.github.io/tags/ReinforcementLearning/"/>
    
  </entry>
  
  <entry>
    <title>Principal Component Analysis</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Principal-Component-Analysis/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/machine-learning/Principal-Component-Analysis/</id>
    <published>2020-03-01T13:28:55.000Z</published>
    <updated>2020-03-01T14:03:13.495Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Principal-Component-Analysis-PCA"><a href="#Principal-Component-Analysis-PCA" class="headerlink" title="Principal Component Analysis (PCA)"></a>Principal Component Analysis (PCA)</h1><p>데이터는 signal과 noise로 구성되어 있다. PCA의 핵심 목표는 데이터로부터 signal만 분리해 내는 것이다. PCA는 데이터 feature space를 회전시켜서 signal에 가까운 축과 noise에 가까운 축을 찾아준다. 그리고 데이터를 signal 축에 사영시킴으로써, 다른 축을 날려버리는데, 이때 데이터 정보 손실이 발생할 수 있다. 그러나, 그걸 감수하는 뛰어난 이득이 있기 때문에 PCA는 매우 유용하다.</p><p>PCA의 주 활용 목적은 다음과 같이 크게 두 가지로 분류가 가능하다. (둘 다 dimensionality reduction이다)</p><ul><li><p><strong>Feature Transformation</strong></p><p>Feature 공간의 축을 변환시켜서 signal축과 noise축을 구분되게 한다. 이때, 주의할 점은, PCA는 feature 공간을 non-linear하게 변환하는게 아니라 회전만 시킨다. 정확히 말하면, 축들만 회전시켜서 signal축과 noise축을 찾겠다는 것이다.</p></li><li><p><strong>Data Whitening</strong></p><p>Data whitening이란, 각 feature의 scale을 맞춰 주는 것을 말한다. PCA로 공간을 회전시킨 후엔, 각 축이 독립적이다. 그리고, 각 축의 variance가 구해지므로, 각 축을 standard deviation으로 나눠주면 whitening이 가능하다.</p></li><li><p><strong>Visualization</strong></p><p>PCA를 통해 signal에 가까운 축과 noise에 가까운 축을 찾아냈다면, signal에 가까운 축이 있을 것이다. Visualization을 위해 가장 signal다운 축 2개만 선택하는 방법도 있는데, 이럴 경우, 데이터에 대한 정보 상당수를 잃어버리지만, 데이터의 정보 상당량을 유지한 체, visualization을 할 수 있다.</p></li></ul><p>다음은 2차원 데이터를 PCA를 통해 축 2개를 찾은 것을 보여준다. 파란색 방향의 축이 signal이 되고, 빨간색 방향의 축이 noise가 될 수 있다(이것은 상대적인 것으로, 절대적으로 어떤것이 noise라고는 판단할 수 없다).</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20191219132645587.png" alt="image-20191219132645587"></p><p>여기서 만약에, 모든 데이터포인트를 파란색 축으로 사영시킨다고 하자. 그러면, 빨간색 방향의 정보는 사라지고, 데이터를 파란색 축 1차원으로만 표현이 가능하다. 빨간색 방향이 사라졌으므로 어느 정도의 데이터 분포에 대한 정보 손실이 있지만, 파란색 축은 대다수 정보를 보전하고 있는 축이다. 이렇게 데이터의 dimension을 reduction할 수 있는데, 이건 머신러닝 알고리즘의 속도를 크게 향상시킬 수 있다.</p><p>이건 매우 중요한 요소이다. 데이터의 모든 정보를 다 머신러닝에 때려넣기보단, 가장 메이저한 정보만 줘도 성능 하락없이 빠른 속도로 비슷한 결과를 달성할 수 있다. 이건 머신러닝에서 매우 중요하게 여기는, latent vector를 찾는다는 방향성과 일치한다.</p><h2 id="PCA-Methods"><a href="#PCA-Methods" class="headerlink" title="PCA Methods"></a>PCA Methods</h2><p>그럼 signal 축은 무엇으로 정의해야 할지에 대해서 고민해야 한다. 정답을 말하면, signal 축은 데이터 분포가 가장 널리 퍼진 방향이다. 그래야, 사영 후에, 퍼진 정도의 정보를 최대한 보존할 수 있다. 즉, 데이터들을 이 축에 사영시키면, 다른 어느 축에 사영시키는 것보다 분산이 커야 한다.</p><p>즉,</p><ol><li>분산이 가장 큰 축에 해당하는 벡터를 찾고,</li><li>데이터포인트를 그 벡터에 정사영시킨다.</li></ol><p>참고로, 정보 이론을 잠깐 가져오면, 분산이 클수록 정보량이 많다. 즉, 정보량이 많은 축을 찾는 것이다.</p><h3 id="Data-Rotation"><a href="#Data-Rotation" class="headerlink" title="Data Rotation"></a>Data Rotation</h3><p>PCA는 데이터 feature space안에서 데이터간의 관계를 건들지 않는다. 그저 rotation만 시킴으로써, 축만 변화시킬 뿐이다. 변화시킨 축이 signal이라고 할 수 있는 축들이 있고, noise 라고 할 수 있는 축이 있어서, 축을 골라서 사영시킬 수 있을 뿐이다. 데이터 자체를 non-linear하게 변형시키지는 않는다.</p><h2 id="PCA-Derivation"><a href="#PCA-Derivation" class="headerlink" title="PCA Derivation"></a>PCA Derivation</h2><p>우리가 원하는 축의 <strong>단위 벡터를 $$e$$라고 하자.</strong> 기본적으로 transpose가 없으면 column vector로 간주한다. 데이터들을 $x$라고 한다.</p><p>먼저, 데이터를 0-centerize시킨다.<br>$$<br>x’<em>i = x_i - \mu<br>$$<br>그리고 난 후, 데이터 $x’$를 축 $e$에 사영시킨 결과는, $e^Tx’$ 또는 $x’^Te$가 될 것이다. 이 사영시킨 결과로 나온 데이터들의 분포의 분산이 최대화되도록 하는 벡터 $e$를 찾아야 한다. 또한, 여기서, $e$는 단위 벡터로 제한하고자 한다. 즉,<br>$$<br>\underset{e}{\text{max}} \frac{1}{N} \sum</em>{i=1}^N (e^Tx_i’)^2 =<br>\underset{e}{\text{max}} \frac{1}{N} \sum_{i=1}^N e^Tx_i’x_i’^Te \<br>\text{s.t} <del>~ e^Te = 1<br>$$<br>참고로, $e^Tx’ = x’^Te = \text{scalar value}$이므로, $(e^Tx’<em>i)^2$을 위 식처럼 표현이 가능하다. 여기서, $e$는 $\sum$에 영향을 받지 않는 변수이므로 밖으로 뺄 수 있다.<br>$$<br>\underset{e}{\text{max}} ~e^T (\frac{1}{N}\sum</em>{i=1}^N x’_i x_i’^T) e \<br>\text{s.t} ~</del> e^Te = 1<br>$$<br>그런데, $x’$는 column vector이므로, $()$안의 값은 $x’$의 covariance matrix와 같다는 것을 알 수 있다. 이를 $\Sigma$라고 하자.<br>$$<br>\underset{e}{\text{max}} ~ e^T \Sigma e \<br>\text{s.t} <del>~</del> e^Te = 1<br>$$<br>이를 Lagrangian multiplier를 이용해서 식을 변형한다.<br>$$<br>\underset{e}{\text{max}} ~ e^T \Sigma e - \lambda (e^Te - 1) = \underset{e}{\text{max}} ~F(e)<br>$$<br>이제 미분을 수행해서 그 결과가 0이 나오는 지점을 찾아야 한다.<br>$$<br>\frac{dF(e)}{de} = 2\Sigma e - 2\lambda e = 0 \<br>\Sigma e = \lambda e<br>$$<br>즉, 우리가 찾던 벡터 $e$는 0-centered 된 데이터 $x’$의 covariance matrix의 eigen vector라는 사실을 알 수 있다.</p><p>그런데, 그럼, $\Sigma$에겐 eigen vector가 많을 것인데, 어떤 eigen vector에다가 사영할 것인지 결정해야 할 것이다. 다음으로 다시 돌아가보면,<br>$$<br>\underset{e}{\text{max}} ~ e^T \Sigma e<br>$$<br>위 식은 벡터 $e$ 방향으로의 분산에 대한 식을 변형시킨 결과로 얻었던 것이었다. 그런데, $e$가 $\Sigma$의 eigen vector라는 사실을 알았으므로,<br>$$<br>\underset{e}{\text{max}} ~ e^T \Sigma e =  \underset{e}{\text{max}} ~ e^T \lambda e<br>$$<br>가 되고, $\lambda$는 eigen value, 즉 스칼라값이므로, 맨 앞으로 올 수 있다.<br>$$<br>\underset{e}{\text{max}} ~ \lambda e^Te<br>$$<br>근데, $e^Te = 1$이라고 이미 제약을 걸어놓았다. 즉,<br>$$<br>\underset{e}{\text{max}} ~ \lambda<br>$$<br>다시말해, eigen value가 가장 큰 eigen vector를 고르면, 분산이 가장 큰 축을 찾을 수 있다는 말이 된다.</p><p>이 축은 데이터의 첫번째 principal component라고 부르며, 데이터의 가장 많은 정보를 포함하는 축이다.</p><h2 id="Principal-Components-After-1st-Component"><a href="#Principal-Components-After-1st-Component" class="headerlink" title="Principal Components After 1st Component"></a>Principal Components After 1st Component</h2><p>그래서, 가장 정보량이 많은 방향을 구했다. 그런데, dimensionality reduction을 하려고 할 때, 1차원으로만 압축해버리면 잃어버리는 정보가 매우 많다. 그래서, 첫번째 principal component(PC)를 제외하고 다른 방향으로 가장 많은 정보를 포함하는 축을 찾으려고 한다. 즉, 두번째 principal component를 찾을 것이다.</p><p>그런데 다음의 제한 사항이 있어야 한다.</p><ul><li><p>PC들은 모두 서로 직교해야 한다.</p><p>직교한다는 의미는 각 축이 서로 캡쳐하지 못하는 순수한 정보만 캡쳐할 수 있다는 이야기이다. 첫번째 PC와 비슷한 방향 축을 고른다고 해서 그 축이 정보를 많이 포함할까? 그 축은 첫번째 PC와 정보가 매우 많이 겹칠 것이다.</p></li></ul><p>그래서, 첫번째 PC에 수직인 sub-space에서 다시 분산이 가장 큰 방향을 구하게 된다.</p><p>두 번째 PC를 $e_2$라고 했을 때, $e_1^Te_2 = 0, e_2^Te_2 = 1$이라는 제약 조건을 만족시키면서 다음 variance를 최대화시켜야 한다. ($X$는 zero-centered 시켰다고 가정)<br>$$<br>\text{Var}(e_2) = (Xe_2)^T(Xe_2) \<br>= e_2^T \Sigma e_2<br>$$<br>Lagrangian multiplier에 의해,<br>$$<br>\underset{e_2}{\text{argmax}} ~ e_2^T \Sigma e_2 - c_1(e_2^Te_2 - 1) - c_2(e_1^Te_2)<br>$$<br>이를 미분한 결과가 0이 나와야 하므로,<br>$$<br>2 \Sigma e_2 - 2c_1e_2 - c_2 e_1 = 0<br>$$<br>양변에 $e_1^T$를 곱하면,<br>$$<br>2 e_1^T \Sigma e_2 - 2 c_1 e_1^T e_2 - c_2 e_1^T e_1 = 0 \<br>2 e_1^T \Sigma e_2 - c_2 = 0<br>$$<br>그런데 이때, $\Sigma$는 eigen decomposition에 의해 다음과 같다. ($d$는 데이터의 차원이라고 하자)<br>$$<br>\Sigma = \begin{pmatrix}<br>v_1 &amp; v_2 &amp; \cdots v_d<br>\end{pmatrix}<br>\begin{pmatrix}<br>\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; \lambda_2 &amp; \cdots &amp; 0 \<br>\cdots \<br>0 &amp; 0 &amp; \cdots &amp; \lambda_d<br>\end{pmatrix}<br>\begin{pmatrix}<br>v_1^T \<br>v_2^T \<br>\cdots \<br>v_d^T<br>\end{pmatrix}<br>$$<br>따라서 위 식을 다음처럼 고칠 수 있다.<br>$$<br>2 e_1^T \begin{pmatrix}<br>v_1 &amp; v_2 &amp; \cdots v_d<br>\end{pmatrix}<br>\begin{pmatrix}<br>\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; \lambda_2 &amp; \cdots &amp; 0 \<br>\cdots \<br>0 &amp; 0 &amp; \cdots &amp; \lambda_d<br>\end{pmatrix}<br>\begin{pmatrix}<br>v_1^T \<br>v_2^T \<br>\cdots \<br>v_d^T<br>\end{pmatrix} e_2 - c_2 = 0 \</p><p>2 \begin{pmatrix}<br>1 &amp; 0 &amp; \cdots 0<br>\end{pmatrix}<br>\begin{pmatrix}<br>\lambda_1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; \lambda_2 &amp; \cdots &amp; 0 \<br>\cdots \<br>0 &amp; 0 &amp; \cdots &amp; \lambda_d<br>\end{pmatrix}<br>\begin{pmatrix}<br>v_1^T \<br>v_2^T \<br>\cdots \<br>v_d^T<br>\end{pmatrix} e_2 - c_2 = 0 \</p><p>\begin{pmatrix}<br>2 \lambda_1 &amp; 0 &amp; \cdots 0<br>\end{pmatrix}<br>\begin{pmatrix}<br>v_1^T \<br>v_2^T \<br>\cdots \<br>v_d^T<br>\end{pmatrix} e_2 - c_2 = 0 \</p><p>2\lambda_1 v_1^T e_2 - c_2 = 0<br>$$<br>근데, $v_1$는 첫 번째 eigen vector로, $e_1$과 같다.<br>$$<br>2 \lambda_1 e_1^T e_2 - c_2 = 0 \<br>c_2 = 0<br>$$<br>다시 원래 최대화 식으로 돌아가보자.<br>$$<br>\underset{e_2}{\text{argmax}} ~ e_2^T \Sigma e_2 - c_1(e_2^Te_2 - 1) - c_2(e_1^Te_2)<br>$$<br>이건 다음처럼 변경된다.<br>$$<br>\underset{e_2}{\text{argmax}} ~ e_2^T \Sigma e_2 - c_1(e_2^Te_2 - 1)<br>$$<br>얘네를 미분하는 것? 첫 번째 PC를 구할때 지나왔던 길과 같다. 따라서, 다음처럼 유도될 것이다.<br>$$<br>\Sigma e_2 = \lambda_2 e_2<br>$$<br>즉, $e_2$ 역시, eigen vector이며, 이 $e_2$방향으로도 $e_1$ 방향을 제외하고 분산이 가장 커야 한다. 다음의 식에 의해 $e_2$방향의 분산은 eigen value $\lambda_2$와 같다.<br>$$<br>\text{Var}(e_2) = e_2^T \Sigma e_2 = e_2^T \lambda_2 e_2 = \lambda_2<br>$$<br>따라서, $e_1$방향의 분산인 $\lambda_1$보다 작으면서 가장 큰 eigen value, 즉, 두번째로 큰 eigen value가 두 번째 PC의 분산값이 된다. 즉, 두번째로 큰 eigen value에 해당하는 eigen vector가 두 번째 PC축이다.</p><p>(사실, 두 번째 PC를 다음처럼 추측도 가능하다. 왜냐하면, covariate matrix는 symmetric matrix이다. 그런데, symmetric matrix의 eigen vector들은 모두 서로 직교한다. 따라서, 첫 번째 PC가 eigen vector임이 밝혀진 상황에서, 두 번째 PC는 첫 번째 PC와 직교하므로, 두 번째 PC 역시 eigen vector라는 사실을 알 수 있다.)</p><h3 id="Covariance-between-PCs"><a href="#Covariance-between-PCs" class="headerlink" title="Covariance between PCs"></a>Covariance between PCs</h3><p>데이터를 각 PCs(covariate matarix의 eigen vector들)에 사영시켰을 때, 사영된 데이터들간 covariance 또는 correlation은 0이다.</p><p>편의를 위해 2차원이라고 생각해보자. 2차원에 분포된 데이터들 $x_i$가 있고, 그들의 principal component를 $e_1, e_2$라고 했을 때, $x_1$을 $e_1$에 사영시킨 것은 $(x_i^T e_1)e_1$이고, $e_2$에 사영시킨 것은 $(x_i^T e_2)e_2$이다. 따라서 이들의 covariance가 0이라는 것은 다음을 만족한다는 의미이다.<br>$$<br>((x_i^Te_1)e_1)^T \cdot (x_i^T e_2)e_2 = 0<br>$$<br>이때, $()$안에 있는 term들은 모두 scalar값이므로(내적이니까),<br>$$<br>(x_i^T e_1)e_1^T \cdot (x_i^T e_2) e_2 = 0 \<br>(x_i^T e_1)(x_i^T e_2)e_1^Te_2 = 0 \<br>(x_i^T e_1)(x_i^T e_2)0 = 0 \<br>0 = 0<br>$$<br>Symmetric matrix의 eigen vector끼리는 orthogonal하므로 $e_1^Te_2 = 0$이다.</p><p>따라서, 등식이 성립하고, 데이터 포인트에 대해 PC축들은 서로 covariance가 0이다. 그러니까, covariance matrix $\text{Cov}(i,j)$에서, $\text{Cov}(i,j) = 0 ~ \text{if} ~ i \not = j$라는 의미.</p><h2 id="Implementation-of-PCA-Dimensionality-Reduction"><a href="#Implementation-of-PCA-Dimensionality-Reduction" class="headerlink" title="Implementation of PCA (Dimensionality Reduction)"></a>Implementation of PCA (Dimensionality Reduction)</h2><p>구현에서는 데이터 행렬 $X$의 각 데이터 포인트는 row-vector임을 명시한다. 나머지는 모두 column-vector이다.</p><p>PCA의 구현은 다음과 같이 요약이 가능하다.</p><ol><li><p>데이터를 0-centered 한다. $Z = X - \mu_X$</p></li><li><p>$Z$의 covariate matrix를 계산한다. $\Sigma = Z^TZ$ 또는, <figure class="highlight plain"><figcaption><span>바로 계산</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3. $\Sigma$의 eigen decomposition을 계산한다. $V\Lambda V^T$, ```np.linalg.eig(cov_mat)```로 계산</span><br><span class="line"></span><br><span class="line">4. Eigen value를 내림차순으로 정렬한다. 당연히 eigen vector들도 동반 정렬되어야 한다.</span><br><span class="line"></span><br><span class="line">   ```python</span><br><span class="line">   evalues, evectors  = np.linalg.eig(cov_mat)</span><br><span class="line">   lst = sorted(zip(evalues, evectors), key=lambda item: item[0], reverse=True)</span><br><span class="line">   # result:</span><br><span class="line">   # [(e1, v1), (e2, v2), ...]</span><br></pre></td></tr></table></figure></p></li><li><p>최상위 $k$개의 eigen vector를 뽑아낸다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">evectors = list(map(<span class="keyword">lambda</span> item: item[<span class="number">1</span>], lst))[:k]</span><br><span class="line">E = np.hstack(evectors)</span><br></pre></td></tr></table></figure></li><li><p>데이터 포인트를 각 eigen vector에 사영시킨다. $\text{inner prod}(x_i, e_j)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_reduced = np.matmul(X, E)</span><br></pre></td></tr></table></figure></li></ol><h2 id="Implementation-of-Data-Whitening"><a href="#Implementation-of-Data-Whitening" class="headerlink" title="Implementation of Data Whitening"></a>Implementation of Data Whitening</h2><p>데이터 whitening이란, 데이터의 분포를 타원형에서 원형으로 re-scaling해주는 것을 말한다. 각 feature들의 scale을 일치시킨다. PCA를 이용하면 이를 수행할 수 있다. 단, feature는 eigen vector로 변환된 상태로 whitening이 이루어진다.</p><p>방법은, PCA로 데이터를 rotation시킨 후(basis가 eigen vector들이 된다), 각 eigen vector축 방향을 그 방향의 standard deviation으로 나눠준다. PCA로 변환된(rotation된) 데이터들을 $Z$라고 했을 때,<br>$$<br>\begin{pmatrix}<br>1 \over \sqrt{\lambda_1} &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; 1 \over \sqrt{\lambda_2} &amp; \cdots &amp; 0 \<br>\cdots \<br>0 &amp; 0 &amp; \cdots &amp; 1 \over \sqrt{\lambda_d}<br>\end{pmatrix} Z<br>$$<br>왜냐하면, 각 축 방향의 variance는 eigen vector들이기 때문.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200221092153746.png" alt="image-20200221092153746"></p><p>구현은 다음과 같다.</p><ol><li><p>데이터를 0-centered 한다. $Z = X - \mu_X$</p></li><li><p>$Z$의 covariate matrix를 계산한다. $\Sigma = Z^TZ$ 또는, <figure class="highlight plain"><figcaption><span>바로 계산</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3. $\Sigma$의 eigen decomposition을 계산한다. $V\Lambda V^T$, ```np.linalg.eig(cov_mat)```로 계산</span><br><span class="line"></span><br><span class="line">4. 데이터 $X$를 eigen vector에 사영시킨다. (정렬은 해도되고 안해도되는데, dimensionality reduction까지 하려면 정렬하고 $k$개만 뽑고 거기에 사영시킨다)</span><br><span class="line"></span><br><span class="line">   ```python</span><br><span class="line">   evalues, evectors = np.linalg.eig(cov_mat)</span><br><span class="line">   E = np.hstack(evectors)</span><br><span class="line">   X_transformed = np.matmul(X, E)</span><br></pre></td></tr></table></figure></p></li><li><p>얘네에다가 $\Lambda^{-\frac{1}{2}}$를 곱해준다. (Dimensionality reduction했으면 $k$개만 있는 diagonal matrix이다)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LAMBDA = np.diag(evalues)</span><br><span class="line">X_whitened = np.matmul(LAMBDA, X_transformed)</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Principal-Component-Analysis-PCA&quot;&gt;&lt;a href=&quot;#Principal-Component-Analysis-PCA&quot; class=&quot;headerlink&quot; title=&quot;Principal Component Analysis
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Machine Learning" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Machine-Learning/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="MachineLearning" scheme="https://jaeyoung-blog.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Appendix 2. Empirical Bayes</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/APPENDIX-2-Empirical-Bayes/</id>
    <published>2020-03-01T13:08:16.000Z</published>
    <updated>2020-03-03T01:57:15.622Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Empirical-Bayes"><a href="#Empirical-Bayes" class="headerlink" title="Empirical Bayes"></a>Empirical Bayes</h1><p><strong>하이퍼파라미터 또한 데이터로 구하자.</strong></p><p>베이지안 모델링을 하게 되면, likelihood를 모델링하고, 그 파라미터를 prior로 모델링을 하게 된다.<br>$$<br>D \sim \text{Lieklihood}(\theta) <del>~</del> [P(D|\theta)]\<br>\theta \sim \text{prior}(\lambda)<br>$$<br>그런데, 이때, prior의 파라미터(위에서는 $\lambda$)는 하이퍼파라미터로, 사용자가 직접 constant로 세팅해 주게 된다. Empirical bayes는 이 하이퍼파라미터를 사용자 대신, 데이터를 이용해서 MAP으로 추론하는 것을 말한다(point estimation).</p><p>즉,<br>$$<br>\hat{\lambda} = \underset{\lambda}{\text{argmax}} ~ P(\lambda|D)<br>$$<br>하이퍼파라미터의 posterior를 구하는 방법은 다음과 같다.<br>$$<br>P(\lambda|D) \approx P(\lambda) \int P(D|\theta)P(\theta|\lambda) d\theta<br>$$<br>그런데, 이때, $\lambda$의 prior가 필요해지는데, 그냥 uniform prior로 둔다. 그럼 다음과 같다.<br>$$<br>P(\lambda|D) \approx \int P(D|\theta)P(\theta|\lambda) d\theta \approx P(D|\lambda)<br>$$<br> 그리고, 이놈을 최대화하는 $\lambda$를 구하는 것이다.<br>$$<br>\hat{\lambda} = \underset{\lambda}{\text{argmax}} ~ \int P(D|\theta)P(\theta|\lambda) d\theta<br>$$</p><h2 id="Examples-beta-binomial-Model"><a href="#Examples-beta-binomial-Model" class="headerlink" title="Examples: beta-binomial Model"></a>Examples: beta-binomial Model</h2><p>어떤 데이터에 대해 다음처럼 모델링했다 치자.<br>$$<br>x_i \sim \text{binom}(x_i|N_i, \theta_i) \<br>\theta_i \sim \text{beta}(\theta_i|a, b)<br>$$<br>이때, 각 데이터 샘플이 서로 다른 binomial distribution에서 왔다는 것에 주목한다. 즉, $N_i, \theta_i$가 데이터마다 모두 다르다.</p><p>따라서, likelihood는 다음과 같다.<br>$$<br>\text{Likelihood}(X|\Theta) = \prod_i \text{binom}(x_i|N_i, \theta_i)<br>$$<br>그럼 다음처럼 EB(Empirical Bayes)를 이용해서 $a, b$에 대한 posterior에 비례(approximate)하는 함수를 구할 수 있다.<br>$$<br>P(a, b|D) \approx \prod_i \int \text{binom}(x_i|N_i, \theta_i) \cdot \text{beta}(\theta_i|a, b) ~ d\theta_i<br>$$<br>이 식을 최대화하는 $a, b$를 구하면 된다.<br>$$<br>= \prod_i \frac{\text{beta}(a + x_i, b + N_i - x_i)}{\text{beta}(a, b)}<br>$$</p><p>(왜 저렇게 나오지??) </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Empirical-Bayes&quot;&gt;&lt;a href=&quot;#Empirical-Bayes&quot; class=&quot;headerlink&quot; title=&quot;Empirical Bayes&quot;&gt;&lt;/a&gt;Empirical Bayes&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;하이퍼파라미터 또한
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>Appendix 1. Maximize a Posterior</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/APPENDIX-1-MAP/</id>
    <published>2020-03-01T13:08:15.000Z</published>
    <updated>2020-03-03T01:57:13.055Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Maximize-a-Posterior"><a href="#Maximize-a-Posterior" class="headerlink" title="Maximize a Posterior"></a>Maximize a Posterior</h1><p><strong>사후확률 최대화 추정법(MAP Estimation).</strong></p><p>Likelihood를 최대화하는 추정법(MLE - Maximum Likelihood Estimation)은 오직 데이터만을 이용해서 파라미터를 point estimation하는 방법이다. 하지만, MAP는 선행 지식 또는 개인의 믿음을 파라미터 추정에 집어 넣고, 데이터의 정보와 합해서 파라미터를 point estimation하는 방법이다.</p><p>방법은 다음과 같다.</p><ol><li><p>데이터에 대한 likelihood $P(D|\theta)$를 모델링한다.</p></li><li><p>Likelihood $P(D|\theta)$의 파라미터 $\theta$의 prior $P(\theta)$를 모델링한다.</p></li><li><p>Prior $P(\theta)$와 likelihood $P(D|\theta)$를 이용해서 posterior $P(\theta|D)$를 직접 계산할 수 있으면 계산하되, 불가능하다면, posterior와 비례하는 함수 $g(\theta|D)$를 계산한<br>$$<br>P(\theta|D) = \frac{P(D|\theta)P(\theta)}{\sum_{\theta’}P(D|\theta’)P(\theta’)}<br>$$<br>분모인 normalization constant가 계산 불가능할 수도 있다. 그럼, $g(\theta|D)$를 대신 구한다.<br>$$<br>P(\theta|D) \approx g(\theta|D) = P(D|\theta)P(\theta)<br>$$<br>보통, 파라미터의 prior $P(\theta)$가 likelihood $P(D|\theta)$와 conjugate를 이루면, posterior를 직접 계산할 수 있을 것이다. 그렇지 못하면서 가능한 $\theta$의 개수가 많다면, $g(\theta|D)$를 계산해야 하는 경우가 많다.</p></li><li><p>Posterior $P(\theta|D)$가 가장 커지는 $\theta$를 구한다.<br>$$<br>\hat{\theta} = \underset{\theta}{\text{argmax}} ~ P(\theta|D)<br>$$<br>또는 $g(\theta|D)$가 가장 커지는 $\theta$를 구한다.<br>$$<br>\hat{\theta} = \underset{\theta}{\text{argmax}} ~ g(\theta|D)<br>$$<br>미분을 통한 극점을 이용하자. (Gradient descent 같은)</p></li></ol><h3 id="Is-MAP-a-Bayesian-Method"><a href="#Is-MAP-a-Bayesian-Method" class="headerlink" title="Is MAP a Bayesian Method?"></a>Is MAP a Bayesian Method?</h3><p>Bayes 통계의 특징은, 파라미터를 하나의 값이 아닌 분포로 본다는 것이다. 하지만, MAP는 파라미터의 분포를 추정하는게 최종 목표가 아니라 파라미터를 <strong>point estimation</strong>하는 것이 최종 목표이다.</p><p>MAP의 계산 과정상 파라미터의 posterior를 계산하게 되지만, 결국은 point estimation으로, 파라미터를 하나의 값으로 본다는 점에서 완전한 베이지안 통계적 방법이라고 보지 않는 경우가 많다고 한다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Maximize-a-Posterior&quot;&gt;&lt;a href=&quot;#Maximize-a-Posterior&quot; class=&quot;headerlink&quot; title=&quot;Maximize a Posterior&quot;&gt;&lt;/a&gt;Maximize a Posterior&lt;/h1&gt;&lt;
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>14. Predictive Simulations</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/14_Predictive_Simulation/</id>
    <published>2020-03-01T13:08:14.000Z</published>
    <updated>2020-03-03T01:55:44.758Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Predictive-Simulation"><a href="#Predictive-Simulation" class="headerlink" title="Predictive Simulation"></a>Predictive Simulation</h1><p>어떤 예측해야 할 변수 $\lambda$의 분포에서 샘플링하는 시뮬레이션을 말한다. $\lambda$를 데이터를 관찰하기 전의 분포 $p(\lambda)$에서 시뮤레이션하는가, 관찰한 후의 분포 $p(\lambda|D)$에서 시뮬레이션 하는가에 따라 다음과 같이 나뉜다.</p><ul><li>Prior Predictive Simulation</li><li>Posterior Predictive Simulation</li></ul><h2 id="Prior-Predictive-Simulation"><a href="#Prior-Predictive-Simulation" class="headerlink" title="Prior Predictive Simulation"></a>Prior Predictive Simulation</h2><p>Prior를 설정한 후, predictive simulation을 수행하는 것을 말한다. 다음처럼 모델링한 모델이 있다고 하자.<br>$$<br> y_i|\lambda_{j} \sim \text{Pois}(\lambda_{j}) \<br> \lambda_j|\alpha,\beta \sim \text{Gamma}(\alpha, \beta) \<br> \alpha \sim p(\alpha), \beta \sim p(\beta)<br>$$<br>이때, $\alpha,\beta$에 대한 prior를 각각 설정했다면, 그 prior를 바탕으로 $\alpha^<em>, \beta^</em>$를 샘플링할 수 있다. 그런 다음, $\lambda^<em>$를 샘플링한다. 이때, $\lambda^</em>$를 샘플링하는 확률분포는 다음처럼 표시할 수 있다.<br>$$<br>p(\lambda^<em>) = \int p(\lambda^</em>|\alpha,\beta) p(\alpha) p(\beta) ~d\alpha ~d\beta<br>$$<br>위 확률 분포에 따라 $\lambda^*$를 샘플링하는 것을 prior predictive simulation이라고 부르고, 위 확률 분포를 prior predictive distribution이라고 부른다. 이 분포는 likelihood와 prior의 곱의 합으로 이루어진다.</p><p>$\lambda^<em>$를 샘플링했다면, $\lambda$와 마찬가지로 $y^</em>$를 샘플링할 수 있다. 일단 $\lambda^<em>$를 얻었다면, 다음 식에 의해 $y^</em>$를 샘플링할 수 있다.<br>$$<br>p(y^<em>) = \int p(y^</em>|\lambda)p(\lambda) ~d\lambda<br>$$<br>이렇게 계층을 올라가면서 각 파라미터와 예측값에 대해 prior predictive simulation을 할 수 있다.</p><h2 id="Posterior-Predictive-Simulation"><a href="#Posterior-Predictive-Simulation" class="headerlink" title="Posterior Predictive Simulation"></a>Posterior Predictive Simulation</h2><p>데이터를 관측해서 prior를 credential distribution(posterior)로 수정한 이후, predictive simulation하는 것을 말한다. 이때, $\lambda$에 대한 시뮬레이션은 다음과 같다.<br>$$<br>p(\lambda|D) = \int p(\lambda|D,\alpha,\beta)p(\alpha|D)p(\beta|D) ~d\alpha ~d\beta<br>$$<br>위 식을 posterior predictive distribution이라고 부르는데, prior predictive distribution과 다른 점은 각 파라미터 분포에 prior 대신 posterior가 쓰였다는 점이다.</p><p>$y$의 경우도 마찬가지.<br>$$<br>p(y|D) = \int p(y|D,\lambda)p(\lambda|D) ~p\lambda<br>$$<br>시뮬레이션할 때는, posterior로부터 $\alpha^<em>,\beta^</em>$를 샘플링하고, 그 $\alpha^<em>, \beta^</em>$를 이용해서 $\lambda^<em>$를 샘플링한다. 그리고 그 $\lambda^</em>$를 이용해서 $y^*$를 샘플링하면 된다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Predictive-Simulation&quot;&gt;&lt;a href=&quot;#Predictive-Simulation&quot; class=&quot;headerlink&quot; title=&quot;Predictive Simulation&quot;&gt;&lt;/a&gt;Predictive Simulation&lt;/
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>13. Hierarchical Models</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/13_Hierarchical_models/</id>
    <published>2020-03-01T13:08:13.000Z</published>
    <updated>2020-03-03T01:55:41.694Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hierarchical-Models"><a href="#Hierarchical-Models" class="headerlink" title="Hierarchical Models"></a>Hierarchical Models</h1><p>데이터 생성 프로세스를 계층적으로 모델링한 것을 의미한다. 즉, likelihood의 파라미터는 또 다른 파라미터를 갖는 분포를 가지는 형태. 예를들어 likelihood는 poisson 분포를 따른다고 모델링하고, 그 파라미터 $\lambda$는 또 다른 파라미터 $\alpha, \beta$를 가지는 gamma 분포를 따른다고 모델링했다고 하자. 이 경우가 계층적 모델링에 속한다.<br>$$<br>y_i|\lambda_{j} \sim \text{Pois}(\lambda_{j}) \<br>\lambda_j|\alpha,\beta \sim \text{Gamma}(\alpha, \beta) \<br>\alpha \sim p(\alpha), \beta \sim p(\beta)<br>$$<br>이때, 각 $\lambda$는 여러개가 있고, 그중 하나에서 $y$가 생성되지만, $\lambda$는 모두 같은 분포에서 나온 녀석들이라고 모델링 한 것이다. $p(\alpha),p(\beta)$는 각각 $\alpha,\beta$의 prior이다.</p><p>이 경우의 장점은, 데이터가 모두 독립이지 않고 같은 성질을 갖는 놈들(같은 $\lambda$에서 나온 놈들에 해당)은 비슷하고 다른 성질은 갖는 놈들은 조금 다르다는, 약간의 correlation이 있는 데이터를 모델링할 수 있다.</p><p>$\alpha,\beta$는 고정적으로 줘도 될 것인데, 왜 궂이 prior를 할당해서 샘플링하는가? 이건 $$\alpha,\beta$$에 대한 uncertainty(불확실성) 때문이다.</p><p>$$\alpha,\beta$$는 독립적으로 샘플링된다. 그러나, 샘플링된 $\lambda$들 끼리는 독립이 아니다. 대신, $\alpha,\beta$가 주어진다면, 각 $\lambda$끼리는 해당 특정한 $\alpha, \beta$를 파라미터로 하는 분포에서 독립적으로 샘플링됬을 것이므로 조건부 독립이다. $y$끼리도 독립이 아니지만, $\lambda$가 주어진다면 $y$들 끼리 독립(조건부 독립)이다. $\lambda$가 주어졌다는 의미는 어느 한 그룹으로 좁혔고, 그 그룹 내에서 샘플들끼리는 독립이기 때문이다(그렇게 모델링 했으니까).</p><p>이렇게 함으로써, 다른 그룹은 다른 모델로 모델링하기 보다는 계층적인 하나의 모델로 모델링할 수 있다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hierarchical-Models&quot;&gt;&lt;a href=&quot;#Hierarchical-Models&quot; class=&quot;headerlink&quot; title=&quot;Hierarchical Models&quot;&gt;&lt;/a&gt;Hierarchical Models&lt;/h1&gt;&lt;p&gt;데이
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>12. Prior Sensitivity Analysis</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/12_Prior_Sensitivity_Analysis/</id>
    <published>2020-03-01T13:08:12.000Z</published>
    <updated>2020-03-03T01:55:38.746Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prior-Sensitivity-Analysis"><a href="#Prior-Sensitivity-Analysis" class="headerlink" title="Prior Sensitivity Analysis"></a>Prior Sensitivity Analysis</h1><p>Prior sensitivity analysis란, 하나의 파라미터에 대해 여러가지 prior 분포를 적용해보고 만든 여러 모델의 성능을 분석하는 과정을 말한다.</p><p>여러가지 prior를 적용해서 여러 모델을 만들었다고 하자. 그럼 여러 모델들이 내놓는 결과를 바탕으로 다음과 같이 해석할 수 있다. (Prior sensitivity analysis의 결과로 다음 두 가지의 경우가 나온다)</p><ul><li><p>결과가 prior-sensitive하다.</p><p>어떤 prior를 선택하느냐에 따라 추정 성능 및 결과가 크게 달라지는 경우를 말한다. 즉, 데이터보다는 prior가 결과에 영향을 많이 미치는 경우이다. 이 경우, 내가 왜 이 prior를 선택해야 하고 이 모델을 선택해야 하는지 팀에게 설명할 필요가 있다.</p></li><li><p>결과가 prior-insensitive하다.</p><p>이 경우, prior보다는 데이터가 결과에 지대한 영향을 미치는 경우로, prior의 선택에 큰 힘을 쏟을 필요가 없음을 보일 수 있다.</p></li></ul><p>Prior sensitivity analysis는 내가 선택한 prior가 적절하다는 가정을 증명하기 위해서도 사용한다(즉, 가설검정에 사용할 수 있음). 내가 원하는 prior를 선택해서 모델을 구성하고, 내가 원하지 않는 prior를 선택해서 모델을 구성하게 된다. 이때, 내가 원하지 않는 prior를 <strong>skeptical prior</strong>라고 한다. 만약, skeptical prior로 시도해본 여러 모델들이 모두 내가 원한 prior 모델보다 일정 이상 성능이 좋지 않으면 나의 prior 선택을 증명 또는 설명할 수 있다.</p><p>Prior sensitivity analysis를 할때, 해당 prior로 posterior estimation을 통해 성능을 측정하게 된다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Prior-Sensitivity-Analysis&quot;&gt;&lt;a href=&quot;#Prior-Sensitivity-Analysis&quot; class=&quot;headerlink&quot; title=&quot;Prior Sensitivity Analysis&quot;&gt;&lt;/a&gt;Prior Se
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>11. Linear Regression</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/11_Linear_Regression/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/11_Linear_Regression/</id>
    <published>2020-03-01T13:08:11.000Z</published>
    <updated>2020-03-03T01:55:35.786Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>선형 회귀라고도 불리며, 예측해야할 dependent variable이 continuous할때, 유용하다.</p><p>선형 회귀는 다음과 같다.<br>$$<br>y_i \sim \mathbb{N}(\mu_i, \sigma^2) \<br>\mu_i = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k<br>$$<br>또는 다음과 같이 표현할 수 있다.<br>$$<br>y_i = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + \epsilon \<br>\epsilon \sim \mathbb{N}(0, \sigma^2)<br>$$<br>즉, 예측값 $y_i$는 일직선 값에 오차값(residual) $\epsilon$을 더한 값이며, 이 residual값은 normal distribution을 따른다.</p><p>베이지안 통계에서는 $\beta$들은 분포를 갖는 random variable들이다. 따라서, 선형 회귀를 학습시킬때, 각 $\beta$에 대해 prior를 설정하고 posterior를 계산하게 된다.</p><p>각 $\beta$값에는 normal prior를 주는게 보통이지만, 다른 prior도 상관없다.</p><p>단, 어떤 variable $x_i$가 $y_i$에 영향을 주는 놈인지 알고 싶다면, $\beta_i$의 prior로 <strong>Laplace prior</strong>를 설정하기도 한다. Laplace distribution은 double exponential 이라고도 불리며, 0점에서 뾰족한 모양이고 $y$축 대칭이다.</p><p>만약, 어떤 $i$번째 $\beta_i$의 posterior 분포가 그냥 Laplace처럼 0점에 뾰족한 모양에 가깝다면, 그 $\beta_i$에 대응되는 $x_i$는 영향력이 거의 없다고 할 수 있다. 이런 방법을 <strong>Lasso</strong> 라고 부른다.</p><p>JAGS 문법으로 표현하면 다음과 같다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rjags</span></span><br><span class="line"></span><br><span class="line">mod.string &lt;- <span class="string">"model &#123;</span></span><br><span class="line"><span class="string">    # likelihood</span></span><br><span class="line"><span class="string">    for (i in 1:length(y)) &#123;</span></span><br><span class="line"><span class="string">        y[i] ~ dnorm(mu[i], prec)</span></span><br><span class="line"><span class="string">        mu[i] = b0 + b[1]*x1[i] + b[2]*x2[i]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    # prior</span></span><br><span class="line"><span class="string">    # prec의 사후샘플들의 effective size가 5, variability가 2라고 기대하는 경우의</span></span><br><span class="line"><span class="string">    # inverse-gamma는 다음과 같다.</span></span><br><span class="line"><span class="string">    prec ~ dgamma(5/2, 5*2/2)</span></span><br><span class="line"><span class="string">    b0 ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    for (i in 1:2) &#123;</span></span><br><span class="line"><span class="string">        b[i] ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;"</span></span><br></pre></td></tr></table></figure><h2 id="Poison-Regression"><a href="#Poison-Regression" class="headerlink" title="Poison Regression"></a>Poison Regression</h2><p>선형 회귀의 일종으로, response variable인 $y$가 count 값인 경우에 사용되는 경우가 있다. poison 분포는 0보다 크거나 같은 값을 도메인으로 가지므로, $y$역시 0보다 같거나 큰 값이어야 한다. 반대로, $y$의 범위가 0이상이라면, Poison regression을 생각해 볼 수 있다.</p><p>Poison regression은 likelihood가 poison distribution으로 모델링된 형태이다. 그런데, 이 경우, $y$가 0 이상 값이므로, 선형 회귀처럼 $y_i = \beta_0 + \beta_1 x_{1,i}$로 할 수 없다. 대신, $y$를 적절히 변형해서 $[-\infin, \infin]$범위로 만들어 준다면, 선형 함수로 적용이 가능할 것이다. 이때 사용하는 것이 $\text{log}$함수이다. 즉, 다음과 같다.<br>$$<br>\text{log}~y_i = \beta_0 + \beta_1x_{1,i} + \beta_2 x_{2,i} + \cdots \<br>y_i = \text{exp}(\beta_0 + \beta_1x_{1,i} + \beta_2 x_{2,i} + \cdots)<br>$$<br>위의 방법으로 모델링한다. Poison distribution의 파라미터 $\lambda$는 곧 분포의 기댓값이다. 즉, $y_i = \lambda_i$로 생각하면 된다.  JAGS 문법으로 표현하면 다음과 같다.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rjags</span></span><br><span class="line"></span><br><span class="line">mod.string &lt;- <span class="string">"model &#123;</span></span><br><span class="line"><span class="string">    # likelihood</span></span><br><span class="line"><span class="string">    for (i in 1:length(y)) &#123;</span></span><br><span class="line"><span class="string">        y[i] ~ dpois(lambda[i])</span></span><br><span class="line"><span class="string">        log(lambda[i]) = b0 + b[1]*x1[i] + b[2]*x2[i]</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    # prior</span></span><br><span class="line"><span class="string">    b0 ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    for (i in 1:2) &#123;</span></span><br><span class="line"><span class="string">        b[i] ~ dnorm(0, 1e-6)</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Linear-Regression&quot;&gt;&lt;a href=&quot;#Linear-Regression&quot; class=&quot;headerlink&quot; title=&quot;Linear Regression&quot;&gt;&lt;/a&gt;Linear Regression&lt;/h1&gt;&lt;p&gt;선형 회귀라고도 불
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>10. Markov Chain Monte Carlo</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/10_Markov_chain_Monte_Carlo/</id>
    <published>2020-03-01T13:08:10.000Z</published>
    <updated>2020-03-03T01:55:33.288Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Markov-Chain-Monte-Carlo"><a href="#Markov-Chain-Monte-Carlo" class="headerlink" title="Markov Chain Monte Carlo"></a>Markov Chain Monte Carlo</h1><p>MCMC라고도 불린다. 파라미터 $\theta$의 분포 $p(\theta)$를 추정하고자 한다. 그러기 위해, bayesian inference를 하려고 하는데, 그러려면, 데이터를 수집한 후 posterior $p(\theta|Y)$를 계산해야 한다. 그러나, 이 posterior를 계산하기에 상당히 어려울 수 있기 때문에(특히, normalization constant) 대신, posterior를 추정하기로 한다. 이 posterior를 추정할때 쓰일 수 있는 알고리즘 중 하나가 MCMC이다.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>우리가 추정하고자 하는 파라미터 $\theta$의 분포인 $\mathbb{p}(\theta)$를 추정하고자 한다. 그래서 $\theta$를 파라미터로 하는 어떤 데이터 분포 $y|\theta$로부터 $y_1,…y_k$를 샘플링했다. 이를 바탕으로 $\theta$의 prior $p(\theta)$를 설정하고, 데이터로 posterior $p(\theta|y_1,…y_k)$를 계산해서 $\mathbb{p}(\theta)$에 대한 베이지안 추론을 하고자 한다. 그런데, posterior인 $p(\theta|y_1,…,y_k)$를 계산하는게 매우 어렵거나 불가능한 경우가 있다. 이때, posterior 분포 $p(\theta|y_1,…,y_k)$를 추정하기 위한 방법으로 MCMC가 사용될 수 있다.</p><h3 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h3><p>MCMC는 posterior의 분포 $p(\theta|y_1,…y_k)$를 추정하기 위해 마치 이 posterior로부터 샘플링됬을 법한 샘플 $\theta^<em>_1,…,\theta^</em>_m$을 생성해 준다. 이들은 posterior로부터 샘플링 되었을 거라고 가정하고 posterior를 monte carlo estimation으로 추정한다.</p><p>MCMC의 알고리즘으로 여러 개가 있다고 하는데, 대표적으로 Metropolis-Hastings 알고리즘이 있다.</p><h2 id="Matropolis-Hastings-Algorithms"><a href="#Matropolis-Hastings-Algorithms" class="headerlink" title="Matropolis-Hastings Algorithms"></a>Matropolis-Hastings Algorithms</h2><p>알고리즘은 다음과 같다.</p><ol><li><p>시작하기에 앞서 posterior $p(\theta|y_1,…,y_k)$를 정확히 계산할 수는 없더라도, 이 posterior에 비례하는 어떤 함수는 알고 있어야 한다. 즉, 다음을 만족하는 $g(\theta)$는 알고 있어야 한다.<br>$$<br>p(\theta|y_1,…,y_k) \propto g(\theta)<br>$$</p></li><li><p>$\theta$와 도메인이 같거나 최대한 비슷한 분포 아무거나 고른다. 이 분포는 마르코프 체인을 만족하면 좋다. 즉, $q(\theta^*|\theta_{i-1})$.</p></li><li><p>적당히 큰 수 $m$번을 반복하는데, $m$개의 $\theta^*$를 1개씩 샘플링할 것이다.</p><ol><li><p>$\theta^<em>$를 $q(\theta^</em>|\theta_{i-1})$로부터 1개를 샘플링한다.</p></li><li><p>다음을 계산한다.<br>$$<br>\alpha = \frac{g(\theta^<em>)q(\theta^</em>|\theta_{i-1})}{g(\theta_{i-1})q(\theta_{i-1}|\theta^*)}<br>$$</p></li><li><p>$\alpha \geq 1$이면, $\theta_i \leftarrow \theta<em>$로 accept한다. $0 \leq \alpha &lt; 1$이면, $\alpha$의 확률로 $\theta_i \leftarrow \theta^</em>$로 accept하고, reject되면 $\theta_i \leftarrow \theta_{i-1}$한다.</p></li></ol></li></ol><p>분자에 $g(\theta^*)$가 있고, 분모에 $g(\theta_{i-1})$가 있어서, 이전에 뽑은 $\theta$보다 현재 뽑은 $\theta$가 더 $p(\theta|y_1,…,y_k)$에서 확률이 높다면, $\alpha \geq 1$이 되어서 accept된다. $g$가 $p$에 비례하기 때문에 그렇다.</p><p>이렇게 뽑은 $\theta^*$는 초반 샘플링된 놈들을 제외하면, posterior $p(\theta|y_1,…,y_k)$에서 샘플링된 것처럼 역할을 할 수 있다. 분포로부터 샘플링된 놈이 있으므로 posterior에 대해 monte carlo estimation이 가능해진다.</p><h2 id="Random-Walk-Algorithm"><a href="#Random-Walk-Algorithm" class="headerlink" title="Random Walk Algorithm"></a>Random Walk Algorithm</h2><p>Matropolis-hastings 알고리즘에서, proposal distribution $q(\theta^<em>|\theta_{i-1})$를 $\theta_{i-1}$을 평균으로 하는 normal distribution으로 놓은 것을 말한다. Normal distribution은 대칭 분포이기 때문에, $\alpha=\frac{g(\theta^</em>)}{g(\theta_{i-1})}$이 된다.</p><h2 id="Gibbs-Sampling"><a href="#Gibbs-Sampling" class="headerlink" title="Gibbs Sampling"></a>Gibbs Sampling</h2><p>파라미터가 여러개라면, gibbs sampling이 Metropolis-hastings 알고리즘 보다 편할 수 있다. Metropolis-hastings 알고리즘에서는 파라미터 $\theta_1, …, \theta_k$에 대해 proposal distribution을 각 파라미터마다 정의하고, accept, reject과정을 거칠 테지만, gibbs sampling과정에서는 이 과정을 없앴다. 대신 다음의 과정이 있다.</p><p>이때, parameter $\theta_1,…,\theta_k$를 모두 업데이트 1번씩 하는 과정을 1번의 iteration이라고 하자.</p><ol><li><p>일단, $p(\theta_1,…,\theta_k|y) \propto g(\theta_1,…,\theta_k)$를 만족하는 $g(\theta_1, …, \theta_k)$를 알고 있어야 한다. $p(\theta_1,…,\theta_k|y) \propto p(y|\theta_1,…,\theta_k)p(\theta_1,…,\theta_k)$를 활용.</p></li><li><p>하나의 parameter에 대한 full conditional distribution의 proportion을 계산해야 하는데, 다음과 같이 posterior 분포에 비례하므로(Bayes’ rule에 의해), $g$에 비례한다.<br>$$<br>p(\theta_i|\theta_1,…,\theta_{i-1},\theta_{i+1},…,\theta_k,y) \propto p(\theta_1,…,\theta_k|y) \propto g(\theta_1,…,\theta_k)<br>$$<br>그리고, 나머지 파라미터는 모두 주어진 것으로 가정한다. 나머지 파라미터는 초기값이거나 가장 최근에 업데이트한 값으로 들어간다.</p></li><li><p>그렇게 되면, $g$에서 $\theta_i$에 의해 parameterize되지 않는 항은 모두 constant로 취급할 수 있으며, proportion에서 제외할 수 있다. 그럼 $g$가 간소화된다.</p></li><li><p>이렇게 되면, $\theta_i$에 대한 full conditional distribution이 우리가 아는 분포, 즉 샘플링이 가능한 분포가 되는 경우가 있다. 이럴 경우, 그냥 그 분포에서 샘플링하면 되기 때문에 accept, reject과정이 필요가 없다. 하나를 샘플링하고 $\theta_i$를 업데이트한다.</p></li><li><p>파라미터 $\theta_{i+1}$에 대해 같은 과정을 반복하는데, $\theta_{1,…i}$은 이전 iteration의 값이 아니라, 현재 iteration값을 이용한다.</p></li><li><p>만약, 4번 과정에서 샘플링이 가능한 표준적인 분포가 아니라면, 그 안에서, $\theta_i$ 하나에 대해서 matropolis-hastings 알고리즘의 방식을 사용해서, 하나의 샘플을 accept 혹은 reject로 업데이트한다.</p></li><li><p>업데이트 이전 값은 어디다가 저장해두자. 그 값들이 샘플들이다.</p></li></ol><h2 id="Assessing-Convergence-of-MCMC"><a href="#Assessing-Convergence-of-MCMC" class="headerlink" title="Assessing Convergence of MCMC"></a>Assessing Convergence of MCMC</h2><p>MCMC알고리즘에서 샘플링한 샘플들 $\theta^<em>_1, …, \theta^</em>_k$의 평균값 $\bar{\theta^<em>}$이 $\theta$의 posterior 분포 $p(\theta|Y)$를 잘 추정하려면, 마르코프 체인이 충분히 수렴해야 하고, 수렴한 체인으로부터 $\theta^</em>$가 충분히 샘플링되어야 한다. 하지만, 마르코프 체인이 언제 수렴할지를 모르기 때문에 몇 개의 샘플까지가 수렴이 안된 상태의 샘플인지, 몇 개가 유용한 샘플인지 알 수가 없다.</p><h3 id="Stationary-Distribution"><a href="#Stationary-Distribution" class="headerlink" title="Stationary Distribution"></a>Stationary Distribution</h3><p>마르코프 체인이 추정하고자 하는 target distribution(parameter의 posterior가 된다)을 최대한 추정한 distribution을 의미하며, 마르코프 체인이 충분히 수렴한 상태에서의 distribution을 의미한다. 당연히 알 수 없으며, 여기서 마르코프체인으로 샘플링만 가능하다.</p><h3 id="Monte-Carlo-Effective-Sample-Size"><a href="#Monte-Carlo-Effective-Sample-Size" class="headerlink" title="Monte Carlo Effective Sample Size"></a>Monte Carlo Effective Sample Size</h3><p>진짜 Stationary distribution으로부터 독립적으로 샘플링한 샘플을 $\theta_{eff}$이라고 하자. 즉, 이들은 실제로 posterior로부터 샘플링한 샘플과 매우 유사할 것이다.</p><p>우리가 마르코프 체인으로부터 샘플링한 샘플의 개수를 $n$이라고 하자. 하지만, 수렴이 제대로 되지 않은 상태에서 뽑은 것은 독립적인 샘플일 수가 없고, 마르코프 체인이기에, 완전히 독립적이기는 어렵다. 따라서, 유용한 샘플들은 일부일 것이다.</p><p>이 $n$개의 샘플이 가지고 있는 정보가 과연 몇 개의 $\theta_{eff}$들이 가지는 정보와 같은지를 나타내는게 monte carlo effective sample size이다. 즉, $n=1000000$개의 샘플을 뽑았는데, effective sample size $n_{eff}=500$이라고 하면, 이 100만개의 샘플들은 실제로 posterior에서 500개를 샘플링한 것과 같은 정보를 가진다.</p><p>이는, 마르코프 체인이 posterior를 완전히 추정하지 못하기 때문이다. 또한, $n_{eff}$이 너무 작다면, 수렴 속도가 느린 것일수도 있다.</p><h3 id="Auto-correlation"><a href="#Auto-correlation" class="headerlink" title="Auto-correlation"></a>Auto-correlation</h3><p>마르코프 체인에서 샘플링한 한 샘플이 과거 샘플들과 얼마나 많은 dependency가 있는가를 나타낸다. [-1,1] 범위의 값을 가지며, 0에 가까울수록 그 샘플은 과거 샘플들과 관계없는, 독립적인 샘플들이다. monte carlo sample size를 증가시키려면, 이 독립적인 샘플들이 필요하다.</p><p>마르코프 체인으로부터 샘플링을 하면, 초기 몇 개의 샘플까지는 수렴이 되지 않아서 correlation이 높다. 초기 correlation이 0에 가까운 값이 되는 지점까지의 샘플은 버리는 것도 방법(<strong>burn-in 이라고 부름</strong>).</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/1567820594976.png" alt="1567820594976"></p><p>Auto-correlation이 0과 가까운 값이 적으면 effective sample size가 감소한다.</p><h3 id="Gelman-Rubin-Diagnostic"><a href="#Gelman-Rubin-Diagnostic" class="headerlink" title="Gelman-Rubin Diagnostic"></a>Gelman-Rubin Diagnostic</h3><p>마르코프 체인으로부터 샘플링한 샘플들을 주면, 실수값을 반환하는데, 1에 가가우면 수렴이 된 것이고, 1보다 많이 크면, 수렴이 아직 안된 것이다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Markov-Chain-Monte-Carlo&quot;&gt;&lt;a href=&quot;#Markov-Chain-Monte-Carlo&quot; class=&quot;headerlink&quot; title=&quot;Markov Chain Monte Carlo&quot;&gt;&lt;/a&gt;Markov Chain M
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>09. Monte Carlo Estimation</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/09_Monte_Carlo_Estimation/</id>
    <published>2020-03-01T13:08:09.000Z</published>
    <updated>2020-03-03T01:55:30.410Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Monte-Carlo-Estimation"><a href="#Monte-Carlo-Estimation" class="headerlink" title="Monte Carlo Estimation"></a>Monte Carlo Estimation</h1><p>쉽게 말하면, 몬테 카를로 추정법이란, 어떤 특정한 파라미터를 얻기 위해서, 파라미터의 distribution으로부터 많은 샘플링 시뮬레이션을 한 후, 그 샘플들의 평균을 계산한 것을 파라미터의 기댓값으로 추정하는 것이다.</p><p>예를 들어, 어떤 파라미터 $\theta$는 어떤 분포 $p(\theta)$를 따른다. 우리는 파라미터 $\theta$의 기댓값 $E[\theta]$를 계산하고 싶다. 이를 계산하기 위해서는 원래 $E[\theta] = \int p(\theta) \cdot \theta ~~ d\theta$를 계산해야 한다. 하지만, 이 계산은 불가능하거나 매우 힘들 수 있다.</p><p>$E[\theta]$를 계산하는 대신 추정하는 방법으로 몬테 카를로 추정법을 이용한다. 우선, 컴퓨터로 $p(\theta)$로부터 $\theta$를 많이 샘플링한다. 그리고 그들의 평균 $\bar{\theta} = \frac{1}{m}\sum_i^m \theta_i$를 계산하고 $\bar{\theta}$를 $E[\theta]$로 추정하는 것이다.</p><p>분포 $p(\theta)$로부터 높은 확률의 $\theta$가 많이 샘플링되고 낮은 확률의 $\theta$는 적게 샘플링 되었을 것이다. 따라서 이 추정법은 유효할 수 있다. 다른 방식으로 해석하면, central limit theorem에 의해 샘플평균 $\bar{\theta}$는 실제 평균인 $E[\theta]$를 평균으로 하고 $\frac{1}{m}Var[\theta]$를 분산으로 하는 normal distribution을 따른다. 특히, 샘플수가 많아질수록, 계산한 샘플평균은 실제 평균값과 매우 유사할 확률이 높다.</p><p>$h(\theta)$의 기댓값 $E[h(\theta)]$를 추정하고 싶다. 그러면, $\theta$를 많이 샘플링해서 각 샘플로 $h(\theta)$를 계산하고 평균을 내면 $E[h(\theta)]$의 추정값이 된다.</p><h3 id="Monte-Carlo-Error"><a href="#Monte-Carlo-Error" class="headerlink" title="Monte Carlo Error"></a>Monte Carlo Error</h3><p>CLT(Central Limit Theorem)에 의해 파라미터 $\theta$에 대해 모은 샘플들은 $\mathbb{N}(E[\theta],\frac{Var[\theta]}{m})$를 따른다. $Var[\theta]$는 $\theta$의 분산으로, 다음으로 대체한다.<br>$$<br>Var[\theta] = \frac{1}{m}\sum_i (\bar{\theta} - \theta_i)^2<br>$$<br>그리고, $\frac{Var[\theta]}{m}$값을 <strong>monte carlo error</strong>라고 한다. Monte carlo estimation 값($E[\theta]$의 추정값인 $\bar{\theta}$)이 진짜 $E[\theta]$로부터 어느정도로 오차가 있을지에 대한 term이라고 볼 수 있다.</p><h3 id="Monte-Carlo-Marginalization"><a href="#Monte-Carlo-Marginalization" class="headerlink" title="Monte Carlo Marginalization"></a>Monte Carlo Marginalization</h3><p>Paremter가 hierarchical하게 연결된 경우도 있다. 예를들어, 데이터 $Y$는 베르누이 분포 $\text{Bern}(\phi)$를 따르는데, 이 $\phi$가 또 베타분포 $\text{Beta}(2, 2)$를 따른다고 하자. 데이터 $Y$의 기댓값 $E[Y]$를 몬테 카를로 추정법으로 추정하기 위해서는 다음의 과정이 필요하다.</p><ol><li>$\text{Beta}(2, 2)$로부터 $\phi$를 샘플링한다.</li><li>샘플링한 $\phi$를 가지고 $Y|\phi$를 샘플링한다.</li><li>이제, ($Y,\phi$)한 쌍이 생성되었다.</li><li>반복한다.</li></ol><p>이 과정의 특징이, 샘플 ($Y,\phi$)가 자연스럽게 $P(Y,\phi)$의 joint distribution을 반영한다는 것이다.</p><p>그런데, 위에서 샘플링한 $\phi$를 그냥 무시하고 $Y$만 취하면 그게 $\phi$에 대해 marginalization한 것과 같다. 즉, prior predictive distribution을 취한 것이다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Monte-Carlo-Estimation&quot;&gt;&lt;a href=&quot;#Monte-Carlo-Estimation&quot; class=&quot;headerlink&quot; title=&quot;Monte Carlo Estimation&quot;&gt;&lt;/a&gt;Monte Carlo Estimati
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
  <entry>
    <title>08. Bayesian Modeling</title>
    <link href="https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/"/>
    <id>https://jaeyoung-blog.github.io/wiki/studynotes/bayesian-statistics/08_Bayesian_Modeling/</id>
    <published>2020-03-01T13:08:08.000Z</published>
    <updated>2020-03-03T01:55:28.007Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bayesian-Modeling"><a href="#Bayesian-Modeling" class="headerlink" title="Bayesian Modeling"></a>Bayesian Modeling</h1><h2 id="Statistical-Modeling"><a href="#Statistical-Modeling" class="headerlink" title="Statistical Modeling"></a>Statistical Modeling</h2><p>Bayesian modeling은 statistical modeling의 일종이다. Statistical modeling이란, 데이터가 생성/샘플링되는 프로세스를 모델링하는 것을 의미한다.</p><p>Bayesian modeling은 이러한 모델링을 할때, 베이지안 방법론을 적용한 것을 말한다. 전체적인 모델링 프로세스는 bayesian modeling이나 frequentist modeling이나 같다.</p><ol><li>문제 이해</li><li>데이터 수집</li><li>데이터 관찰</li><li>모델 구성</li><li>모델의 구현 및 fit</li><li>샘플공간 분포 파라미터 추정</li><li>테스트 및 예측성능 검사</li><li>5~7번 반복</li><li>모델의 이용.</li></ol><p>이때, frequentist modeling과 bayesian modeling에서의 차이는 모델의 구현과 fit, 파라미터 추정에 있다.</p><h2 id="Model-Specification-모델-구성"><a href="#Model-Specification-모델-구성" class="headerlink" title="Model Specification - 모델 구성"></a>Model Specification - 모델 구성</h2><p>모델의 구성은 계층적으로 적어 내려가면서 파악하는게 어느정도 쉽다. 일단, likelihood를 적고 likelihood에 영향을 미치는 random variable 또는 parameter를 찾는다.</p><p>어느 학교의 학생들의 키(height)의 분포를 예로 들자. 키의 분포는 normal distribution을 따른다고 가정하고 likelihood를 만든다.<br>$$<br>f(y|\theta) = \mathbb{N}(\mu, \sigma^2)<br>$$<br>그리고 $\mu$, $\sigma^2$의 분포가 필요한데, 이들의 prior를 정한다.<br>$$<br>\mu \approx \mathbb{N}(\mu_0, \sigma_0^2)<br>$$<br>$$<br>\sigma^2 \approx \mathbb{IG}(\nu_0, \beta_0)<br>$$</p><p>여기서 $\mathbb{IG}$는 inverse-gamma distribution을 뜻한다. 그리고 각 prior는 독립이라고 가정하면, $p(\mu,\sigma^2) = p(\mu)p(\sigma^2)$일 것이다. 모델로 그려보면 다음과 같다.</p><p><img src="https://raw.githubusercontent.com/wayexists02/my-study-note/image/typora/image/image-20200303093449313.png" alt="image-20200303093449313"></p><p>이렇게, 우선 데이터가 어떻게 생성되었을지에 대해 그 생성 과정을 모델링하는데, likelihood부터 적고, 아래 파라미터까지 노드를 뻗어 나간다.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bayesian-Modeling&quot;&gt;&lt;a href=&quot;#Bayesian-Modeling&quot; class=&quot;headerlink&quot; title=&quot;Bayesian Modeling&quot;&gt;&lt;/a&gt;Bayesian Modeling&lt;/h1&gt;&lt;h2 id=&quot;Stati
      
    
    </summary>
    
      <category term="Study Notes" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/"/>
    
      <category term="Bayesian Statistics" scheme="https://jaeyoung-blog.github.io/categories/Study-Notes/Bayesian-Statistics/"/>
    
    
      <category term="StudyNotes" scheme="https://jaeyoung-blog.github.io/tags/StudyNotes/"/>
    
      <category term="BayesianStatistics" scheme="https://jaeyoung-blog.github.io/tags/BayesianStatistics/"/>
    
  </entry>
  
</feed>
